[
  {
    "paperId": "5172b24d2d37602446b77aff7ef0d49000246703",
    "title": "Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study",
    "abstract": "This paper explores the utility of a Large Language Model (LLM) to automatically generate queries and query variants from a description of an information need. Given a set of information needs described as backstories, we explore how similar the queries generated by the LLM are to those generated by humans. We quantify the similarity using different metrics and examine how the use of each set would contribute to document pooling when building test collections. Our results show potential in using LLMs to generate query variants. While they may not fully capture the wide variety of human-generated variants, they generate similar sets of relevant documents, reaching up to 71.1% overlap at a pool depth of 100.",
    "year": 2023,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2501.17981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2219810354",
        "name": "An Exploratory Study"
      },
      {
        "authorId": "35154089",
        "name": "L. Gallagher"
      },
      {
        "authorId": "73067160",
        "name": "Marwah Alaofi"
      },
      {
        "authorId": "144721996",
        "name": "M. Sanderson"
      },
      {
        "authorId": "1732541",
        "name": "Falk Scholer"
      }
    ]
  },
  {
    "paperId": "6a4deeb40aed8a4d56c8d9401c94b6c7a769e8c3",
    "title": "CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example",
    "abstract": "Query by Example is a well-known information retrieval task in which a document is chosen by the user as the search query and the goal is to retrieve relevant documents from a large collection. However, a document often covers multiple aspects of a topic. To address this scenario we introduce the task of faceted Query by Example in which users can also specify a finer grained aspect in addition to the input query document. We focus on the application of this task in scientific literature search. We envision models which are able to retrieve scientific papers analogous to a query scientific paper along specifically chosen rhetorical structure elements as one solution to this problem. In this work, the rhetorical structure elements, which we refer to as facets, indicate objectives, methods, or results of a scientific paper. We introduce and describe an expert annotated test collection to evaluate models trained to perform this task. Our test collection consists of a diverse set of 50 query documents in English, drawn from computational linguistics and machine learning venues. We carefully follow the annotation guideline used by TREC for depth-k pooling (k = 100 or 250) and the resulting data collection consists of graded relevance scores with high annotation agreement. State of the art models evaluated on our dataset show a significant gap to be closed in further work. Our dataset may be accessed here: https://github.com/iesl/CSFCube",
    "year": 2021,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2103.12906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "9076501",
        "name": "Sheshera Mysore"
      },
      {
        "authorId": "1388957618",
        "name": "Timothy J. O'Gorman"
      },
      {
        "authorId": "143753639",
        "name": "A. McCallum"
      },
      {
        "authorId": "2499986",
        "name": "Hamed Zamani"
      }
    ]
  },
  {
    "paperId": "3f2702e8cf0c113c388ba1f0b674ed873d0126fe",
    "title": "UQV100: A Test Collection with Query Variability",
    "abstract": "We describe the UQV100 test collection, designed to incorporate variability from users. Information need ?backstories? were written for 100 topics (or sub-topics) from the TREC 2013 and 2014 Web Tracks. Crowd workers were asked to read the backstories, and provide the queries they would use; plus effort estimates of how many useful documents they would have to read to satisfy the need. A total of 10,835 queries were collected from 263 workers. After normalization and spell-correction, 5,764 unique variations remained; these were then used to construct a document pool via Indri-BM25 over the ClueWeb12-B corpus. Qualified crowd workers made relevance judgments relative to the backstories, using a relevance scale similar to the original TREC approach; first to a pool depth of ten per query, then deeper on a set of targeted documents. The backstories, query variations, normalized and spell-corrected queries, effort estimates, run outputs, and relevance judgments are made available collectively as the UQV100 test collection. We also make available the judging guidelines and the gold hits we used for crowd-worker qualification and spam detection. We believe this test collection will unlock new opportunities for novel investigations and analysis, including for problems such as task-intent retrieval performance and consistency (independent of query variation), query clustering, query difficulty prediction, and relevance feedback, among others.",
    "year": 2016,
    "citationCount": 81,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2911451.2914671?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2911451.2914671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "144753756",
        "name": "P. Bailey"
      },
      {
        "authorId": "144448479",
        "name": "Alistair Moffat"
      },
      {
        "authorId": "1732541",
        "name": "Falk Scholer"
      },
      {
        "authorId": "41202995",
        "name": "Paul Thomas"
      }
    ]
  },
  {
    "paperId": "ec682d9c7d68149dcd8932acd01a751f2f8b5611",
    "title": "Testing Database Engines via Query Plan Guidance",
    "abstract": "Database systems are widely used to store and query data. Test oracles have been proposed to find logic bugs in such systems, that is, bugs that cause the database system to compute an incorrect result. To realize a fully automated testing approach, such test oracles are paired with a test case generation technique; a test case refers to a database state and a query on which the test oracle can be applied. In this work, we propose the concept of Query Plan Guidance (QPG) for guiding automated testing towards \u201cinteresting\u201d test cases. SQL and other query languages are declarative. Thus, to execute a query, the database system translates every operator in the source language to one of the potentially many so-called physical operators that can be executed; the tree of physical operators is referred to as the query plan. Our intuition is that by steering testing towards exploring a variety of unique query plans, we also explore more interesting behaviors-some of which are potentially incorrect. To this end, we propose a mutation technique that gradually applies promising mutations to the database state, causing the DBMS to create potentially unseen query plans for subsequent queries. We applied our method to three mature, widely-used, and extensively-tested database systems-SQLite, TiDB, and CockroachDB-and found 53 unique, previously unknown bugs. Our method exercises $4.85-408.48\\times$ more unique query plans than a naive random generation method and $7.46\\times$ more than a code coverage guidance method. Since most database systems-including commercial ones-expose query plans to the user, we consider QPG a generally applicable, black-box approach and believe that the core idea could also be applied in other contexts (e.g., to measure the quality of a test suite).",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.17510",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2312.17510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2161601891",
        "name": "Jinsheng Ba"
      },
      {
        "authorId": "2280834096",
        "name": "Manuel Rigger"
      }
    ]
  },
  {
    "paperId": "a2a514ed839dafdd0fb76d6c2615f25f35bf8087",
    "title": "Testing Graph Database Engines via Query Partitioning",
    "abstract": "Graph Database Management Systems (GDBMSs) store data as graphs and allow the efficient querying of nodes and their relationships. Logic bugs are bugs that cause a GDBMS to return an incorrect result for a given query (e.g., by returning incorrect nodes or relationships). The impact of such bugs can be severe, as they often go unnoticed. The core insight of this paper is that Query Partitioning, a test oracle that has been proposed to test Relational Database Systems, is applicable to testing GDBMSs as well. The core idea of Query Partitioning is that, given a query, multiple queries are derived whose results can be combined to reconstruct the given query\u2019s result. Any discrepancy in the result indicates a logic bug. We have implemented this approach as a practical tool named GDBMeter and evaluated GDBMeter on three popular GDBMSs and found a total of 40 unique, previously unknown bugs. We consider 14 of them to be logic bugs, the others being error or crash bugs. Overall, 27 of the bugs have been fixed, and 35 confirmed. We compared our approach to the state-of-the-art approach to testing GDBMS, which relies on differential testing; we found that it results in a high number of false alarms, while Query Partitioning reported actual logic bugs without any false alarms. Furthermore, despite the previous efforts in testing Neo4j and JanusGraph, we found 18 additional bugs. The developers appreciate our work and plan to integrate GDBMeter into their testing process. We expect that this simple, yet effective approach and the practical tool will be used to test other GDBMSs.",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3597926.3598044",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3597926.3598044?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3597926.3598044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1663632797",
        "name": "Matteo Kamm"
      },
      {
        "authorId": "2280834096",
        "name": "Manuel Rigger"
      },
      {
        "authorId": "1625425838",
        "name": "Chengyu Zhang"
      },
      {
        "authorId": "38319925",
        "name": "Z. Su"
      }
    ]
  },
  {
    "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
    "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
    "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",
    "year": 2021,
    "citationCount": 654,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2108.12409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "40170001",
        "name": "Ofir Press"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      }
    ]
  },
  {
    "paperId": "26ca95a72994fdba1c1855eb6b699f98c992b5f4",
    "title": "Testing Database Engines via Pivoted Query Synthesis",
    "abstract": "Relational databases are used ubiquitously. They are managed by database management systems (DBMS), which allow inserting, modifying, and querying data using a domain-specific language called Structured Query Language (SQL). Popular DBMS have been extensively tested by fuzzers, which have been successful in finding crash bugs. However, approaches to finding logic bugs, such as when a DBMS computes an incorrect result set, have remained mostly untackled. Differential testing is an effective technique to test systems that support a common language by comparing the outputs of these systems. However, this technique is ineffective for DBMS, because each DBMS typically supports its own SQL dialect. To this end, we devised a novel and general approach that we have termed Pivoted Query Synthesis. The core idea of this approach is to automatically generate queries for which we ensure that they fetch a specific, randomly selected row, called the pivot row. If the DBMS fails to fetch the pivot row, the likely cause is a bug in the DBMS. We tested our approach on three widely-used and mature DBMS, namely SQLite, MySQL, and PostgreSQL. In total, we reported 123 bugs in these DBMS, 99 of which have been fixed or verified, demonstrating that the approach is highly effective and general. We expect that the wide applicability and simplicity of our approach will enable the improvement of robustness of many DBMS.",
    "year": 2020,
    "citationCount": 98,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2001.04174, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2280834096",
        "name": "Manuel Rigger"
      },
      {
        "authorId": "38319925",
        "name": "Z. Su"
      }
    ]
  },
  {
    "paperId": "7de08d1bc7793f1b47e6e768c81ec1d3bf183500",
    "title": "BERT-QE: Contextualized Query Expansion for Document Re-ranking",
    "abstract": "Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However, query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap, inspired by recent advances in applying contextualized models like BERT to the document retrieval task, this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test collections, the proposed BERT-QE model significantly outperforms BERT-Large models.",
    "year": 2020,
    "citationCount": 86,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.424.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2009.07258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2115548452",
        "name": "Zhi Zheng"
      },
      {
        "authorId": "47214884",
        "name": "Kai Hui"
      },
      {
        "authorId": "2046814249",
        "name": "Ben He"
      },
      {
        "authorId": "3194601",
        "name": "Xianpei Han"
      },
      {
        "authorId": "2110832778",
        "name": "Le Sun"
      },
      {
        "authorId": "144115896",
        "name": "Andrew Yates"
      }
    ]
  },
  {
    "paperId": "f98a78700df4d99ffbb6da8a9f425252eda0e2df",
    "title": "Deep Relevance Ranking Using Enhanced Document-Query Interactions",
    "abstract": "We explore several new models for document relevance ranking, building upon the Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM, which uses context-insensitive encodings of terms and query-document term interactions, we inject rich context-sensitive encodings throughout our models, inspired by PACRR\u2019s (Hui et al., 2017) convolutional n-gram matching features, but extended in several ways including multiple views of query and document inputs. We test our models on datasets from the BIOASQ question answering challenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005), showing they outperform BM25-based baselines, DRMM, and PACRR.",
    "year": 2018,
    "citationCount": 124,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/D18-1211.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1809.01682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "143957226",
        "name": "Ryan T. McDonald"
      },
      {
        "authorId": "51456056",
        "name": "George Brokos"
      },
      {
        "authorId": "1752430",
        "name": "Ion Androutsopoulos"
      }
    ]
  },
  {
    "paperId": "57a9754354857276ce9745761d4dc7d9c1b0c9ff",
    "title": "ANNIS3: A new architecture for generic corpus query and visualization",
    "abstract": "This article is concerned with the data structures, properties of query languages, and visualization facilities required for the generic representation of richly annotated, heterogeneous linguistic corpora. We propose that above and beyond a general graph-based data model, which is becoming increasingly popular in many complex annotation formats, a well-defined concept of multiple, potentially conflicting segmentation layers must be introduced to deal with different sources and applications of corpus data flexibly. We also propose a generic solution for specialized corpus visualizations in a Web interface using annotation-triggered style sheets, which leverage the power of modern browsers and CSS for multiple and highly customizable views of primary data. We offer an implementation and evaluation of our architecture in ANNIS3, an open-source browser-based architecture for corpus search and visualization. We present three case studies to test the coverage of the system, encompassing core linguistic and digital humanities use-cases including richly annotated newspaper treebanks, multilingual diplomatic and normalized manuscript materials edited in TEI, and analysis of multimodal recordings of spoken language.",
    "year": 2016,
    "citationCount": 175,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1093/llc/fqu057?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/llc/fqu057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "48548479",
        "name": "Thomas Krause"
      },
      {
        "authorId": "2548420",
        "name": "Amir Zeldes"
      }
    ]
  },
  {
    "paperId": "f81d7944e9b741bb9ea6c26ef7e4eb47a38a7362",
    "title": "Scaling Up Subgraph Query Processing with Efficient Subgraph Matching",
    "abstract": "A subgraph query finds all data graphs in a graph database each of which contains the given query graph. Existing work takes the indexing-filtering-verification (IFV) approach to first index all data graphs, then filter out some of them based on the index, and finally test subgraph isomorphism on each of the remaining data graphs. This final test of subgraph isomorphism is a sub-problem of subgraph matching, which finds all subgraph isomorphisms from a query graph to a data graph. As such, in this paper, we study whether, and if so, how to utilize efficient subgraph matching to improve subgraph query processing. Specifically, we modify leading subgraph matching algorithms and integrate them with top-performing subgraph querying algorithms. Our results show that (1) the slow verification method in existing IFV algorithms can lead us to over-estimate the gain of filtering; and (2) our modified subgraph querying algorithms with efficient subgraph matching are competitive in time performance and can scale to hundreds of thousands of data graphs and graphs of thousands of vertices.",
    "year": 2019,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDE.2019.00028?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDE.2019.00028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1955271",
        "name": "Shixuan Sun"
      },
      {
        "authorId": "153889459",
        "name": "Qiong Luo"
      }
    ]
  },
  {
    "paperId": "48031e454325487b5fa7972280d1a2400bdef1d4",
    "title": "Query-adaptive Video Summarization via Quality-aware Relevance Estimation",
    "abstract": "Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.",
    "year": 2017,
    "citationCount": 91,
    "openAccessPdf": {
      "url": "https://lirias.kuleuven.be/bitstream/123456789/627830/2/4342_final.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1705.00581, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2326243",
        "name": "A. Vasudevan"
      },
      {
        "authorId": "3037160",
        "name": "Michael Gygli"
      },
      {
        "authorId": "145800409",
        "name": "Anna Volokitin"
      },
      {
        "authorId": "1681236",
        "name": "L. Gool"
      }
    ]
  },
  {
    "paperId": "ad91f1d313b3857a9feb909572aa4ccff33d8270",
    "title": "Large-scale spatial join query processing in Cloud",
    "abstract": "The rapidly increasing amount of location data available in many applications has made it desirable to process their large-scale spatial queries in Cloud for performance and scalability. We report our designs and implementations of two prototype systems that are ready for Cloud deployments: SpatialSpark based on Apache Spark and ISP-MC based on Cloudera Impala. Both systems support indexed spatial joins based on point-in-polygon test and point-to-polyline distance computation. Experiments on the pickup locations of ~170 million taxi trips in New York City and ~10 million global species occurrences records have demonstrated both efficiency and scalability using Amazon EC2 clusters.",
    "year": 2015,
    "citationCount": 204,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDEW.2015.7129541?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDEW.2015.7129541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "3287783",
        "name": "Simin You"
      },
      {
        "authorId": "47540071",
        "name": "Jianting Zhang"
      },
      {
        "authorId": "144099017",
        "name": "L. Gruenwald"
      }
    ]
  },
  {
    "paperId": "5f05544e68687b1bea8a15d08c7aff91bd9401c1",
    "title": "Retrieval Consistency in the Presence of Query Variations",
    "abstract": "A search engine that can return the ideal results for a person's information need, independent of the specific query that is used to express that need, would be preferable to one that is overly swayed by the individual terms used; search engines should be consistent in the presence of syntactic query variations responding to the same information need. In this paper we examine the retrieval consistency of a set of five systems responding to syntactic query variations over one hundred topics, working with the UQV100 test collection, and using Rank-Biased Overlap (RBO) relative to a centroid ranking over the query variations per topic as a measure of consistency. We also introduce a new data fusion algorithm, Rank-Biased Centroid (RBC), for constructing a centroid ranking over a set of rankings from query variations for a topic. RBC is compared with alternative data fusion algorithms. Our results indicate that consistency is positively correlated to a moderate degree with \"deep'' relevance measures. However, it is only weakly correlated with \"shallow'' relevance measures, as well as measures of topic complexity and variety in query expression. These findings support the notion that consistency is an independent property of a search engine's retrieval effectiveness.",
    "year": 2017,
    "citationCount": 56,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3077136.3080839?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3077136.3080839, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "144753756",
        "name": "P. Bailey"
      },
      {
        "authorId": "144448479",
        "name": "Alistair Moffat"
      },
      {
        "authorId": "1732541",
        "name": "Falk Scholer"
      },
      {
        "authorId": "41202995",
        "name": "Paul Thomas"
      }
    ]
  },
  {
    "paperId": "c71cb23b18f73f4a156831fd2e5e1145699ab3eb",
    "title": "QIRANA: A Framework for Scalable Query Pricing",
    "abstract": "Users are increasingly engaging in buying and selling data over the web. Facilitated by the proliferation of online marketplaces that bring such users together, data brokers need to serve requests where they provide results for user queries over the underlying datasets, and price them fairly according to the information disclosed by the query. In this work, we present a novel pricing system, called QIRANA, that performs query-based data pricing for a large class of SQL queries (including aggregation) in real time. QIRANA provides prices with formal guarantees: for example, it avoids prices that create arbitrage opportunities. Our framework also allows flexible pricing, by allowing the data seller to choose from a variety of pricing functions, as well as specify relation and attribute-level parameters that control the price of queries and assign different value to different portions of the data. We test QIRANA on a variety of real-world datasets and query workloads, and we show that it can efficiently compute the prices for queries over large-scale data.",
    "year": 2017,
    "citationCount": 44,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3035918.3064017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3035918.3064017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "34691891",
        "name": "Shaleen Deep"
      },
      {
        "authorId": "1713920",
        "name": "Paraschos Koutris"
      }
    ]
  },
  {
    "paperId": "8b022a5a889c26f637361927d6cf93e1a1d62ebe",
    "title": "Query-by-example keyword spotting using long short-term memory networks",
    "abstract": "We present a novel approach to query-by-example keyword spotting (KWS) using a long short-term memory (LSTM) recurrent neural network-based feature extractor. In our approach, we represent each keyword using a fixed-length feature vector obtained by running the keyword audio through a word-based LSTM acoustic model. We use the activations prior to the softmax layer of the LSTM as our keyword-vector. At runtime, we detect the keyword by extracting the same feature vector from a sliding window and computing a simple similarity score between this test vector and the keyword vector. With clean speech, we achieve 86% relative false rejection rate reduction at 0.5% false alarm rate when compared to a competitive phoneme posteriorgram with dynamic time warping KWS system, while the reduction in the presence of babble noise is 67%. Our system has a small memory footprint, low computational cost, and high precision, making it suitable for on-device applications.",
    "year": 2015,
    "citationCount": 168,
    "openAccessPdf": {
      "url": "http://www.clsp.jhu.edu/%7Eguoguo/papers/icassp2015_myhotword.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP.2015.7178970?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP.2015.7178970, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2335354",
        "name": "Guoguo Chen"
      },
      {
        "authorId": "2057314286",
        "name": "Carolina Parada"
      },
      {
        "authorId": "1784851",
        "name": "Tara N. Sainath"
      }
    ]
  },
  {
    "paperId": "a6944f6494bab5616430c721fdcfa6229be78866",
    "title": "Query Expansion Based on a Feedback Concept Model for Microblog Retrieval",
    "abstract": "We tackle the problem of improving microblog retrieval algorithms by proposing a Feedback Concept Model for query expansion. In particular, we expand the query using knowledge information derived from Probase so that the expanded one could better reflect users' search intent, which allows for microblog retrieval at a concept-level, rather than term-level. In the proposed feedback concept model: (i) we mine the concept information implicit in short-texts based on the external knowledge bases; (ii) with the relevant concepts associated with short-texts, a mixture model is generated to estimate a concept language model; (iii) finally, we utilize the concept language model for query expansion. Moreover, we incorporate temporal prior into the proposed query expansion method to satisfy real-time information need. Finally, we test the generalization power of the feedback concept model on the TREC Microblog corpora. The experimental results demonstrate that the proposed model outperforms the previous methods for microblog retrieval significantly.",
    "year": 2017,
    "citationCount": 50,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3038912.3052710?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3038912.3052710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "3292396",
        "name": "Yashen Wang"
      },
      {
        "authorId": "4590286",
        "name": "Heyan Huang"
      },
      {
        "authorId": "144579978",
        "name": "Chong Feng"
      }
    ]
  },
  {
    "paperId": "e6e8a2c56243847b77b604259cde9e10a2daccb8",
    "title": "Semantic Evaluation for Text-to-SQL with Distilled Test Suite",
    "abstract": "We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.",
    "year": 2020,
    "citationCount": 117,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2010.02840",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2010.02840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "51011000",
        "name": "Ruiqi Zhong"
      },
      {
        "authorId": "48881008",
        "name": "Tao Yu"
      },
      {
        "authorId": "38666915",
        "name": "D. Klein"
      }
    ]
  },
  {
    "paperId": "a49a96c561c10177764e63817fecf814b04a1739",
    "title": "Testing query execution engines with mutations",
    "abstract": "Query optimizer engine plays an important role in modern database systems. However, due to the complex nature of query optimizers, validating the correctness of a query execution engine is inherently challenging. In particular, the high cost of testing query execution engines often prevents developers from making fast iteration during the development process, which can increase the development cycle or lead to production-level bugs. To address this challenge, we propose a tool, MutaSQL, that can quickly discover correctness bugs in SQL execution engines. MutaSQL generates test cases by mutating a query Q over database D into a query Q\u2032 that should evaluate to the same result as Q on D. MutaSQL then checks the execution results of Q\u2032 and Q on the tested engine. We evaluated MutaSQL on previous SQLite versions with known bugs as well as the newest SQLite release. The result shows that MutaSQL can effectively reproduce 34 bugs in previous versions and discover a new bug in the current SQLite release.",
    "year": 2020,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3395032.3395322?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3395032.3395322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2144155769",
        "name": "Xinyue Chen"
      },
      {
        "authorId": "98243944",
        "name": "Chenglong Wang"
      },
      {
        "authorId": "144385783",
        "name": "Alvin Cheung"
      }
    ]
  },
  {
    "paperId": "df40c7fac21d211eefa39340d40df6b50e627bc6",
    "title": "Performance and Scalability of Indexed Subgraph Query Processing Methods",
    "abstract": "Graph data management systems have become very popular as graphs are the natural data model for many applications. One of the main problems addressed by these systems is subgraph query processing; i.e., given a query graph, return all graphs that contain the query. The naive method for processing such queries is to perform a subgraph isomorphism test against each graph in the dataset. This obviously does not scale, as subgraph isomorphism is NP-Complete. Thus, many indexing methods have been proposed to reduce the number of candidate graphs that have to underpass the subgraph isomorphism test. In this paper, we identify a set of key factors-parameters, that influence the performance of related methods: namely, the number of nodes per graph, the graph density, the number of distinct labels, the number of graphs in the dataset, and the query graph size. We then conduct comprehensive and systematic experiments that analyze the sensitivity of the various methods on the values of the key parameters. Our aims are twofold: first to derive conclusions about the algorithms' relative performance, and, second, to stress-test all algorithms, deriving insights as to their scalability, and highlight how both performance and scalability depend on the above factors. We choose six well-established indexing methods, namely Grapes, CT-Index, GraphGrepSX, gIndex, Tree+\u0394, and gCode, as representative approaches of the overall design space, including the most recent and best performing methods. We report on their index construction time and index size, and on query processing performance in terms of time and false positive ratio. We employ both real and synthetic datasets. Specifically, four real datasets of different characteristics are used: AIDS, PDBS, PCM, and PPI. In addition, we generate a large number of synthetic graph datasets, empowering us to systematically study the algorithms' performance and scalability versus the aforementioned key parameters.",
    "year": 2015,
    "citationCount": 52,
    "openAccessPdf": {
      "url": "http://pdfs.semanticscholar.org/b8a5/d2ab3a1f57ac55cbc3b9f9dee3b809c71fea.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.14778/2824032.2824054?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14778/2824032.2824054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2621606",
        "name": "Foteini Katsarou"
      },
      {
        "authorId": "3016157",
        "name": "Nikos Ntarmos"
      },
      {
        "authorId": "1732298",
        "name": "P. Triantafillou"
      }
    ]
  },
  {
    "paperId": "b1ecd03dde90e3d1343ccb5f6e1a43c59e85fc96",
    "title": "Information retrieval from historical newspaper collections in highly inflectional languages: A query expansion approach",
    "abstract": "The aim of the study was to test whether query expansion by approximate string matching methods is beneficial in retrieval from historical newspaper collections in a language rich with compounds and inflectional forms (Finnish). First, approximate string matching methods were used to generate lists of index words most similar to contemporary query terms in a digitized newspaper collection from the 1800s. Top index word variants were categorized to estimate the appropriate query expansion ranges in the retrieval test. Second, the effectiveness of approximate string matching methods, automatically generated inflectional forms, and their combinations were measured in a Cranfield\u2010style test. Finally, a detailed topic\u2010level analysis of test results was conducted. In the index of historical newspaper collection the occurrences of a word typically spread to many linguistic and historical variants along with optical character recognition (OCR) errors. All query expansion methods improved the baseline results. Extensive expansion of around 30 variants for each query word was required to achieve the highest performance improvement. Query expansion based on approximate string matching was superior to using the inflectional forms of the query words, showing that coverage of the different types of variation is more important than precision in handling one type of variation.",
    "year": 2016,
    "citationCount": 25,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1002/asi.23379?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/asi.23379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2869702",
        "name": "Anni J\u00e4rvelin"
      },
      {
        "authorId": "1942885",
        "name": "Heikki Keskustalo"
      },
      {
        "authorId": "1991333",
        "name": "Eero Sormunen"
      },
      {
        "authorId": "1898188",
        "name": "Miamaria Saastamoinen"
      },
      {
        "authorId": "34933662",
        "name": "K. Kettunen"
      }
    ]
  },
  {
    "paperId": "de72cd9e173c02176a69b1388033e2ef60ff8587",
    "title": "Evaluating search features of Google Knowledge Graph and Bing Satori: Entity types, list searches and query interfaces",
    "abstract": "Purpose \u2013 The purpose of this paper is to better understand three main aspects of semantic web search engines of Google Knowledge Graph and Bing Satori. The authors investigated: coverage of entity types, the extent of their support for list search services and the capabilities of their natural language query interfaces. Design/methodology/approach \u2013 The authors manually submitted selected queries to these two semantic web search engines and evaluated the returned results. To test the coverage of entity types, the authors selected the entity types from Freebase database. To test the capabilities of natural language query interfaces, the authors used a manually developed query data set about US geography. Findings \u2013 The results indicate that both semantic search engines cover only the very common entity types. In addition, the list search service is provided for a small percentage of entity types. Moreover, both search engines support queries with very limited complexity and with limited set of recognised ...",
    "year": 2015,
    "citationCount": 62,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1108/OIR-10-2014-0257?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/OIR-10-2014-0257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "7905446",
        "name": "A. Uyar"
      },
      {
        "authorId": "2496554",
        "name": "Farouk Musa Aliyu"
      }
    ]
  },
  {
    "paperId": "89ce0f3479754c3b3fb158a0ae28610dc69aff25",
    "title": "QAGen: generating query-aware test databases",
    "abstract": "Today, a common methodology for testing a database management system (DBMS) is to generate a set of test databases and then execute queries on top of them. However, for DBMS testing, it would be a big advantage if we can control the input and/or the output (e.g., the cardinality) of each individual operator of a test query for a particular test case. Unfortunately, current database generators generate databases independent of queries. As a result, it is hard to guarantee that executing the test query on the generated test databases can obtain the desired (intermediate) query results that match the test case. In this paper, we propose a novel way for DBMS testing. Instead of first generating a test database and then seeing how well it matches a particular test case (or otherwise use a trial-and-error approach to generate another test database), we propose to generate a query-aware database for each test case. To that end, we designed a query-aware test database generator called QAGen. In addition to the database schema and the set of basic constraints defined on the base tables, QAGen takes the query and the set of constraints defined on the query as input, and generates a query-aware test database as output. The generated database guarantees that the test query can get the desired (intermediate) query results as defined in the test case. This approach of testing facilitates a wide range of DBMS testing tasks such as testing of memory managers and testing the cardinality estimation components of query optimizers.",
    "year": 2007,
    "citationCount": 149,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1247480.1247520?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1247480.1247520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2691974",
        "name": "Carsten Binnig"
      },
      {
        "authorId": "1691108",
        "name": "Donald Kossmann"
      },
      {
        "authorId": "145373095",
        "name": "Eric Lo"
      },
      {
        "authorId": "1705151",
        "name": "M. Tamer \u00d6zsu"
      }
    ]
  },
  {
    "paperId": "5c7e23da854d169bd865a927510939e8db3b88f2",
    "title": "Query-Aware Test Generation Using a Relational Constraint Solver",
    "abstract": "We present a novel approach for black-box testing of database management systems (DBMS) using the Alloy tool-set. Given a database schema and an SQL query as inputs, our approach first formulates Alloy models for both inputs, and then using the Alloy Analyzer, it generates (1) input data to populate test databases, and (2) the expected result of executing the given query on the generated data. The Alloy Analyzer results form a complete test suite (input/oracle) for verifying the execution result of a DBMS query processor. By incorporating both the schema and the query during the analysis, our approach performs query-aware data generation where executing the query on the generated data produces meaningful non-empty results. We developed a prototype tool, ADUSA, and used it to evaluate our approach. Experimental results show the ability of our approach to detect bugs in both open-source as well as commercial database management systems.",
    "year": 2008,
    "citationCount": 90,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ASE.2008.34?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ASE.2008.34, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2993970",
        "name": "Shadi Abdul Khalek"
      },
      {
        "authorId": "1991408",
        "name": "Bassem Elkarablieh"
      },
      {
        "authorId": "47262186",
        "name": "Yai O. Laleye"
      },
      {
        "authorId": "145802044",
        "name": "S. Khurshid"
      }
    ]
  },
  {
    "paperId": "75379546ec1c1ba882c9d4ffca4f4e3d67e08004",
    "title": "Query-based test generation for database applications",
    "abstract": "This paper describes a new approach to generating inputs to database applications. The goal is to generate inputs that satisfy certain properties specified by the tester and that also cause queries to return non-empty result sets and cause updates and inserts to execute without violating uniqueness or referential integrity constraints. Based on the SQL statements in the application, test generation queries are generated; execution of these queries yields test inputs with the desired properties. The test generation algorithm is described and illustrated by an example. The technique has been implemented and experimental evaluation is in progress.",
    "year": 2008,
    "citationCount": 40,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1385269.1385277?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1385269.1385277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1727714",
        "name": "David Chays"
      },
      {
        "authorId": "31796112",
        "name": "John Shahid"
      },
      {
        "authorId": "31844146",
        "name": "P. Frankl"
      }
    ]
  },
  {
    "paperId": "ee7e8d910bb246c5f167661c0c204538824574eb",
    "title": "Pooled Evaluation Over Query Variations: Users are as Diverse as Systems",
    "abstract": "Evaluation of information retrieval systems with test collections makes use of a suite of fixed resources: a document corpus; a set of topics; and associated judgments of the relevance of each document to each topic. With large modern collections, exhaustive judging is not feasible. Therefore an approach called pooling is typically used where, for example, the documents to be judged can be determined by taking the union of all documents returned in the top positions of the answer lists returned by a range of systems. Conventionally, pooling uses system variations to provide diverse documents to be judged for a topic; different user queries are not considered. We explore the ramifications of user query variability on pooling, and demonstrate that conventional test collections do not cover this source of variation. The effect of user query variation on the size of the judging pool is just as strong as the effect of retrieval system variation. We conclude that user query variation should be incorporated early in test collection construction, and cannot be considered effectively post hoc.",
    "year": 2015,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2806416.2806606?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2806416.2806606, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "144448479",
        "name": "Alistair Moffat"
      },
      {
        "authorId": "1732541",
        "name": "Falk Scholer"
      },
      {
        "authorId": "41202995",
        "name": "Paul Thomas"
      },
      {
        "authorId": "144753756",
        "name": "P. Bailey"
      }
    ]
  },
  {
    "paperId": "fc5697bb308c43a71bf3c8d714135a23b9e1ccf5",
    "title": "Trading Query Complexity for Sample-Based Testing and Multi-testing Scalability",
    "abstract": "We show that every non-adaptive property testing algorithm making a constant number of queries, over a fixed alphabet, can be converted to a sample-based (as per [Gold Reich and Ron, 2015]) testing algorithm whose average number of queries is a fixed, smaller than 1, power of n. Since the query distribution of the sample-based algorithm is not dependent at all on the property, or the original algorithm, this has many implications in scenarios where there are many properties that need to be tested for concurrently, such as testing (relatively large) unions of properties, or converting a Merlin-Arthur Proximity proof (as per [Gur and Rothblum, 2013]) to a proper testing algorithm. The proof method involves preparing the original testing algorithm for a combinatorial analysis. For the analysis we develop a structural lemma for hyper graphs that may be of independent interest. When analyzing a hyper graph that was extracted from a 2-sided test, it allows for finding generalized sunflowers that provide for a large-deviation type analysis. For 1-sided tests the bounds can be improved further by applying Janson's inequality directly over our structures.",
    "year": 2015,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "https://eprints.bbk.ac.uk/id/eprint/13240/1/Universal.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1504.00695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "authors": [
      {
        "authorId": "1740362",
        "name": "E. Fischer"
      },
      {
        "authorId": "1714254",
        "name": "Oded Lachish"
      },
      {
        "authorId": "1813664",
        "name": "Y. Vasudev"
      }
    ]
  },
  {
    "paperId": "7cbd97def1b7e1396bd64804ec1ca1c9ada34524",
    "title": "Exploring Query Categorisation for Query Expansion: A Study",
    "abstract": "The vocabulary mismatch problem is one of the important challenges facing traditional keyword-based Information Retrieval Systems. The aim of query expansion (QE) is to reduce this query-document mismatch by adding related or synonymous words or phrases to the query. \nSeveral existing query expansion algorithms have proved their merit, but they are not uniformly beneficial for all kinds of queries. Our long-term goal is to formulate methods for applying QE techniques tailored to individual queries, rather than applying the same general QE method to all queries. As an initial step, we have proposed a taxonomy of query classes (from a QE perspective) in this report. We have discussed the properties of each query class with examples. We have also discussed some QE strategies that might be effective for each query category. \nIn future work, we intend to test the proposed techniques using standard datasets, and to explore automatic query categorisation methods.",
    "year": 2015,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1509.05567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2771134",
        "name": "Dipasree Pal"
      },
      {
        "authorId": "1798723",
        "name": "Mandar Mitra"
      },
      {
        "authorId": "2160421",
        "name": "S. Bhattacharya"
      }
    ]
  },
  {
    "paperId": "0c7a7e6a0aa405f6cc6b686915731865222b6e4b",
    "title": "Predicting query performance",
    "abstract": "We develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model. The resulting clarity score measures the coherence of the language usage in documents whose models are likely to generate the query. We suggest that clarity scores measure the ambiguity of a query with respect to a collection of documents and show that they correlate positively with average precision in a variety of TREC test sets. Thus, the clarity score may be used to identify ineffective queries, on average, without relevance information. We develop an algorithm for automatically setting the clarity score threshold between predicted poorly-performing queries and acceptable queries and validate it using TREC data. In particular, we compare the automatic thresholds to optimum thresholds and also check how frequently results as good are achieved in sampling experiments that randomly assign queries to the two classes.",
    "year": 2002,
    "citationCount": 790,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/564376.564429?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/564376.564429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1405333137",
        "name": "Steve Cronen-Townsend"
      },
      {
        "authorId": "2118116642",
        "name": "Yun Zhou"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      }
    ]
  },
  {
    "paperId": "5622150975290eab199779c3b072ab75e5d99765",
    "title": "An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes",
    "abstract": "Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.",
    "year": 2015,
    "citationCount": 126,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSE.2015.47?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSE.2015.47, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2671585",
        "name": "Ripon K. Saha"
      },
      {
        "authorId": "2145398332",
        "name": "Lingming Zhang"
      },
      {
        "authorId": "145802044",
        "name": "S. Khurshid"
      },
      {
        "authorId": "143977783",
        "name": "D. Perry"
      }
    ]
  },
  {
    "paperId": "a824c6e214dd0118f70af8bb05d67d94a858d076",
    "title": "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers",
    "abstract": "3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9\\% in terms of NDS metric on the nuScenes \\texttt{test} set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. We further show that BEVFormer remarkably improves the accuracy of velocity estimation and recall of objects under low visibility conditions. The code is available at \\url{https://github.com/zhiqi-li/BEVFormer}.",
    "year": 2022,
    "citationCount": 1171,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.17270",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2203.17270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2109766685",
        "name": "Zhiqi Li"
      },
      {
        "authorId": "71074736",
        "name": "Wenhai Wang"
      },
      {
        "authorId": "46382329",
        "name": "Hongyang Li"
      },
      {
        "authorId": "41020000",
        "name": "Enze Xie"
      },
      {
        "authorId": "2144553163",
        "name": "Chonghao Sima"
      },
      {
        "authorId": "2115137018",
        "name": "Tong Lu"
      },
      {
        "authorId": "2116045542",
        "name": "Qiao Yu"
      },
      {
        "authorId": "3304536",
        "name": "Jifeng Dai"
      }
    ]
  },
  {
    "paperId": "9dc481ec44178e797466bbad968071917842156b",
    "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
    "abstract": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+6.0}$\\textbf{AP} and $\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.",
    "year": 2022,
    "citationCount": 1232,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.03605",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2203.03605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2315254849",
        "name": "Hao Zhang"
      },
      {
        "authorId": "2146312758",
        "name": "Feng Li"
      },
      {
        "authorId": "8602739",
        "name": "Shilong Liu"
      },
      {
        "authorId": "2152834943",
        "name": "Lei Zhang"
      },
      {
        "authorId": "2093561216",
        "name": "Hang Su"
      },
      {
        "authorId": "89006344",
        "name": "Jun-Juan Zhu"
      },
      {
        "authorId": "1726587",
        "name": "L. Ni"
      },
      {
        "authorId": "93596028",
        "name": "H. Shum"
      }
    ]
  },
  {
    "paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74",
    "title": "What Makes Good In-Context Examples for GPT-3?",
    "abstract": "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3\u2019s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3\u2019s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).",
    "year": 2021,
    "citationCount": 1252,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.deelio-1.10.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2101.06804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "8807538",
        "name": "Jiachang Liu"
      },
      {
        "authorId": "19178763",
        "name": "Dinghan Shen"
      },
      {
        "authorId": "48378494",
        "name": "Yizhe Zhang"
      },
      {
        "authorId": "66648221",
        "name": "Bill Dolan"
      },
      {
        "authorId": "145006560",
        "name": "L. Carin"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      }
    ]
  },
  {
    "paperId": "14b69114783f65bb9f5245dbfd08323da0cc550e",
    "title": "A Test Collection for Matching Patients to Clinical Trials",
    "abstract": "We present a test collection to study the use of search engines for matching eligible patients (the query) to clinical trials (the document). Clinical trials are experiments conducted in the development of new medical treatments, drugs or devices. Recruiting candidates for a trial is often a time-consuming and resource intensive effort, and imposes delays or even the cancellation of trials. The collection described in this paper provides: i) a large corpus of clinical trials; ii) 60 patient case reports used as topics; iii) multiple query representations for a single topic (long, short and ad-hoc); iv) a user provided estimate of how many trials they expect each patient topic would be eligible for; and v) relevance assessments by medical professionals. The availability of such a collection allows researchers to investigate, among other questions: i) the effectiveness of retrieval methods for this task, ii) how multiple representations of an information affect retrieval iii) what influences relevance assessments in this context, iv) whether automated matching of patients to trials improves patient recruitment. The collection is available at http://doi.org/10.4225/08/5714557510C17.",
    "year": 2016,
    "citationCount": 52,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2911451.2914672?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2911451.2914672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1783566",
        "name": "B. Koopman"
      },
      {
        "authorId": "1692855",
        "name": "G. Zuccon"
      }
    ]
  },
  {
    "paperId": "eda66713401c9d53a5572f1244b47b00e77d0117",
    "title": "Context-aware query suggestion by mining click-through and session data",
    "abstract": "Query suggestion plays an important role in improving the usability of search engines. Although some recently proposed methods can make meaningful query suggestions by mining query patterns from search logs, none of them are context-aware - they do not take into account the immediately preceding queries as context in query suggestion. In this paper, we propose a novel context-aware query suggestion approach which is in two steps. In the offine model-learning step, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. Then, from session data a concept sequence suffix tree is constructed as the query suggestion model. In the online query suggestion step, a user's search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence sufix tree, our approach suggests queries to the user in a context-aware manner. We test our approach on a large-scale search log of a commercial search engine containing 1:8 billion search queries, 2:6 billion clicks, and 840 million query sessions. The experimental results clearly show that our approach outperforms two baseline methods in both coverage and quality of suggestions.",
    "year": 2008,
    "citationCount": 602,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1401890.1401995?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1401890.1401995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "4205363",
        "name": "Huanhuan Cao"
      },
      {
        "authorId": "71790825",
        "name": "Daxin Jiang"
      },
      {
        "authorId": "145525190",
        "name": "J. Pei"
      },
      {
        "authorId": "145109822",
        "name": "Qi He"
      },
      {
        "authorId": "143696247",
        "name": "Zhen Liao"
      },
      {
        "authorId": "2227868312",
        "name": "Enhong Chen"
      },
      {
        "authorId": "49404233",
        "name": "Hang Li"
      }
    ]
  },
  {
    "paperId": "29ab59feb1ae9387f6bc011aae0e5b6dbb27c562",
    "title": "Query Performance Prediction By Considering Score Magnitude and Variance Together",
    "abstract": "Query Performance prediction aims to evaluate the effectiveness of the results returned by a search system in response to a query without any relevance information. In this paper, we propose a method that considers both magnitude and variance of scores of the ranked list of results to measure the performance of a query. Using six different TREC test sets, we compare our predictor with three of the state-of-the-art techniques. The experimental results show that our method is very competitive. Pairwise comparisons with each of the three other methods show that our predictor performs better in more data sets.",
    "year": 2014,
    "citationCount": 54,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2661829.2661906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2661829.2661906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2126380",
        "name": "Yongquan Tao"
      },
      {
        "authorId": "1684230",
        "name": "Shengli Wu"
      }
    ]
  },
  {
    "paperId": "2e6c034382d0ebfd3d3204cf823478d282dba15f",
    "title": "The Circle Game: Scalable Private Membership Test Using Trusted Hardware",
    "abstract": "Malware checking is changing from being a local service to a cloud-assisted one where users' devices query a cloud server, which hosts a dictionary of malware signatures, to check if particular applications are potentially malware. Whilst such an architecture gains all the benefits of cloud-based services, it opens up a major privacy concern since the cloud service can infer personal traits of the users based on the lists of applications queried by their devices. Private membership test (PMT) schemes can remove this privacy concern. However, known PMT schemes do not scale well to a large number of simultaneous users and high query arrival rates. We propose a simple PMT approach using a carousel: circling the entire dictionary through trusted hardware on the cloud server. Users communicate with the trusted hardware via secure channels. We show how the carousel approach, using different data structures to represent the dictionary, can be realized on two different commercial hardware security architectures (ARM TrustZone and Intel SGX). We highlight subtle aspects of securely implementing seemingly simple PMT schemes on these architectures. Through extensive experimental analysis, we show that for the malware checking scenario our carousel approach surprisingly outperforms Path ORAM on the same hardware by supporting a much higher query arrival rate while guaranteeing acceptable response latency for individual queries.",
    "year": 2016,
    "citationCount": 54,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1606.01655",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1606.01655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2058606769",
        "name": "Sandeep Tamrakar"
      },
      {
        "authorId": "77595850",
        "name": "Jian Liu"
      },
      {
        "authorId": "1682503",
        "name": "Andrew J. Paverd"
      },
      {
        "authorId": "2416715",
        "name": "Jan-Erik Ekberg"
      },
      {
        "authorId": "1689531",
        "name": "Benny Pinkas"
      },
      {
        "authorId": "2248858512",
        "name": "N. Asokan"
      }
    ]
  },
  {
    "paperId": "7b58a4545c093b828f7f3a887e665c64b36acc4f",
    "title": "Thesaurus-based automatic query expansion for interface-driven code search",
    "abstract": "Software engineers often resort to code search practices to support software maintenance and evolution tasks, in particular code reuse. An issue that affects code search is the vocabulary mismatch problem: while searching for a particular function, users have to guess the exact words that were chosen by original developers to name code entities. In this paper we present an automatic query expansion (AQE) approach that uses word relations to increase the chances of finding relevant code. The approach is applied on top of Test-Driven Code Search (TDCS), a promising code retrieval technique that uses test cases as inputs to formulate the search query, but can also be used with other techniques that handle interface definitions to produce queries (interface-driven code search). Since these techniques rely on keywords and types, the vocabulary mismatch problem is also relevant. AQE is carried out by leveraging WordNet, a type thesaurus for expanding types, and another thesaurus containing only software-related word relations. Our approach is general but was specifically designed for non-native English speakers, who are frequently unaware of the most common terms used to name functions in software. Our evaluation with 36 non-native subjects - including developers and senior Computer Science students - provides evidence that our approach can improve the chances of finding relevant functions by 41% (recall improvement of 30%, on average), without hurting precision.",
    "year": 2014,
    "citationCount": 57,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2597073.2597087?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2597073.2597087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1815847",
        "name": "Ot\u00e1vio Augusto Lazzarini Lemos"
      },
      {
        "authorId": "1731230",
        "name": "A. C. D. Paula"
      },
      {
        "authorId": "3000751",
        "name": "Felipe Capodifoglio Zanichelli"
      },
      {
        "authorId": "34955824",
        "name": "C. Lopes"
      }
    ]
  },
  {
    "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
    "title": "Bidirectional Attention Flow for Machine Comprehension",
    "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
    "year": 2016,
    "citationCount": 2072,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1611.01603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "4418074",
        "name": "Minjoon Seo"
      },
      {
        "authorId": "2684226",
        "name": "Aniruddha Kembhavi"
      },
      {
        "authorId": "143787583",
        "name": "Ali Farhadi"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ]
  },
  {
    "paperId": "db6612061addc8b4046f769e3cecaaffe880b7e6",
    "title": "Test-driven evaluation of linked data quality",
    "abstract": "Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.",
    "year": 2014,
    "citationCount": 302,
    "openAccessPdf": {
      "url": "http://svn.aksw.org/papers/2014/WWW_Databugger/public.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2566486.2568002?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2566486.2568002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2627116",
        "name": "D. Kontokostas"
      },
      {
        "authorId": "50453446",
        "name": "Patrick Westphal"
      },
      {
        "authorId": "145044578",
        "name": "S. Auer"
      },
      {
        "authorId": "2024066",
        "name": "Sebastian Hellmann"
      },
      {
        "authorId": "144568027",
        "name": "Jens Lehmann"
      },
      {
        "authorId": "39494666",
        "name": "R. Cornelissen"
      },
      {
        "authorId": "3305769",
        "name": "A. Zaveri"
      }
    ]
  },
  {
    "paperId": "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
    "title": "Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection",
    "abstract": "Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder \"generalizes\" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE.",
    "year": 2019,
    "citationCount": 1183,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1904.02639",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1904.02639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "145542268",
        "name": "Dong Gong"
      },
      {
        "authorId": "2161037",
        "name": "Lingqiao Liu"
      },
      {
        "authorId": "144672395",
        "name": "Vuong Le"
      },
      {
        "authorId": "1735360",
        "name": "Budhaditya Saha"
      },
      {
        "authorId": "1761350",
        "name": "M. Mansour"
      },
      {
        "authorId": "143761093",
        "name": "S. Venkatesh"
      },
      {
        "authorId": "5546141",
        "name": "A. Hengel"
      }
    ]
  },
  {
    "paperId": "fdd8d4c86af081048fa7d12619da7daed84f5a4c",
    "title": "Search-Based Test Data Generation for SQL Queries",
    "abstract": "Database-centric systems strongly rely on SQL queries to manage and manipulate their data. These SQL commands can range from very simple selections to queries that involve several tables, subqueries, and grouping operations. And, as with any important piece of code, developers should properly test SQL queries. In order to completely test a SQL query, developers need to create test data that exercise all possible coverage targets in a query, e.g., JOINs and WHERE predicates. And indeed, this task can be challenging and time-consuming for complex queries. Previous studies have modeled the problem of generating test data as a constraint satisfaction problem and, with the help of SAT solvers, generate the required data. However, such approaches have strong limitations, such as partial support for queries with JOINs, subqueries, and strings (which are commonly used in SQL queries). In this paper, we model test data generation for SQL queries as a search-based problem. Then, we devise and evaluate three different approaches based on random search, biased random search, and genetic algorithms (GAs). The GA, in particular, uses a fitness function based on information extracted from the physical query plan of a database engine as search guidance. We then evaluate each approach in 2,135 queries extracted from three open source software and one industrial software system. Our results show that GA is able to completely cover 98.6% of all queries in the dataset, requiring only a few seconds per query. Moreover, it does not suffer from the limitations affecting state-of-the art techniques.",
    "year": 2018,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "https://repository.tudelft.nl/islandora/object/uuid%3A3b7e2cc6-4810-44a9-a19a-3cc8a260d495/datastream/OBJ/download",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3180155.3180202?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3180155.3180202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "46229213",
        "name": "J. Castelein"
      },
      {
        "authorId": "2289215",
        "name": "M. Aniche"
      },
      {
        "authorId": "1853891",
        "name": "Mozhan Soltani"
      },
      {
        "authorId": "3013302",
        "name": "Annibale Panichella"
      },
      {
        "authorId": "1737202",
        "name": "A. Deursen"
      }
    ]
  },
  {
    "paperId": "e553ff7e5e965437cc9832bf66361f19bfbe076d",
    "title": "DNA barcoding and taxonomy in Diptera: a tale of high intraspecific variability and low identification success.",
    "abstract": "DNA barcoding and DNA taxonomy have recently been proposed as solutions to the crisis of taxonomy and received significant attention from scientific journals, grant agencies, natural history museums, and mainstream media. Here, we test two key claims of molecular taxonomy using 1333 mitochondrial COI sequences for 449 species of Diptera. We investigate whether sequences can be used for species identification (\"DNA barcoding\") and find a relatively low success rate (< 70%) based on tree-based and newly proposed species identification criteria. Misidentifications are due to wide overlap between intra- and interspecific genetic variability, which causes 6.5% of all query sequences to have allospecific or a mixture of allo- and conspecific (3.6%) best-matching barcodes. Even when two COI sequences are identical, there is a 6% chance that they belong to different species. We also find that 21% of all species lack unique barcodes when consensus sequences of all conspecific sequences are used. Lastly, we test whether DNA sequences yield an unambiguous species-level taxonomy when sequence profiles are assembled based on pairwise distance thresholds. We find many sequence triplets for which two of the three pairwise distances remain below the threshold, whereas the third exceeds it; i.e., it is impossible to consistently delimit species based on pairwise distances. Furthermore, for species profiles based on a 3% threshold, only 47% of all profiles are consistent with currently accepted species limits, 20% contain more than one species, and 33% only some sequences from one species; i.e., adopting such a DNA taxonomy would require the redescription of a large proportion of the known species, thus worsening the taxonomic impediment. We conclude with an outlook on the prospects of obtaining complete barcode databases and the future use of DNA sequences in a modern integrative taxonomy.",
    "year": 2006,
    "citationCount": 1315,
    "openAccessPdf": {
      "url": "https://academic.oup.com/sysbio/article-pdf/55/5/715/26558042/10635150600969864.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1080/10635150600969864?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10635150600969864, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Medicine",
      "Biology"
    ],
    "authors": [
      {
        "authorId": "48836973",
        "name": "R. Meier"
      },
      {
        "authorId": "9969136",
        "name": "Kwong Shiyang"
      },
      {
        "authorId": "3054526",
        "name": "Gaurav Vaidya"
      },
      {
        "authorId": "2024557",
        "name": "P. Ng"
      }
    ]
  },
  {
    "paperId": "35a98b5f26e5e97e1a3b0f1298632bc620a8eb4f",
    "title": "Uncertainty-Driven Black-Box Test Data Generation",
    "abstract": "We can never be certain that a software system is correct simply by testing it, but with every additional successful test we become less uncertain about its correctness. In absence of source code or elaborate specifications and models, tests are usually generated or chosen randomly. However, rather than randomly choosing tests, it would be preferable to choose those tests that decrease our uncertainty about correctness the most. In order to guide test generation, we apply what is referred to in Machine Learning as \"Query Strategy Framework\": We infer a behavioural model of the system under test and select those tests which the inferred model is \"least certain\" about. Running these tests on the system under test thus directly targets those parts about which tests so far have failed to inform the model. We provide an implementation that uses a genetic programming engine for model inference in order to enable an uncertainty sampling technique known as \"query by committee\", and evaluate it on eight subject systems from the Apache Commons Math framework and JodaTime. The results indicate that test generation using uncertainty sampling outperforms conventional and Adaptive Random Testing.",
    "year": 2016,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "https://eprints.whiterose.ac.uk/120345/1/selecting-effective-black.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1608.03181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1772527",
        "name": "Neil Walkinshaw"
      },
      {
        "authorId": "145507591",
        "name": "G. Fraser"
      }
    ]
  },
  {
    "paperId": "824aac4970a4d149b35c19a9d2d2dec4c994688e",
    "title": "QBIC project: querying images by content, using color, texture, and shape",
    "abstract": "In the query by image content (QBIC) project we are studying methods to query large on-line image databases using the images' content as the basis of the queries. Examples of the content we use include color, texture, and shape of image objects and regions. Potential applications include medical (`Give me other images that contain a tumor with a texture like this one'), photo-journalism (`Give me images that have blue at the top and red at the bottom'), and many others in art, fashion, cataloging, retailing, and industry. Key issues include derivation and computation of attributes of images and objects that provide useful query functionality, retrieval methods based on similarity as opposed to exact match, query by image example or user drawn image, the user interfaces, query refinement and navigation, high dimensional database indexing, and automatic and semi-automatic database population. We currently have a prototype system written in X/Motif and C running on an RS/6000 that allows a variety of queries, and a test database of over 1000 images and 1000 objects populated from commercially available photo clip art images. In this paper we present the main algorithms for color texture, shape and sketch query that we use, show example query results, and discuss future directions.",
    "year": 1993,
    "citationCount": 2253,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.143648?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.143648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2141915",
        "name": "W. Niblack"
      },
      {
        "authorId": "50281905",
        "name": "R. Barber"
      },
      {
        "authorId": "1712308",
        "name": "W. Equitz"
      },
      {
        "authorId": "1712991",
        "name": "M. Flickner"
      },
      {
        "authorId": "123856769",
        "name": "E. Glasman"
      },
      {
        "authorId": "143867341",
        "name": "D. Petkovic"
      },
      {
        "authorId": "70341848",
        "name": "P. Yanker"
      },
      {
        "authorId": "1702392",
        "name": "C. Faloutsos"
      },
      {
        "authorId": "1690237",
        "name": "G. Taubin"
      }
    ]
  },
  {
    "paperId": "7d986dac610e20441adb9161e5466c88932626e9",
    "title": "A study of smoothing methods for language models applied to information retrieval",
    "abstract": "Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role---to make the estimated document language model more accurate and to \"explain\" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.",
    "year": 2004,
    "citationCount": 1378,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/984321.984322?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/984321.984322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "143869012",
        "name": "Chengxiang Zhai"
      },
      {
        "authorId": "1739581",
        "name": "J. Lafferty"
      }
    ]
  },
  {
    "paperId": "4837083ec5c15fdcbb3580b04bc80e17332f75f3",
    "title": "Improving the accuracy of PSI-BLAST protein database searches with composition-based statistics and other refinements.",
    "abstract": "PSI-BLAST is an iterative program to search a database for proteins with distant similarity to a query sequence. We investigated over a dozen modifications to the methods used in PSI-BLAST, with the goal of improving accuracy in finding true positive matches. To evaluate performance we used a set of 103 queries for which the true positives in yeast had been annotated by human experts, and a popular measure of retrieval accuracy (ROC) that can be normalized to take on values between 0 (worst) and 1 (best). The modifications we consider novel improve the ROC score from 0.758 +/- 0.005 to 0.895 +/- 0.003. This does not include the benefits from four modifications we included in the 'baseline' version, even though they were not implemented in PSI-BLAST version 2.0. The improvement in accuracy was confirmed on a small second test set. This test involved analyzing three protein families with curated lists of true positives from the non-redundant protein database. The modification that accounts for the majority of the improvement is the use, for each database sequence, of a position-specific scoring system tuned to that sequence's amino acid composition. The use of composition-based statistics is particularly beneficial for large-scale automated applications of PSI-BLAST.",
    "year": 2001,
    "citationCount": 1365,
    "openAccessPdf": {
      "url": "https://academic.oup.com/nar/article-pdf/29/14/2994/9905966/292994.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1093/NAR/29.14.2994?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/NAR/29.14.2994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "authors": [
      {
        "authorId": "1783217",
        "name": "A. Sch\u00e4ffer"
      },
      {
        "authorId": "144750405",
        "name": "L. Aravind"
      },
      {
        "authorId": "34806045",
        "name": "Thomas L. Madden"
      },
      {
        "authorId": "9929570",
        "name": "Serge Shavirin"
      },
      {
        "authorId": "2513436",
        "name": "J. Spouge"
      },
      {
        "authorId": "2245561",
        "name": "Y. Wolf"
      },
      {
        "authorId": "2967695",
        "name": "E. Koonin"
      },
      {
        "authorId": "1805699",
        "name": "S. Altschul"
      }
    ]
  },
  {
    "paperId": "054080d32f41ec461758fdc382935a45b033836b",
    "title": "A study of smoothing methods for language models applied to Ad Hoc information retrieval",
    "abstract": "Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections.",
    "year": 2001,
    "citationCount": 1337,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/383952.384019?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/383952.384019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1736467",
        "name": "ChengXiang Zhai"
      },
      {
        "authorId": "1739581",
        "name": "J. Lafferty"
      }
    ]
  },
  {
    "paperId": "3464374899e799cbd516d00f75e425efd495150e",
    "title": "IR evaluation methods for retrieving highly relevant documents",
    "abstract": "This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In-Query1) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.",
    "year": 2000,
    "citationCount": 1373,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/345508.345545",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/345508.345545?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/345508.345545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2768186",
        "name": "K. J\u00e4rvelin"
      },
      {
        "authorId": "2732839",
        "name": "Jaana Kek\u00e4l\u00e4inen"
      }
    ]
  },
  {
    "paperId": "9fdd5e7cbc13df1ed1323fe4b3496ea004df8112",
    "title": "Dynamic Test Collections for Retrieval Evaluation",
    "abstract": "Batch evaluation with test collections of documents, search topics, and relevance judgments has been the bedrock of IR evaluation since its adoption by Salton for his experiments on vector space systems. Such test collections have limitations: they contain no user interaction data; there is typically only one query per topic; they have limited size due to the cost of constructing them. In the last 15-20 years, it has become evident that having a log of user interactions and a large space of queries is invaluable for building effective retrieval systems, but such data is generally only available to search engine companies. Thus there is a gap between what academics can study using static test collections and what industrial researchers can study using dynamic user data. In this work we propose dynamic test collections to help bridge this gap. Like traditional test collections, a dynamic test collection consists of a set of topics and relevance judgments. But instead of static one-time queries, dynamic test collections generate queries in response to the system. They can generate other actions such as clicks and time spent reading documents. Like static test collections, there is no human in the loop, but since the queries are dynamic they can generate much more data for evaluation than static test collections can. And since they can simulate user interactions across a session, they can be used for evaluating retrieval systems that make use of session history or other user information to try to improve results.",
    "year": 2015,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2808194.2809470?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2808194.2809470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1750995",
        "name": "Ben Carterette"
      },
      {
        "authorId": "2528096",
        "name": "Ashraf Bah Rabiou"
      },
      {
        "authorId": "2059852789",
        "name": "M. Zengin"
      }
    ]
  },
  {
    "paperId": "089c6224cfbcf5c18b63564eb65001c7c42a7acf",
    "title": "Knockoff Nets: Stealing Functionality of Black-Box Models",
    "abstract": "Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such ``victim'' models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we study complex victim blackbox models, and an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a ``knockoff'' with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as show that a reasonable knockoff of an image analysis API could be created for as little as $30.",
    "year": 2018,
    "citationCount": 513,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/1812.02766",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1812.02766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "9517443",
        "name": "Tribhuvanesh Orekondy"
      },
      {
        "authorId": "48920094",
        "name": "B. Schiele"
      },
      {
        "authorId": "1739548",
        "name": "Mario Fritz"
      }
    ]
  },
  {
    "paperId": "2c03d0e5113cc34ff607c652c68a5e542e607735",
    "title": "Property testing and its connection to learning and approximation",
    "abstract": "The authors study the question of determining whether an unknown function has a particular property or is /spl epsiv/-far from any function with that property. A property testing algorithm is given a sample of the value of the function on instances drawn according to some distribution, and possibly may query the function on instances of its choice. First, they establish some connections between property testing and problems in learning theory. Next, they focus on testing graph properties, and devise algorithms to test whether a graph has properties such as being k-colorable or having a /spl rho/-clique (clique of density /spl rho/ w.r.t. the vertex set). The graph property testing algorithms are probabilistic and make assertions which are correct with high probability utilizing only poly(1//spl epsiv/) edge-queries into the graph, where /spl epsiv/ is the distance parameter. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph which correspond to the property being tested, if it holds for the input graph.",
    "year": 1996,
    "citationCount": 1163,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/285055.285060",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/SFCS.1996.548493?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SFCS.1996.548493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "authors": [
      {
        "authorId": "1707322",
        "name": "Oded Goldreich"
      },
      {
        "authorId": "1706681",
        "name": "S. Goldwasser"
      },
      {
        "authorId": "1735952",
        "name": "D. Ron"
      }
    ]
  },
  {
    "paperId": "7d5cf22c70484fe217936c66741fb73b2a278bde",
    "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents",
    "abstract": "Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence \u2014 effectively performing multihop, alias multi-step, inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 54.5% on an annotated test set, compared to human performance at 85.0%, leaving ample room for improvement.",
    "year": 2017,
    "citationCount": 499,
    "openAccessPdf": {
      "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00021",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1710.06481, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1851564",
        "name": "Johannes Welbl"
      },
      {
        "authorId": "1918552",
        "name": "Pontus Stenetorp"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      }
    ]
  },
  {
    "paperId": "e77c484af99fc1eb3d3c36699ac81822e98cb74d",
    "title": "Image Segmentation Using Text and Image Prompts",
    "abstract": "Image segmentation is usually addressed by training a model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system that can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation. We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail. This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated. Finally, we find our system to adapt well to generalized queries involving affordances or properties. Code is available at https://eckerlab.org/code/CLIPSeg",
    "year": 2021,
    "citationCount": 418,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2112.10003",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2112.10003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "73235537",
        "name": "Timo L\u00fcddecke"
      },
      {
        "authorId": "1746183",
        "name": "Alexander S. Ecker"
      }
    ]
  },
  {
    "paperId": "a27fcc2eb7c64d726af7af9e39d299e36cfc0122",
    "title": "Query chains: learning to rank from implicit feedback",
    "abstract": "This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. We observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. Using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. We also implemented a real-world search engine to test our approach, using a modified ranking SVM to learn an improved ranking function from preference data. Our results demonstrate significant improvements in the ranking given by the search engine. The learned rankings outperform both a static ranking function, as well as one trained without considering query chains.",
    "year": 2005,
    "citationCount": 579,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/cs/0605035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1803571",
        "name": "Filip Radlinski"
      },
      {
        "authorId": "1680188",
        "name": "T. Joachims"
      }
    ]
  },
  {
    "paperId": "c6e5df6322659276da6133f9b734a389d7a255e8",
    "title": "Attention-over-Attention Neural Networks for Reading Comprehension",
    "abstract": "Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces \u201cattended attention\u201d for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children\u2019s Book Test.",
    "year": 2016,
    "citationCount": 435,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/P17-1055.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1607.04423, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "3043830",
        "name": "Yiming Cui"
      },
      {
        "authorId": "46842323",
        "name": "Z. Chen"
      },
      {
        "authorId": "144572674",
        "name": "Si Wei"
      },
      {
        "authorId": "2108620507",
        "name": "Shijin Wang"
      },
      {
        "authorId": "40282288",
        "name": "Ting Liu"
      },
      {
        "authorId": "40936264",
        "name": "Guoping Hu"
      }
    ]
  },
  {
    "paperId": "8ff67b8bb53883f2e1c30990bdf44ec2d8b502b9",
    "title": "Word Spotting and Recognition with Embedded Attributes",
    "abstract": "This paper addresses the problems of word spotting and word recognition on images. In word spotting, the goal is to find all instances of a query word in a dataset of images. In recognition, the goal is to recognize the content of the word image, usually aided by a dictionary or lexicon. We describe an approach in which both word images and text strings are embedded in a common vectorial subspace. This is achieved by a combination of label embedding and attributes learning, and a common subspace regression. In this subspace, images and strings that represent the same word are close together, allowing one to cast recognition and retrieval tasks as a nearest neighbor problem. Contrary to most other existing methods, our representation has a fixed length, is low dimensional, and is very fast to compute and, especially, to compare. We test our approach on four public datasets of both handwritten documents and natural images showing results comparable or better than the state-of-the-art on spotting and recognition tasks.",
    "year": 2014,
    "citationCount": 498,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2014.2339814?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2014.2339814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "authors": [
      {
        "authorId": "145467588",
        "name": "Jon Almaz\u00e1n"
      },
      {
        "authorId": "1821267",
        "name": "Albert Gordo"
      },
      {
        "authorId": "1686569",
        "name": "A. Forn\u00e9s"
      },
      {
        "authorId": "2864362",
        "name": "Ernest Valveny"
      }
    ]
  },
  {
    "paperId": "c87c6745fd0568ae94554a198ee0f42c1b9d8bf3",
    "title": "T3i: a tool for generating and querying test suites for Java",
    "abstract": "T3i is an automated unit-testing tool to test Java classes. To expose interactions T3i generates test-cases in the form of sequences of calls to the methods of the target class. What separates it from other testing tools is that it treats test suites as first class objects and allows users to e.g. combine, query, and filter them. With these operations, the user can construct a test suite with specific properties. Queries can be used to check correctness properties. Hoare triples, LTL formulas, and algebraic equations can be queried. T3i can be used interactively, thus facilitating more exploratory testing, as well as through a script. The familiar Java syntax can be used to control it, or alternatively one can use the much lighter Groovy syntax.",
    "year": 2015,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "https://dspace.library.uu.nl/bitstream/handle/1874/321619/950.pdf?sequence=1&isAllowed=y",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2786805.2803182?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2786805.2803182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2286170507",
        "name": "I. Prasetya"
      }
    ]
  },
  {
    "paperId": "d48cfaee175284682c3a86e8d3232ee1fd0fc576",
    "title": "Benchmarking Ontology-Based Query Rewriting Systems",
    "abstract": "\n \n Query rewriting is a prominent reasoning technique in ontology-based data access applications. A wide variety of query rewriting algorithms have been proposed in recent years and implemented in highly optimised reasoning systems. Query rewriting systems are complex software programs; even if based on provably correct algorithms, sophisticated optimisations make the systems more complex and errors become more likely to happen. In this paper, we present an algorithm that, given an ontology as input, synthetically generates ``relevant'' test queries. Intuitively, each of these queries can be used to verify whether the system correctly performs a certain set of ``inferences'', each of which can be traced back to axioms in the input ontology. Furthermore, we present techniques that allow us to determine whether a system is unsound and/or incomplete for a given test query and ontology. Our evaluation shows that most publicly available query rewriting systems are unsound and/or incomplete, even on commonly used benchmark ontologies; more importantly, our techniques revealed the precise causes of their correctness issues and the systems were then corrected based on our feedback. Finally, since our evaluation is based on a larger set of test queries than existing benchmarks, which are based on hand-crafted queries, it also provides a better understanding of the scalability behaviour of each system.\n \n",
    "year": 2012,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/8215/8073",
      "status": "GOLD",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v26i1.8215?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v26i1.8215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2128983",
        "name": "Martha Imprialou"
      },
      {
        "authorId": "2174835",
        "name": "G. Stoilos"
      },
      {
        "authorId": "1784440",
        "name": "B. C. Grau"
      }
    ]
  },
  {
    "paperId": "e05483a41e8002e7024d39457e55a3fe533f5835",
    "title": "How Many Demonstrations Do You Need for In-context Learning?",
    "abstract": "Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps (\"chain of thoughts (CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into\"correct demos\"leading to the correct answer, and\"wrong demos\"resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicating the weakness of LLMs in finding correct demo(s) for input queries, which is difficult to evaluate on the biased datasets. Furthermore, we observe a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy degrades(improves) when given more correct(wrong) demos. This implies that ICL can be easily misguided by interference among demos and their spurious correlations. Our analyses highlight several fundamental challenges that need to be addressed in LLMs training, ICL, and benchmark design.",
    "year": 2023,
    "citationCount": 34,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.745.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2303.08119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1391200710",
        "name": "Jiuhai Chen"
      },
      {
        "authorId": "2108451006",
        "name": "Lichang Chen"
      },
      {
        "authorId": "2215276411",
        "name": "Chen Zhu"
      },
      {
        "authorId": "2213956781",
        "name": "Tianyi Zhou"
      }
    ]
  },
  {
    "paperId": "c6f5cec8e0cad436ce4f374289a9603a8e865e4c",
    "title": "An acoustic segment modeling approach to query-by-example spoken term detection",
    "abstract": "The framework of posteriorgram-based template matching has been shown to be successful for query-by-example spoken term detection (STD). This framework employs a tokenizer to convert query examples and test utterances into frame-level posteriorgrams, and applies dynamic time warping to match the query posteriorgrams with test posteriorgrams to locate possible occurrences of the query term. It is not trivial to design a reliable tokenizer due to heterogeneous test conditions and the limitation of training resources. This paper presents a study of using acoustic segment models (ASMs) as the tokenizer. ASMs can be obtained following an unsupervised iterative procedure without any training transcriptions. The STD performance of the ASM tokenizer is evaluated on Fisher Corpus with comparison to three alternative tokenizers. Experimental results show that the ASM tokenizer outperforms a conventional GMM tokenizer and a language-mismatched phoneme recognizer. In addition, the performance is significantly improved by applying unsupervised speaker normalization techniques.",
    "year": 2012,
    "citationCount": 61,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP.2012.6289081?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP.2012.6289081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2113216571",
        "name": "Haipeng Wang"
      },
      {
        "authorId": "2146624",
        "name": "C. Leung"
      },
      {
        "authorId": "121589722",
        "name": "Tan Lee"
      },
      {
        "authorId": "144720333",
        "name": "B. Ma"
      },
      {
        "authorId": "1711271",
        "name": "Haizhou Li"
      }
    ]
  },
  {
    "paperId": "2f6a6cc8d3763ca4d2e34f4b38738ff722216383",
    "title": "Finding Task-Relevant Features for Few-Shot Learning by Category Traversal",
    "abstract": "Few-shot learning is an important area of research. Conceptually, humans are readily able to understand new concepts given just a few examples, while in more pragmatic terms, limited-example training situations are common practice. Recent effective approaches to few-shot learning employ a metric-learning framework to learn a feature similarity comparison between a query (test) example, and the few support (training) examples. However, these approaches treat each support class independently from one another, never looking at the entire task as a whole. Because of this, they are constrained to use a single set of features for all possible test-time tasks, which hinders the ability to distinguish the most relevant dimensions for the task at hand. In this work, we introduce a Category Traversal Module that can be inserted as a plug-and-play module into most metric-learning based few-shot learners. This component traverses across the entire support set at once, identifying task-relevant features based on both intra-class commonality and inter-class uniqueness in the feature space. Incorporating our module improves performance considerably (5%-10% relative) over baseline systems on both miniImageNet and tieredImageNet benchmarks, with overall performance competitive with the most recent state-of-the-art systems.",
    "year": 2019,
    "citationCount": 332,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1905.11116",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1905.11116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "46382329",
        "name": "Hongyang Li"
      },
      {
        "authorId": "2060028",
        "name": "D. Eigen"
      },
      {
        "authorId": "143635238",
        "name": "Samuel F. Dodge"
      },
      {
        "authorId": "48799969",
        "name": "Matthew D. Zeiler"
      },
      {
        "authorId": "31843833",
        "name": "Xiaogang Wang"
      }
    ]
  },
  {
    "paperId": "c5b17a4f2ab9b0af2549ff103371de91b3a3c661",
    "title": "Query-by-example spoken term detection using phonetic posteriorgram templates",
    "abstract": "This paper examines a query-by-example approach to spoken term detection in audio files. The approach is designed for low-resource situations in which limited or no in-domain training material is available and accurate word-based speech recognition capability is unavailable. Instead of using word or phone strings as search terms, the user presents the system with audio snippets of desired search terms to act as the queries. Query and test materials are represented using phonetic posteriorgrams obtained from a phonetic recognition system. Query matches in the test data are located using a modified dynamic time warping search between query templates and test utterances. Experiments using this approach are presented using data from the Fisher corpus.",
    "year": 2009,
    "citationCount": 309,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ASRU.2009.5372889?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ASRU.2009.5372889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1798550",
        "name": "Timothy J. Hazen"
      },
      {
        "authorId": "2529583",
        "name": "Wade Shen"
      },
      {
        "authorId": "1719880",
        "name": "Christopher M. White"
      }
    ]
  },
  {
    "paperId": "625686866788358b426da13cddd12b6862ce27d2",
    "title": "Test Collection Based Evaluation of Information Retrieval Systems",
    "abstract": "Use of test collections and evaluation measures to assess the effectiveness of information retrieval systems has its origins in work dating back to the early 1950s. Across the nearly 60 years since that work started, use of test collections is a de facto standard of evaluation. This monograph surveys the research conducted and explains the methods and measures devised for evaluation of retrieval systems, including a detailed look at the use of statistical significance testing in retrieval experimentation. This monograph reviews more recent examinations of the validity of the test collection approach and evaluation measures as well as outlining trends in current research exploiting query logs and live labs. At its core, the modern-day test collection is little different from the structures that the pioneering researchers in the 1950s and 1960s conceived of. This tutorial and review shows that despite its age, this long-standing evaluation method is still a highly valued tool for retrieval research.",
    "year": 2010,
    "citationCount": 437,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1561/1500000009?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1561/1500000009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "144721996",
        "name": "M. Sanderson"
      }
    ]
  },
  {
    "paperId": "1df3f95b59b56a779aeecdc6d76f369b7dfe1d36",
    "title": "Deriving query intents from web search engine queries",
    "abstract": "The purpose of this article is to test the reliability of query intents derived from queries, either by the user who entered the query or by another juror. We report the findings of three studies. First, we conducted a large-scale classification study (~50,000 queries) using a crowdsourcing approach. Next, we used clickthrough data from a search engine log and validated the judgments given by the jurors from the crowdsourcing study. Finally, we conducted an online survey on a commercial search engine's portal. Because we used the same queries for all three studies, we also were able to compare the results and the effectiveness of the different approaches. We found that neither the crowdsourcing approach, using jurors who classified queries originating from other users, nor the questionnaire approach, using searchers who were asked about their own query that they just entered into a Web search engine, led to satisfying results. This leads us to conclude that there was little understanding of the classification tasks, even though both groups of jurors were given detailed instructions. Although we used manual classification, our research also has important implications for automatic classification. We must question the success of approaches using automatic classification and comparing its performance to a baseline from human jurors. \u00a9 2012 Wiley Periodicals, Inc.",
    "year": 2012,
    "citationCount": 34,
    "openAccessPdf": {
      "url": "http://eprints.rclis.org/17245/1/JASIST_Query_Intents_Preprint.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1002/asi.22706?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/asi.22706, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1490868646",
        "name": "D. Lewandowski"
      },
      {
        "authorId": "14770107",
        "name": "Jessica Drechsler"
      },
      {
        "authorId": "41185746",
        "name": "S. Mach"
      }
    ]
  },
  {
    "paperId": "6b291dea9bb80fea41f4ffa17aaad52ab5d26ada",
    "title": "Learning To Count Everything",
    "abstract": "Existing works on visual counting primarily focus on one specific category at a time, such as people, animals, and cells. In this paper, we are interested in counting everything, that is to count objects from any category given only a few annotated instances from that category. To this end, we pose counting as a few-shot regression task. To tackle this task, we present a novel method that takes a query image together with a few exemplar objects from the query image and predicts a density map for the presence of all objects of interest in the query image. We also present a novel adaptation strategy to adapt our network to any novel visual category at test time, using only a few exemplar objects from the novel category. We also introduce a dataset of 147 object categories containing over 6000 images that are suitable for the few-shot counting task. The images are annotated with two types of annotation, dots and bounding boxes, and they can be used for developing few-shot counting models. Experiments on this dataset shows that our method outperforms several state-of-the-art object detectors and few-shot counting approaches. Our code and dataset can be found at https://github.com/cvlab-stonybrook/LearningToCountEverything.",
    "year": 2021,
    "citationCount": 129,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.08391",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2104.08391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2673180",
        "name": "Viresh Ranjan"
      },
      {
        "authorId": "2070827645",
        "name": "U. Sharma"
      },
      {
        "authorId": "2116085788",
        "name": "Thua Nguyen"
      },
      {
        "authorId": "2356016",
        "name": "Minh Hoai"
      }
    ]
  },
  {
    "paperId": "a6d02a259d7a153cc83b35c06e664d4063c56c3f",
    "title": "SQLite",
    "abstract": "Queries Write a query that will answer each of the following questions based on the data in a SQLite database named sakila.db. Test your queries using the sqlite3 command line client. Then copy and paste the queries and the output they produce into a document called queries.txt and submit the document via Blackboard. NOTE: some queries (like the first one) will produce hundreds of lines of output. In these cases add \u201climit 5\u201d to the end of your query. 1. What are the first and last names of all the customers? select first_name, last_name from customer; 2. What are the e-mail addresses of customers whose first name is \u201cWillard\u201d? select email from customer where first_name = \"WILLARD\"; 3. How many customers are there? select count(*) from customer; 4. How many customers shop at store number 1? select count(*) from customer where store_id = 1; 5. How much does it cost to rent the film named \u201cVirtual Spoilers\u201d? select rental_rate from film where title = \"VIRTUAL SPOILERS\"; 6. Do any films have the word \u201cPrincess\u201d in the title? select title from film where title like \"%PRINCESS%\"; 7. What are the titles of the films that are longer than 180 minutes? select title from film where length > 180; 8. How many films have a \u201cG\u201d rating and are less than 60 minutes long? select count(*) from film where rating = \"G\" and length < 60; 9. What is the maximum replacement cost for any film? select max(replacement_cost) from film; 10. Print a table that lists the different types of ratings and the number of films that have that rating. select rating, count(*) from film group by rating; CIS 211 Winter 2014",
    "year": 2019,
    "citationCount": 262,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1017/9781108591942.021?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/9781108591942.021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": null,
    "authors": [
      {
        "authorId": "2288180309",
        "name": "Jude Cruise"
      }
    ]
  },
  {
    "paperId": "2a6ac80d7b8c8721159db816f6422c309edc289c",
    "title": "Testing the accuracy of query optimizers",
    "abstract": "The accuracy of a query optimizer is intricately connected with a database system performance and its operational cost: the more accurate the optimizer's cost model, the better the resulting execution plans. Database application programmers and other practitioners have long provided anecdotal evidence that database systems differ widely with respect to the quality of their optimizers, yet, to date no formal method is available to database users to assess or refute such claims.\n In this paper, we develop a framework to quantify an optimizer's accuracy for a given workload. We make use of the fact that optimizers expose switches or hints that let users influence the plan choice and generate plans other than the default plan. Using these implements, we force the generation of multiple alternative plans for each test case, time the execution of all alternatives and rank the plans by their effective costs. We compare this ranking with the ranking of the estimated cost and compute a score for the accuracy of the optimizer.\n We present initial results of an anonymized comparisons for several major commercial database systems demonstrating that there are in fact substantial differences between systems. We also suggest ways to incorporate this knowledge into the commercial development process.",
    "year": 2012,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2304510.2304525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2304510.2304525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2368366",
        "name": "Zhongxian Gu"
      },
      {
        "authorId": "144018773",
        "name": "Mohamed A. Soliman"
      },
      {
        "authorId": "1826748",
        "name": "F. Waas"
      }
    ]
  },
  {
    "paperId": "9e6e48e67b3ade4a3f080f4fa74b202a68ee91ec",
    "title": "Computation of Octanol-Water Partition Coefficients by Guiding an Additive Model with Knowledge",
    "abstract": "We have developed a new method, i.e., XLOGP3, for logP computation. XLOGP3 predicts the logP value of a query compound by using the known logP value of a reference compound as a starting point. The difference in the logP values of the query compound and the reference compound is then estimated by an additive model. The additive model implemented in XLOGP3 uses a total of 87 atom/group types and two correction factors as descriptors. It is calibrated on a training set of 8199 organic compounds with reliable logP data through a multivariate linear regression analysis. For a given query compound, the compound showing the highest structural similarity in the training set will be selected as the reference compound. Structural similarity is quantified based on topological torsion descriptors. XLOGP3 has been tested along with its predecessor, i.e., XLOGP2, as well as several popular logP methods on two independent test sets: one contains 406 small-molecule drugs approved by the FDA and the other contains 219 oligopeptides. On both test sets, XLOGP3 produces more accurate predictions than most of the other methods with average unsigned errors of 0.24-0.51 units. Compared to conventional additive methods, XLOGP3 does not rely on an extensive classification of fragments and correction factors in order to improve accuracy. It is also able to utilize the ever-increasing experimentally measured logP data more effectively.",
    "year": 2007,
    "citationCount": 663,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1021/CI700257Y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1021/CI700257Y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "authors": [
      {
        "authorId": "1797121",
        "name": "Tiejun Cheng"
      },
      {
        "authorId": "2110150815",
        "name": "Yuan Zhao"
      },
      {
        "authorId": "2116244380",
        "name": "Xun Li"
      },
      {
        "authorId": "47183493",
        "name": "F. Lin"
      },
      {
        "authorId": "2146647321",
        "name": "Yong Xu"
      },
      {
        "authorId": "2153649581",
        "name": "Xing-long Zhang"
      },
      {
        "authorId": "1941434",
        "name": "Yan Li"
      },
      {
        "authorId": "1953190",
        "name": "Renxiao Wang"
      },
      {
        "authorId": "79487286",
        "name": "L. Lai"
      }
    ]
  },
  {
    "paperId": "f3f2fdf2e17ddf6ca778d7b87924a35b63605de4",
    "title": "Evaluating multi-query sessions",
    "abstract": "The standard system-based evaluation paradigm has focused on assessing the performance of retrieval systems in serving the best results for a single query. Real users, however, often begin an interaction with a search engine with a sufficiently under-specified query that they will need to reformulate before they find what they are looking for. In this work we consider the problem of evaluating retrieval systems over test collections of multi-query sessions. We propose two families of measures: a model-free family that makes no assumption about the user's behavior over a session, and a model-based family with a simple model of user interactions over the session. In both cases we generalize traditional evaluation metrics such as average precision to multi-query session evaluation. We demonstrate the behavior of the proposed metrics by using the new TREC 2010 Session track collection and simulations over the TREC-9 Query track collection.",
    "year": 2011,
    "citationCount": 87,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2009916.2010056?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2009916.2010056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1713134",
        "name": "E. Kanoulas"
      },
      {
        "authorId": "1750995",
        "name": "Ben Carterette"
      },
      {
        "authorId": "1704149",
        "name": "Paul D. Clough"
      },
      {
        "authorId": "144721996",
        "name": "M. Sanderson"
      }
    ]
  },
  {
    "paperId": "8d2d497b2d5f08c024e84307b212510bc8ff8e73",
    "title": "Reducing long queries using query quality predictors",
    "abstract": "Long queries frequently contain many extraneous terms that hinder retrieval of relevant documents. We present techniques to reduce long queries to more effective shorter ones that lack those extraneous terms. Our work is motivated by the observation that perfectly reducing long TREC description queries can lead to an average improvement of 30% in mean average precision. Our approach involves transforming the reduction problem into a problem of learning to rank all sub-sets of the original query (sub-queries) based on their predicted quality, and selecting the top sub-query. We use various measures of query quality described in the literature as features to represent sub-queries, and train a classifier. Replacing the original long query with the top-ranked sub-query chosen by the ranker results in a statistically significant average improvement of 8% on our test sets. Analysis of the results shows that query reduction is well-suited for moderately-performing long queries, and a small set of query quality predictors are well-suited for the task of ranking sub-queries.",
    "year": 2009,
    "citationCount": 200,
    "openAccessPdf": {
      "url": "http://www.cs.cmu.edu/~vitor/papers/SIGIR09.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1571941.1572038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1571941.1572038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1752202",
        "name": "G. Kumaran"
      },
      {
        "authorId": "145177235",
        "name": "Vitor R. Carvalho"
      }
    ]
  },
  {
    "paperId": "51613c8082ecb89b570d03a22b188385077a891e",
    "title": "Query Expansion for Microblog Retrieval",
    "abstract": "The extreme brevity of Microblog posts (such as \u2018tweets\u2019) exacerbates the well-known vocabulary mismatch problem when retrieving tweets in response to user queries. In this study, we explore various query expansion approaches as a way to address this problem. We use the Web as a source of query expansion terms. We also tried a variation of a standard pseudo-relevance feedback method. Results on the TREC 2011 Microblog test data (TWEETS11 corpus) are very promising \u2013 significant improvements are obtained over a baseline retrieval strategy that uses no query expansion. Since many of the TREC queries were oriented towards the news genre, we also tried using only news sites (BBC and NYTIMES) in the hope that these would be a cleaner, less noisy source for expansion terms. This turned out to be counter-productive.",
    "year": 2011,
    "citationCount": 58,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1504/IJWS.2012.052535?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1504/IJWS.2012.052535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2064008723",
        "name": "Ayan Bandyopadhyay"
      },
      {
        "authorId": "153408379",
        "name": "Kripabandhu Ghosh"
      },
      {
        "authorId": "1911868",
        "name": "Prasenjit Majumder"
      },
      {
        "authorId": "1798723",
        "name": "Mandar Mitra"
      }
    ]
  },
  {
    "paperId": "56f944d0fb742d23ad8abcd1727c3871b577a021",
    "title": "Query performance prediction in web search environments",
    "abstract": "Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist. In this paper, we present three techniques to address these challenges. We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding. Our evaluation is mainly performed on the GOV2 collection. In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types. To assist prediction under the mixed-query situation, a novel query classifier is adopted. Results show that our prediction of web query performance is substantially more accurate than the current state-of-the-art prediction techniques. Consequently, our paper provides a practical approach to performance prediction in real-world web settings.",
    "year": 2007,
    "citationCount": 275,
    "openAccessPdf": {
      "url": "http://maroo.cs.umass.edu/pub/web/getpdf.php?id=726",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1277741.1277835?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1277741.1277835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2118116642",
        "name": "Yun Zhou"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      }
    ]
  },
  {
    "paperId": "e76b8b2fc244687a0df2d2ec4d2141b701b600fe",
    "title": "The RCSB Protein Data Bank: a redesigned query system and relational database based on the mmCIF schema",
    "abstract": "The Protein Data Bank (PDB) is the central worldwide repository for three-dimensional (3D) structure data of biological macromolecules. The Research Collaboratory for Structural Bioinformatics (RCSB) has completely redesigned its resource for the distribution and query of 3D structure data. The re-engineered site is currently in public beta test at http://pdbbeta.rcsb.org. The new site expands the functionality of the existing site by providing structure data in greater detail and uniformity, improved query and enhanced analysis tools. A new key feature is the integration and searchability of data from over 20 other sources covering genomic, proteomic and disease relationships. The current capabilities of the re-engineered site, which will become the RCSB production site at http://www.pdb.org in late 2005, are described.",
    "year": 2004,
    "citationCount": 342,
    "openAccessPdf": {
      "url": "https://academic.oup.com/nar/article-pdf/33/suppl_1/D233/7621857/gki057.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC540011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Biology",
      "Computer Science",
      "Medicine"
    ],
    "authors": [
      {
        "authorId": "38571497",
        "name": "Nita Deshpande"
      },
      {
        "authorId": "3295755",
        "name": "K. J. Addess"
      },
      {
        "authorId": "48966347",
        "name": "Wolfgang Bluhm"
      },
      {
        "authorId": "1402500523",
        "name": "Jeffrey C. Merino-Ott"
      },
      {
        "authorId": "1402500530",
        "name": "Wayne Townsend-Merino"
      },
      {
        "authorId": "2112724335",
        "name": "Qing Zhang"
      },
      {
        "authorId": "3095361",
        "name": "Charlie Knezevich"
      },
      {
        "authorId": "2152788272",
        "name": "Lie Xie"
      },
      {
        "authorId": "144423437",
        "name": "Li Chen"
      },
      {
        "authorId": "2272834",
        "name": "Zukang Feng"
      },
      {
        "authorId": "39150499",
        "name": "Rachel Kramer Green"
      },
      {
        "authorId": "1401264789",
        "name": "J. Flippen-Anderson"
      },
      {
        "authorId": "34819339",
        "name": "J. Westbrook"
      },
      {
        "authorId": "1679221",
        "name": "H. Berman"
      },
      {
        "authorId": "2048644",
        "name": "P. Bourne"
      }
    ]
  },
  {
    "paperId": "201d996011ec64807257c93c54bbfeeb884e4778",
    "title": "Guided test generation for database applications via synthesized database interactions",
    "abstract": "Testing database applications typically requires the generation of tests consisting of both program inputs and database states. Recently, a testing technique called Dynamic Symbolic Execution (DSE) has been proposed to reduce manual effort in test generation for software applications. However, applying DSE to generate tests for database applications faces various technical challenges. For example, the database application under test needs to physically connect to the associated database, which may not be available for various reasons. The program inputs whose values are used to form the executed queries are not treated symbolically, posing difficulties for generating valid database states or appropriate database states for achieving high coverage of query-result-manipulation code. To address these challenges, in this article, we propose an approach called SynDB that synthesizes new database interactions to replace the original ones from the database application under test. In this way, we bridge various constraints within a database application: query-construction constraints, query constraints, database schema constraints, and query-result-manipulation constraints. We then apply a state-of-the-art DSE engine called Pex for .NET from Microsoft Research to generate both program inputs and database states. The evaluation results show that tests generated by our approach can achieve higher code coverage than existing test generation approaches for database applications.",
    "year": 2014,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2491529?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2491529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "145834128",
        "name": "Kai Pan"
      },
      {
        "authorId": "7916525",
        "name": "Xintao Wu"
      },
      {
        "authorId": "145906750",
        "name": "Tao Xie"
      }
    ]
  },
  {
    "paperId": "d3f4161bf2d6b7fdf56425c4ce48877299d5a43d",
    "title": "Automated SQL query generation for systematic testing of database engines",
    "abstract": "We present a novel approach for generating syntactically and semantically correct SQL queries as inputs for testing relational databases. We leverage the SAT-based Alloy tool-set to reduce the problem of generating valid SQL queries into a SAT problem. Our approach translates SQL query constraints into Alloy models, which enable it to generate valid queries that cannot be automatically generated using conventional grammar-based generators. Given a database schema, our new approach combined with our previous work on ADUSA, automatically generates (1) syntactically and semantically valid SQL queries for testing, (2) input data to populate test databases, and (3) expected result of executing the given query on the generated data. Experimental results show that not only can we automatically generate valid queries which detect bugs in database engines, but also we are able to combine this work with our previous work on ADUSA to automatically generate input queries and tables as well as expected query execution outputs to enable automated testing of database engines.",
    "year": 2010,
    "citationCount": 60,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1858996.1859063?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1858996.1859063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2993970",
        "name": "Shadi Abdul Khalek"
      },
      {
        "authorId": "145802044",
        "name": "S. Khurshid"
      }
    ]
  },
  {
    "paperId": "81b4273287561169d112e8c5413f677b89ec5d77",
    "title": "Math Spotting: Retrieving Math in Technical Documents Using Handwritten Query Images",
    "abstract": "A method for locating mathematical expressions in document images without the use of optical character recognition is presented. An index of document regions is produced from recursive X-Y trees produced for each page in the corpus. Queries are provided as images of handwritten expressions, for which an X-Y tree is computed. During retrieval, the query is looked up in the document region index using features of its X-Y tree, producing a set of candidate regions. Candidate regions are ranked by the similarity of vertical pixel projections in their upper and lower halves with those of the query image, as computed using Dynamic Time Warping of the image columns. In an experiment, ten participants each wrote twenty queries from a 200-page corpus. On average, the top-10 retrieval candidates included a candidate covering 43.3% of the test query image (\u03c3 = 14.0), with the correct page being returned between 30.0% and 85.0% of the time across participants (\u03bc = 63.2%, s = 14.9%). When testing using the original query images, 90.0% of the queries were retrieved correctly.",
    "year": 2011,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "http://www.cs.rit.edu/%7Erlaz/files/RetrievalICDAR2011.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2011.96?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2011.96, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1793699",
        "name": "R. Zanibbi"
      },
      {
        "authorId": "2149483869",
        "name": "Li Yu"
      }
    ]
  },
  {
    "paperId": "c3e8686609e5bfc250e3785cba9979c1d0a2db04",
    "title": "Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing",
    "abstract": "Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the DB schema can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8% to 39.4%, dramatically above the current state of the art, which is at 19.7%.",
    "year": 2019,
    "citationCount": 174,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/P19-1448.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1905.06241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "50757607",
        "name": "Ben Bogin"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "1750652",
        "name": "Jonathan Berant"
      }
    ]
  },
  {
    "paperId": "b1f442a2ff37d6fe52068fc51833e3d38e0376fd",
    "title": "rentrez: An R package for the NCBI eUtils API",
    "abstract": "The USA National Center for Biotechnology Information (NCBI) is one of the world\u2019s most important sources of biological information. NCBI databases like PubMed and GenBank contain millions of records describing bibliographic, genetic, genomic, and medical data. Here I present rentrez, a package which provides an R interface to 50 NCBI databases. The package is well-documented, contains an extensive suite of unit tests and has an active user base. The programmatic interface to the NCBI provided by rentrez allows researchers to query databases and download or import particular records into R sessions for subsequent analysis. The complete nature of the package, its extensive test-suite and the fact the package implements the NCBI\u2019s usage policies all make rentrez a powerful aid to developers of new packages that perform more specific tasks.",
    "year": 2017,
    "citationCount": 227,
    "openAccessPdf": {
      "url": "https://journal.r-project.org/archive/2017/RJ-2017-058/RJ-2017-058.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.7287/PEERJ.PREPRINTS.3179V2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7287/PEERJ.PREPRINTS.3179V2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "144434170",
        "name": "D. Winter"
      }
    ]
  },
  {
    "paperId": "e955330fec7945e84fe365dbf16f7aa833421ecd",
    "title": "Comparison of Different Approaches to Define the Applicability Domain of QSAR Models",
    "abstract": "One of the OECD principles for model validation requires defining the Applicability Domain (AD) for the QSAR models. This is important since the reliable predictions are generally limited to query chemicals structurally similar to the training compounds used to build the model. Therefore, characterization of interpolation space is significant in defining the AD and in this study some existing descriptor-based approaches performing this task are discussed and compared by implementing them on existing validated datasets from the literature. Algorithms adopted by different approaches allow defining the interpolation space in several ways, while defined thresholds contribute significantly to the extrapolations. For each dataset and approach implemented for this study, the comparison analysis was carried out by considering the model statistics and relative position of test set with respect to the training space.",
    "year": 2012,
    "citationCount": 451,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/1420-3049/17/5/4791/pdf?version=1403113677",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6268288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "authors": [
      {
        "authorId": "2800525",
        "name": "F. Sahigara"
      },
      {
        "authorId": "37309372",
        "name": "K. Mansouri"
      },
      {
        "authorId": "3030235",
        "name": "D. Ballabio"
      },
      {
        "authorId": "46224852",
        "name": "A. Mauri"
      },
      {
        "authorId": "2117741",
        "name": "V. Consonni"
      },
      {
        "authorId": "2236752",
        "name": "R. Todeschini"
      }
    ]
  },
  {
    "paperId": "0167fa54502ff948c7de8179d026e35a187d9c4c",
    "title": "Learning query-dependent prefilters for scalable image retrieval",
    "abstract": "We describe an algorithm for similar-image search which is designed to be efficient for extremely large collections of images. For each query, a small response set is selected by a fast prefilter, after which a more accurate ranker may be applied to each image in the response set. We consider a class of prefilters comprising disjunctions of conjunctions (\u201cORs of ANDs\u201d) of Boolean features. AND filters can be implemented efficiently using skipped inverted files, a key component of Web-scale text search engines. These structures permit search in time proportional to the response set size. The prefilters are learned from training examples, and refined at query time to produce an approximately bounded response set. We cast prefiltering as an optimization problem: for each test query, select the OR-of-AND filter which maximizes training-set recall for an adjustable bound on response set size. This may be efficiently implemented by selecting from a large pool of candidate conjunctions of Boolean features using a linear program relaxation. Tests on object class recognition show that this relatively simple filter is nevertheless powerful enough to capture some semantic information.",
    "year": 2009,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2009.5206582?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2009.5206582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1732879",
        "name": "L. Torresani"
      },
      {
        "authorId": "1778989",
        "name": "M. Szummer"
      },
      {
        "authorId": "47139824",
        "name": "A. Fitzgibbon"
      }
    ]
  },
  {
    "paperId": "a9b56e0521699cd45763d51b8e6cedd5df380888",
    "title": "LinkBench: a database benchmark based on the Facebook social graph",
    "abstract": "Database benchmarks are an important tool for database researchers and practitioners that ease the process of making informed comparisons between different database hardware, software and configurations. Large scale web services such as social networks are a major and growing database application area, but currently there are few benchmarks that accurately model web service workloads.\n In this paper we present a new synthetic benchmark called LinkBench. LinkBench is based on traces from production databases that store \"social graph\" data at Facebook, a major social network. We characterize the data and query workload in many dimensions, and use the insights gained to construct a realistic synthetic benchmark. LinkBench provides a realistic and challenging test for persistent storage of social and web service data, filling a gap in the available tools for researchers, developers and administrators.",
    "year": 2013,
    "citationCount": 332,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2463676.2465296?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2463676.2465296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2618428",
        "name": "Timothy G. Armstrong"
      },
      {
        "authorId": "2151776",
        "name": "Vamsi Ponnekanti"
      },
      {
        "authorId": "50499227",
        "name": "Dhruba Borthakur"
      },
      {
        "authorId": "35651670",
        "name": "Mark D. Callaghan"
      }
    ]
  },
  {
    "paperId": "3c452211d3cea26b58b46fefc5c63edb9a0e7b8e",
    "title": "Mining search engine query logs for query recommendation",
    "abstract": "This paper presents a simple and intuitive method for mining search engine query logs to get fast query recommendations on a large scale industrial strength search engine. In order to get a more comprehensive solution, we combine two methods together. On the one hand, we study and model search engine users' sequential search behavior, and interpret this consecutive search behavior as client-side query refinement, that should form the basis for the search engine's own query refinement process. On the other hand, we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data, and more specifically, the shortness of most query sessions. To evaluate our method, we use one hundred day worth query logs from SINA' search engine to do off-line mining. Then we analyze three independent editors evaluations on a query test set. Based on their judgement, our method was found to be effective for finding related queries, despite its simplicity. In addition to the subjective editors' rating, we also perform tests based on actual anonymous user search sessions.",
    "year": 2006,
    "citationCount": 236,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1135777.1136004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1135777.1136004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "2117992691",
        "name": "Zhiyong Zhang"
      },
      {
        "authorId": "2423522",
        "name": "O. Nasraoui"
      }
    ]
  },
  {
    "paperId": "8c3b183972f98c224103e62eb5f3ddd878484c87",
    "title": "New direct-product testers and 2-query PCPs",
    "abstract": "The \"direct product code\" of a function f gives its values on all k-tuples (f(x1),...,f(xk)). This basic construct underlies \"hardness amplification\" in cryptography, circuit complexity and PCPs. Goldreich and Safra [12] pioneered its local testing and its PCP application. A recent result by Dinur and Goldenberg [5] enabled for the first time testing proximity to this important code in the \"list-decoding\" regime. In particular, they give a 2-query test which works for polynomially small success probability 1/k\u03b1, and show that no such test works below success probability 1/k. Our main result is a 3-query test which works for exponentially small success probability exp(-k\u03b1). Our techniques (based on recent simplified decoding algorithms for the same code [15]) also allow us to considerably simplify the analysis of the 2-query test of [5]. We then show how to derandomize their test, achieving a code of polynomial rate, independent of k, and success probability 1/k\u03b1. Finally we show the applicability of the new tests to PCPs. Starting with a 2-query PCP over an alphabet \u03a3 and with soundness error 1-\u03b4, Rao [19] (building on Raz's (k-fold) parallel repetition theorem [20] and Holenstein's proof [13]) obtains a new 2-query PCP over the alphabet \u03a3k with soundness error exp(-\u03b42 k). Our techniques yield a 2-query PCP with soundness error exp(-\u03b4 \u221ak). Our PCP construction turns out to be essentially the same as the miss-match proof system defined and analyzed by Feige and Kilian [8], but with simpler analysis and exponentially better soundness error.",
    "year": 2009,
    "citationCount": 67,
    "openAccessPdf": {
      "url": "http://www.math.ias.edu/~avi/PUBLICATIONS/MYPAPERS/IKW09/IKW09.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1536414.1536435?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1536414.1536435, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "authors": [
      {
        "authorId": "1735759",
        "name": "R. Impagliazzo"
      },
      {
        "authorId": "2347317",
        "name": "Valentine Kabanets"
      },
      {
        "authorId": "1718867",
        "name": "A. Wigderson"
      }
    ]
  },
  {
    "paperId": "03335154cc2332ec04c11c4f273c342cd16dd4b4",
    "title": "Efficient query processing on graph databases",
    "abstract": "We study the problem of processing subgraph queries on a database that consists of a set of graphs. The answer to a subgraph query is the set of graphs in the database that are supergraphs of the query. In this article, we propose an efficient index, FG*-index, to solve this problem.\n The cost of processing a subgraph query using most existing indexes mainly consists of two parts: the index probing cost and the candidate verification cost. Index probing is to find the query in the index, or to find the graphs from which we can generate a candidate answer set for the query. Candidate verification is to test whether each graph in the candidate set is indeed a supergraph of the query. We design FG*-index to minimize these two costs as follows.\n FG*-index consists of three components: the FG-index, the feature-index, and the FAQ-index. First, the FG-index employs the concept of Frequent subGraph (FG) to allow the set of queries that are FGs to be answered without candidate verification. We call this set of queries FG-queries. We can enlarge the set of FG-queries so that more queries can be answered without candidate verification; however, a larger set of FG-queries implies a larger FG-index and hence the index probing cost also increases. We propose the feature-index to reduce the index probing cost. The feature-index uses features to filter false results that are matched in the FG-index, so that we can quickly find the truly matching graphs for a query. For processing non-FG-queries, we propose the FAQ-index, which is dynamically constructed from the set of Frequently Asked non-FG-Queries (FAQs). Using the FAQ-index, verification is not required for processing FAQs and only a small number of candidates need to be verified for processing non-FG-queries that are not frequently asked. Finally, a comprehensive set of experiments verifies that query processing using FG*-index is up to orders of magnitude more efficient than state-of-the-art indexes and it is also more scalable.",
    "year": 2009,
    "citationCount": 67,
    "openAccessPdf": {
      "url": "http://www.cse.ust.hk/faculty/wilfred/paper/tods08b.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1508857.1508859?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1508857.1508859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "1717691",
        "name": "James Cheng"
      },
      {
        "authorId": "2636068",
        "name": "Yiping Ke"
      },
      {
        "authorId": "1695492",
        "name": "Wilfred Ng"
      }
    ]
  },
  {
    "paperId": "b73a9d4e42dbdd0c8320231111c2bdc8243c6d3a",
    "title": "Fast Video Moment Retrieval",
    "abstract": "This paper targets at fast video moment retrieval (fast VMR), aiming to localize the target moment efficiently and accurately as queried by a given natural language sentence. We argue that most existing VMR approaches can be divided into three modules namely video encoder, text encoder, and cross-modal interaction module, where the last module is the test-time computational bottleneck. To tackle this issue, we replace the cross-modal interaction module with a cross-modal common space, in which moment-query alignment is learned and efficient moment search can be performed. For the sake of robustness in the learned space, we propose a fine-grained semantic distillation framework to transfer knowledge from additional semantic structures. Specifically, we build a semantic role tree that decomposes a query sentence into different phrases (subtrees). A hierarchical semantic-guided attention module is designed to perform message propagation across the whole tree and yield discriminative features. Finally, the important and discriminative semantics are transferred to the common space by a matching-score distillation process. Extensive experimental results on three popular VMR benchmarks demonstrate that our proposed method enjoys the merits of high speed and significant performance.",
    "year": 2021,
    "citationCount": 91,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/iccv48922.2021.00155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/iccv48922.2021.00155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "46930271",
        "name": "Junyu Gao"
      },
      {
        "authorId": "145194969",
        "name": "Changsheng Xu"
      }
    ]
  },
  {
    "paperId": "8fbb8885854ba611ca90a7fc8b8efaaff3efc66e",
    "title": "TREC CAsT 2019: The Conversational Assistance Track Overview",
    "abstract": "The Conversational Assistance Track (CAsT) is a new track for TREC 2019 to facilitate Conversational Information Seeking (CIS) research and to create a large-scale reusable test collection for conversational search systems. The document corpus is 38,426,252 passages from the TREC Complex Answer Retrieval (CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty information seeking dialogues (30 train, 50 test) are an average of 9 to 10 questions long. Relevance assessments are provided for 30 training topics and 20 test topics. This year 21 groups submitted a total of 65 runs using varying methods for conversational query understanding and ranking. Methods include traditional retrieval based methods, feature based learning-to-rank, neural models, and knowledge enhanced methods. A common theme through the runs is the use of BERT-based neural reranking methods. Leading methods also employed document expansion, conversational query expansion, and generative language models for conversational query rewriting (GPT-2). The results show a gap between automatic systems and those using the manually resolved utterances, with a 35% relative improvement of manual rewrites over the best automatic system.",
    "year": 2020,
    "citationCount": 122,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2003.13624, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "authors": [
      {
        "authorId": "145269114",
        "name": "Jeffrey Dalton"
      },
      {
        "authorId": "144628574",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "144987107",
        "name": "Jamie Callan"
      }
    ]
  },
  {
    "paperId": "79f537759e7f1e0674a1dd6941ec5912528cd234",
    "title": "Research Paper: A General Natural-language Text Processor for Clinical Radiology",
    "abstract": "OBJECTIVE\nDevelopment of a general natural-language processor that identifies clinical information in narrative reports and maps that information into a structured representation containing clinical terms.\n\n\nDESIGN\nThe natural-language processor provides three phases of processing, all of which are driven by different knowledge sources. The first phase performs the parsing. It identifies the structure of the text through use of a grammar that defines semantic patterns and a target form. The second phase, regularization, standardizes the terms in the initial target structure via a compositional mapping of multi-word phrases. The third phase, encoding, maps the terms to a controlled vocabulary. Radiology is the test domain for the processor and the target structure is a formal model for representing clinical information in that domain.\n\n\nMEASUREMENTS\nThe impression sections of 230 radiology reports were encoded by the processor. Results of an automated query of the resultant database for the occurrences of four diseases were compared with the analysis of a panel of three physicians to determine recall and precision.\n\n\nRESULTS\nWithout training specific to the four diseases, recall and precision of the system (combined effect of the processor and query generator) were 70% and 87%. Training of the query component increased recall to 85% without changing precision.",
    "year": 1994,
    "citationCount": 753,
    "openAccessPdf": {
      "url": "https://europepmc.org/articles/pmc116194?pdf=render",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1136/jamia.1994.95236146?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1136/jamia.1994.95236146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "authors": [
      {
        "authorId": "145133587",
        "name": "C. Friedman"
      },
      {
        "authorId": "2770406",
        "name": "P. Alderson"
      },
      {
        "authorId": "145416160",
        "name": "J. Austin"
      },
      {
        "authorId": "144586369",
        "name": "J. Cimino"
      },
      {
        "authorId": "47163349",
        "name": "Stephen B. Johnson"
      }
    ]
  },
  {
    "paperId": "12db2109ac017929aeb2a605181dbc06b6cdb87b",
    "title": "Memory efficient subsequence DTW for Query-by-Example Spoken Term Detection",
    "abstract": "In this paper we propose a fast and memory efficient Dynamic Time Warping (MES-DTW) algorithm for the task of Query-by-Example Spoken Term Detection (QbE-STD). The proposed algorithm is based on the subsequence-DTW (S-DTW) algorithm, which allows the search for small spoken queries within a much bigger search collection of spoken documents by considering fixed start-end points in the query and discovering optimal matching subsequences along the search collection. The proposed algorithm applies some modifications to S-DTW that make it better suited for the QbE-STD task, including a way to perform the matching with virtually no system memory, optimal when querying large scale databases. We also describe the system used to perform QbE-STD, including an energy-based quantification for speech/non-speech detection and an overlap detector for putative matches. We test the system proposed using the Mediaeval 2012 spoken-web-search dataset and show that, in addition to the memory savings, the proposed algorithm brings an advantage in terms of matching accuracy (up to 0.235 absolute MTWV increase) and speed (around 25% faster) in comparison to the original S-DTW.",
    "year": 2013,
    "citationCount": 50,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICME.2013.6607546?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICME.2013.6607546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2013-07-15",
    "authors": [
      {
        "authorId": "1727017",
        "name": "Xavier Anguera Mir\u00f3"
      },
      {
        "authorId": "2268018",
        "name": "Miquel Ferrarons"
      }
    ]
  },
  {
    "paperId": "19052318fd778eebbf690d71190b2b9a2f4a3551",
    "title": "Implementation of logical query languages for databases",
    "abstract": "We examine methods of implementing queries about relational databases in the case where these queries are expressed in first-order logic as a collection of Horn clauses. Because queries may be defined recursively, straightforward methods of query evaluation do not always work, and a variety of strategies have been proposed to handle subsets of recursive queries. We express such query evaluation techniques as \u201ccapture rules\u201d on a graph representing clauses and predicates. One essential property of capture rules is that they can be applied independently, thus providing a clean interface for query-evaluation systems that use several different strategies in different situations. Another is that there be an efficient test for the applicability of a given rule. We define basic capture rules corresponding to application of operators from relational algebra, a top-down capture rule corresponding to \u201cbackward chaining,\u201d that is, repeated resolution of goals, a bottom-up rule, corresponding to \u201cforward chaining,\u201d where we attempt to deduce all true facts in a given class, and a \u201csideways\u201d rule that allows us to pass results from one goal to another.",
    "year": 1985,
    "citationCount": 407,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3979.3980",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3979.3980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3979.3980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1985-09-01",
    "authors": [
      {
        "authorId": "1742391",
        "name": "J. Ullman"
      }
    ]
  },
  {
    "paperId": "ea775c61144a28136239f1edfa09b6fedd571db0",
    "title": "Unsupervised query segmentation using generative language models and wikipedia",
    "abstract": "In this paper, we propose a novel unsupervised approach to query segmentation, an important task in Web search. We use a generative query model to recover a query's underlying concepts that compose its original segmented form. The model's parameters are estimated using an expectation-maximization (EM) algorithm, optimizing the minimum description length objective function on a partial corpus that is specific to the query. To augment this unsupervised learning, we incorporate evidence from Wikipedia.\n Experiments show that our approach dramatically improves performance over the traditional approach that is based on mutual information, and produces comparable results with a supervised method. In particular, the basic generative language model contributes a 7.4% improvement over the mutual information based method (measured by segment F1 on the Intersection test set). EM optimization further improves the performance by 14.3%. Additional knowledge from Wikipedia provides another improvement of 24.3%, adding up to a total of 46% improvement (from 0.530 to 0.774).",
    "year": 2008,
    "citationCount": 173,
    "openAccessPdf": {
      "url": "http://www.cse.iitb.ac.in/~soumen/doc/www2013/QirWoo/TanP2008QuerySegment.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1367497.1367545?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1367497.1367545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2008-04-21",
    "authors": [
      {
        "authorId": "143645439",
        "name": "Bin Tan"
      },
      {
        "authorId": "1682136",
        "name": "Fuchun Peng"
      }
    ]
  },
  {
    "paperId": "5a3004260122f029bd672826e49a54f4931977c7",
    "title": "The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval",
    "abstract": "In this study, the effects of query structure and various setups of translation dictionaries on the performance of cross-language information retrieval (CLIR) were tested. The document collection was a subset of the TREC collection, and as test requests the study used TREC's health related topics. The test system was the INQUERY retrieval system. The performance of translated Finnish queries against English documents was compared to the performance of original English queries against English documents. Four natural language query types and three query translation methods, using a general dictionary and a domain specific (= medical) dictionary, were studied. There was only a slight difference in performance between the original English queries and the best crosslanguage queries, i.e., structured queries with medical dictionary and general dictionary translation. The structuring of queries was done on the basis of the output of dictionaries.",
    "year": 1998,
    "citationCount": 279,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/290941.290957?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/290941.290957, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1998-08-01",
    "authors": [
      {
        "authorId": "2388589",
        "name": "Ari Pirkola"
      }
    ]
  },
  {
    "paperId": "bfde3d73c1df8980a0c3915e627236ce818d42c7",
    "title": "The Predictive Power of Google Searches in Forecasting Unemployment",
    "abstract": "We suggest the use of an index of Internet job-search intensity (the Google Index, GI) as the best leading indicator to predict the US monthly unemployment rate. We perform a deep out-of-sample forecasting comparison analyzing many models that adopt our preferred leading indicator (GI), the more standard initial claims or combinations of both. We find that models augmented with the GI outperform the traditional ones in predicting the unemployment rate for different out-of-sample intervals that start before, during and after the Great Recession. Google-based models also outperform standard ones in most state-level forecasts and in comparison with the Survey of Professional Forecasters. These results survive a falsification test and are also confirmed when employing different keywords. Based on our results for the unemployment rate, we believe that there will be an increasing number of applications using Google query data in other fields of economics.",
    "year": 2012,
    "citationCount": 402,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.2139/SSRN.2207915?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2139/SSRN.2207915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Economics"
    ],
    "publicationDate": "2012-11-29",
    "authors": [
      {
        "authorId": "1403763535",
        "name": "F. D\u2019Amuri"
      },
      {
        "authorId": "88021628",
        "name": "Juri Marcucci"
      }
    ]
  },
  {
    "paperId": "e67a2817089312746d69b38ce9abfdc4b1bc69c3",
    "title": "Finding bugs in Gremlin-based graph database systems via Randomized differential testing",
    "abstract": "Graph database systems (GDBs) allow efficiently storing and retrieving graph data, and have become the critical component in many applications, e.g., knowledge graphs, social networks, and fraud detection. It is important to ensure that GDBs operate correctly. Logic bugs can occur and make GDBs return an incorrect result for a given query. These bugs are critical and can easily go unnoticed by developers when the graph and queries become complicated. Despite the importance of GDBs, logic bugs in GDBs have received less attention than those in relational database systems. In this paper, we present Grand, an approach for automatically finding logic bugs in GDBs that adopt Gremlin as their query language. The core idea of Grand is to construct semantically equivalent databases for multiple GDBs, and then compare the results of a Gremlin query on these databases. If the return results of a query on multiple GDBs are different, the likely cause is a logic bug in these GDBs. To effectively test GDBs, we propose a model-based query generation approach to generate valid Gremlin queries that can potentially return non-empty results, and a data mapping approach to unify the format of query results for different GDBs. We evaluate Grand on six widely-used GDBs, e.g., Neo4j and HugeGraph. In total, we have found 21 previously-unknown logic bugs in these GDBs. Among them, developers have confirmed 18 bugs, and fixed 7 bugs.",
    "year": 2022,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3533767.3534409?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3533767.3534409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2022-07-18",
    "authors": [
      {
        "authorId": "2158585032",
        "name": "Yingying Zheng"
      },
      {
        "authorId": "2964640",
        "name": "Wensheng Dou"
      },
      {
        "authorId": "134898163",
        "name": "Yicheng Wang"
      },
      {
        "authorId": "2093481779",
        "name": "Zheng Qin"
      },
      {
        "authorId": "2131285720",
        "name": "Leile Tang"
      },
      {
        "authorId": "2118120527",
        "name": "Yu Gao"
      },
      {
        "authorId": "2152692124",
        "name": "Dong Wang"
      },
      {
        "authorId": "40231586",
        "name": "Wei Wang"
      },
      {
        "authorId": "144525882",
        "name": "Jun Wei"
      }
    ]
  },
  {
    "paperId": "9d1aef4325f73bc5d815db1eb0b2ecb5d0909037",
    "title": "A data model and query language for EXODUS",
    "abstract": "In this paper, we present the design of the EXTRA data model and the EXCESS query language for the EXODUS extensible database system. The EXTRA data model includes support for complex objects with shared subobjects, a novel mix of object- and value-oriented semantics for data, support for persistent objects of any type in the EXTRA type lattice, and user-defined abstract data types (ADTs). The EXCESS query language provides facilities for querying and updating complex object structures, and it can be extended through the addition of ADT functions and operators, procedures and functions for manipulating EXTRA schema types, and generic set functions EXTRA and EXCESS are intended to serve as a test vehicle for tools developed under the EXODUS extensible database system project.",
    "year": 1988,
    "citationCount": 341,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/971701.50252",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/50202.50252?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/50202.50252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1988-06-01",
    "authors": [
      {
        "authorId": "1703347",
        "name": "M. Carey"
      },
      {
        "authorId": "1765659",
        "name": "D. DeWitt"
      },
      {
        "authorId": "2060722884",
        "name": "Scott L. Vandenberg"
      }
    ]
  },
  {
    "paperId": "218d939f79bddf36973cc8f7e464dbc9958c3208",
    "title": "A statistical comparison of tag and query logs",
    "abstract": "We investigate tag and query logs to see if the terms people use to annotate websites are similar to the ones they use to query for them. Over a set of URLs, we compare the distribution of tags used to annotate each URL with the distribution of query terms for clicks on the same URL. Understanding the relationship between the distributions is important to determine how useful tag data may be for improving search results and conversely, query data for improving tag prediction. In our study, we compare both term frequency distributions using vocabulary overlap and relative entropy. We also test statistically whether the term counts come from the same underlying distribution. Our results indicate that the vocabulary used for tagging and searching for content are similar but not identical. We further investigate the content of the websites to see which of the two distributions (tag or query) is most similar to the content of the annotated/searched URL. Finally, we analyze the similarity for different categories of URLs in our sample to see if the similarity between distributions is dependent on the topic of the website or the popularity of the URL.",
    "year": 2009,
    "citationCount": 42,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1571941.1571965?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1571941.1571965, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-07-19",
    "authors": [
      {
        "authorId": "2170598",
        "name": "Mark James Carman"
      },
      {
        "authorId": "145288124",
        "name": "M. Baillie"
      },
      {
        "authorId": "3241997",
        "name": "Robert Gwadera"
      },
      {
        "authorId": "145876066",
        "name": "F. Crestani"
      }
    ]
  },
  {
    "paperId": "a7938b5ca85327b9804da7f674a1a3634c1e18c3",
    "title": "Identifying Insects with Incomplete DNA Barcode Libraries, African Fruit Flies (Diptera: Tephritidae) as a Test Case",
    "abstract": "We propose a general working strategy to deal with incomplete reference libraries in the DNA barcoding identification of species. Considering that (1) queries with a large genetic distance with their best DNA barcode match are more likely to be misidentified and (2) imposing a distance threshold profitably reduces identification errors, we modelled relationships between identification performances and distance thresholds in four DNA barcode libraries of Diptera (n\u200a=\u200a4270), Lepidoptera (n\u200a=\u200a7577), Hymenoptera (n\u200a=\u200a2067) and Tephritidae (n\u200a=\u200a602 DNA barcodes). In all cases, more restrictive distance thresholds produced a gradual increase in the proportion of true negatives, a gradual decrease of false positives and more abrupt variations in the proportions of true positives and false negatives. More restrictive distance thresholds improved precision, yet negatively affected accuracy due to the higher proportions of queries discarded (viz. having a distance query-best match above the threshold). Using a simple linear regression we calculated an ad hoc distance threshold for the tephritid library producing an estimated relative identification error <0.05. According to the expectations, when we used this threshold for the identification of 188 independently collected tephritids, less than 5% of queries with a distance query-best match below the threshold were misidentified. Ad hoc thresholds can be calculated for each particular reference library of DNA barcodes and should be used as cut-off mark defining whether we can proceed identifying the query with a known estimated error probability (e.g. 5%) or whether we should discard the query and consider alternative/complementary identification methods.",
    "year": 2012,
    "citationCount": 87,
    "openAccessPdf": {
      "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0031581&type=printable",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC3281081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "publicationDate": "2012-02-16",
    "authors": [
      {
        "authorId": "2610750",
        "name": "M. Virgilio"
      },
      {
        "authorId": "3814932",
        "name": "K. Jordaens"
      },
      {
        "authorId": "6537332",
        "name": "F. Breman"
      },
      {
        "authorId": "2665345",
        "name": "T. Backeljau"
      },
      {
        "authorId": "32337015",
        "name": "M. De Meyer"
      }
    ]
  },
  {
    "paperId": "24c7266096b640a993d5eaada9e143f0bfd6a507",
    "title": "Identification of Web Query Intent Based on Query Text and Web Knowledge",
    "abstract": "In this paper, we propose a novel approach to identifying user intents of search engine queries. Specifically, we recast it as a classification problem, in which four types of features are adopted. The classification features are based on deep linguistic analysis of queries as well as search engine feedbacks. We evaluate the method with the real web query data. The results show that about 88% of the test queries can be correctly identified with the classification framework via combining all the 4 types of features.",
    "year": 2010,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/PCSPA.2010.40?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/PCSPA.2010.40, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2010-09-17",
    "authors": [
      {
        "authorId": "2118208508",
        "name": "Dayong Wu"
      },
      {
        "authorId": "1682848",
        "name": "Yu Zhang"
      },
      {
        "authorId": "7420210",
        "name": "Shiqi Zhao"
      },
      {
        "authorId": "40282288",
        "name": "Ting Liu"
      }
    ]
  },
  {
    "paperId": "64385020f203451bd60d58d899e98e8b27060f0a",
    "title": "Query interactions in database workloads",
    "abstract": "Database workloads consist of mixes of queries that run concurrently and interact with each other. In this paper, we demonstrate that query interactions can have a significant impact on database system performance. Hence, we argue that it is important to take these interactions into account when characterizing workloads, designing test cases, or developing performance tuning algorithms for database systems. To capture and model query interactions, we propose using an experimental approach that is based on sampling the space of possible interactions and fitting statistical models to the sampled data. We discuss using such an approach for database testing and tuning, and we present some opportunities and research challenges.",
    "year": 2009,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1594156.1594170?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1594156.1594170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-06-29",
    "authors": [
      {
        "authorId": "2110092588",
        "name": "Mumtaz Ahmad"
      },
      {
        "authorId": "1704622",
        "name": "Ashraf Aboulnaga"
      },
      {
        "authorId": "145664618",
        "name": "S. Babu"
      }
    ]
  },
  {
    "paperId": "1bc98849d82eef5e2b72b9d996dd37cbd9abd4ab",
    "title": "Differentially Private SQL with Bounded User Contribution",
    "abstract": "Abstract Differential privacy (DP) provides formal guarantees that the output of a database query does not reveal too much information about any individual present in the database. While many differentially private algorithms have been proposed in the scientific literature, there are only a few end-to-end implementations of differentially private query engines. Crucially, existing systems assume that each individual is associated with at most one database record, which is unrealistic in practice. We propose a generic and scalable method to perform differentially private aggregations on databases, even when individuals can each be associated with arbitrarily many rows. We express this method as an operator in relational algebra, and implement it in an SQL engine. To validate this system, we test the utility of typical queries on industry benchmarks, and verify its correctness with a stochastic test framework we developed. We highlight the promises and pitfalls learned when deploying such a system in practice, and we publish its core components as open-source software.",
    "year": 2019,
    "citationCount": 145,
    "openAccessPdf": {
      "url": "https://www.sciendo.com/pdf/10.2478/popets-2020-0025",
      "status": "HYBRID",
      "license": "CCBYNCND",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1909.01917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2019-09-04",
    "authors": [
      {
        "authorId": "2109033581",
        "name": "Royce J. Wilson"
      },
      {
        "authorId": "2109269408",
        "name": "Celia Yuxin Zhang"
      },
      {
        "authorId": "143995949",
        "name": "William K. C. Lam"
      },
      {
        "authorId": "3144478",
        "name": "Damien Desfontaines"
      },
      {
        "authorId": "1410036201",
        "name": "Daniel Simmons-Marengo"
      },
      {
        "authorId": "1403821598",
        "name": "Bryant Gipson"
      }
    ]
  },
  {
    "paperId": "b66f539e116dcf6aa8444bce9f7beb62f0e2de74",
    "title": "Cross Modal Retrieval with Querybank Normalisation",
    "abstract": "Profiting from large-scale training datasets, advances in neural architecture design and efficient inference, joint embeddings have become the dominant approach for tackling cross-modal retrieval. In this work we first show that, despite their effectiveness, state-of-the-art joint embeddings suffer significantly from the longstanding \u201chubness problem\u201d in which a small number of gallery embeddings form the nearest neighbours of many queries. Drawing inspiration from the NLP literature, we formulate a simple but effective framework called Querybank Normalisation (QB-NORM) that re-normalises query similarities to account for hubs in the embedding space. QB-NORM improves retrieval performance without requiring retraining. Differently from prior work, we show that QB-NORM works effectively without concurrent access to any test set queries. Within the QB-NORM framework, we also propose a novel similarity normalisation method, the Dynamic Inverted Softmax, that is significantly more robust than existing approaches. We showcase QB-NORM across a range of cross modal retrieval models and benchmarks where it consistently enhances strong baselines beyond the state of the art. Code is available at https://vladbogo.github.io/QB-Norm/.",
    "year": 2021,
    "citationCount": 80,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2112.12777",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2112.12777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2021-12-23",
    "authors": [
      {
        "authorId": "9947219",
        "name": "Simion-Vlad Bogolin"
      },
      {
        "authorId": "50272388",
        "name": "Ioana Croitoru"
      },
      {
        "authorId": "41151701",
        "name": "Hailin Jin"
      },
      {
        "authorId": "2152797548",
        "name": "Yang Liu"
      },
      {
        "authorId": "7641268",
        "name": "Samuel Albanie"
      }
    ]
  },
  {
    "paperId": "6be44364db3a46ab5fcf8172910650b210cc5c39",
    "title": "Black-box Adversarial Attacks on Video Recognition Models",
    "abstract": "Deep neural networks (DNNs) are known for their vulnerability to adversarial examples. These are examples that have undergone small, carefully crafted perturbations, and which can easily fool a DNN into making misclassifications at test time. Thus far, the field of adversarial research has mainly focused on image models, under either a white-box setting, where an adversary has full access to model parameters, or a black-box setting where an adversary can only query the target model for probabilities or labels. Whilst several white-box attacks have been proposed for video models, black-box video attacks are still unexplored. To close this gap, we propose the first black-box video attack framework, called V-BAD. V-BAD utilizestentative perturbations transferred from image models andpartition-based rectifications found by the NES to obtain good adversarial gradient estimates with fewer queries to the target model. V-BAD is equivalent to estimating the projection of the adversarial gradient on a selected subspace. Using three benchmark video datasets, we demonstrate that V-BAD can craft both untargeted and targeted attacks to fool two state-of-the-art deep video recognition models. For the targeted attack, it achieves $>$93% success rate using only an average of $3.4 \\sim 8.4 \\times 10^4$ queries, a similar number of queries to state-of-the-art black-box image attacks. This is despite the fact that videos often have two orders of magnitude higher dimensionality than static images. We believe that V-BAD is a promising new tool to evaluate and improve the robustness of video recognition models to black-box adversarial attacks.",
    "year": 2019,
    "citationCount": 139,
    "openAccessPdf": {
      "url": "https://dro.deakin.edu.au/articles/conference_contribution/Black-box_adversarial_attacks_on_video_recognition_models/20701987/2/files/36915334.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1904.05181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2019-04-10",
    "authors": [
      {
        "authorId": "108526754",
        "name": "Linxi Jiang"
      },
      {
        "authorId": "9576855",
        "name": "Xingjun Ma"
      },
      {
        "authorId": "2118434440",
        "name": "Shaoxiang Chen"
      },
      {
        "authorId": "145148600",
        "name": "J. Bailey"
      },
      {
        "authorId": "1717861",
        "name": "Yu-Gang Jiang"
      }
    ]
  },
  {
    "paperId": "9ce41d1899d51d0eddcdfa0bf5b879875968419e",
    "title": "Context-sensitive information retrieval using implicit feedback",
    "abstract": "A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
    "year": 2005,
    "citationCount": 550,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1076034.1076045?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1076034.1076045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2005-08-15",
    "authors": [
      {
        "authorId": "2900084",
        "name": "Xuehua Shen"
      },
      {
        "authorId": "143645439",
        "name": "Bin Tan"
      },
      {
        "authorId": "143869012",
        "name": "Chengxiang Zhai"
      }
    ]
  },
  {
    "paperId": "c66bc947546118bbe78922a0faa91c6097fb707f",
    "title": "Databugger: a test-driven framework for debugging the web of data",
    "abstract": "Linked Open Data (LOD) comprises of an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowd-sourced or extracted data of often relatively low quality. We present Databugger, a framework for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. Databugger ensures a basic level of quality by accompanying vocabularies, ontologies and knowledge bases with a number of test cases. The formalization behind the tool employs SPARQL query templates, which are instantiated into concrete quality test queries. The test queries can be instantiated automatically based on a vocabulary or manually based on the data semantics. One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.",
    "year": 2014,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2567948.2577017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2567948.2577017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2014-04-07",
    "authors": [
      {
        "authorId": "2627116",
        "name": "D. Kontokostas"
      },
      {
        "authorId": "50453446",
        "name": "Patrick Westphal"
      },
      {
        "authorId": "145044578",
        "name": "S. Auer"
      },
      {
        "authorId": "2024066",
        "name": "Sebastian Hellmann"
      },
      {
        "authorId": "144568027",
        "name": "Jens Lehmann"
      },
      {
        "authorId": "39494666",
        "name": "R. Cornelissen"
      }
    ]
  },
  {
    "paperId": "98dbd1bcbc36bd1d219b8da019a5854765784ca3",
    "title": "Using controlled query generation to evaluate blind relevance feedback algorithms",
    "abstract": "Currently in document retrieval there are many algorithms each with different strengths and weakness. There is some difficulty, however, in evaluating the impact of the test query set on retrieval results. The traditional evaluation process, the Cranfield evaluation paradigm, which uses a corpus and a set of user queries, focuses on making the queries as realistic as possible. Unfortunately such query sets lack the fine grained control necessary to test algorithm properties. We present an approach called controlled query generation (CQG) that creates query sets from documents in the corpus in a way that regulates the theoretic information quality of each query. This allows us to generate reproducible and well defined sets of queries of varying length and term specificity. Imposing this level of control over the query sets used for testing retrieval algorithms enables the rigorous simulation of different query environments to identify specific algorithm properties before introducing user queries. In this work, we demonstrate the usefulness of CQG by generating three different query environments to investigate characteristics of two blind relevance feedback approaches",
    "year": 2006,
    "citationCount": 53,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1141753.1141818?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1141753.1141818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2006-06-11",
    "authors": [
      {
        "authorId": "2053283925",
        "name": "Christopher T. Jordan"
      },
      {
        "authorId": "1759624",
        "name": "C. Watters"
      },
      {
        "authorId": "3212432",
        "name": "Q. Gao"
      }
    ]
  },
  {
    "paperId": "e672a5b078b0ba33e7f9a89fb7318628148e24a3",
    "title": "Ranking robustness: a novel framework to predict query performance",
    "abstract": "In this paper, we introduce the notion of ranking robustness, which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncertainty in the ranked documents. We propose a statistical measure called the robustness score to quantify this notion. We demonstrate that the robustness score significantly and consistently correlates with query performance in a variety of TREC test collections including the GOV2 collection. We compare the robustness score with the clarity score method which is the state-of-the-art technique for query performance prediction. Our experimental results show that the robustness score performs better than or at least as good as the clarity score. We find that the clarity score is barely correlated with query performance on the GOV2 collection while the correlation between the robustness score and query performance remains significant. We also notice that a combination of the two usually results in more prediction power.",
    "year": 2006,
    "citationCount": 154,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1183614.1183696?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1183614.1183696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2006-11-06",
    "authors": [
      {
        "authorId": "2118116642",
        "name": "Yun Zhou"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      }
    ]
  },
  {
    "paperId": "5d4b617364fa5ce608f9794384c7d9677ba2cf4a",
    "title": "Query flocks: a generalization of association-rule mining",
    "abstract": "Association-rule mining has proved a highly successful technique for extracting useful information from very large databases. This success is attributed not only to the appropriateness of the objectives, but to the fact that a number of new query-optimization ideas, such as the \u201ca-priori\u201d trick, make association-rule mining run much faster than might be expected. In this paper we see that the same tricks can be extended to a much more general context, allowing efficient mining of very large databases for many different kinds of patterns. The general idea, called \u201cquery flocks,\u201d is a generate-and-test model for data-mining problems. We show how the idea can be used either in a general-purpose mining system or in a next generation of conventional query optimizers.",
    "year": 1998,
    "citationCount": 208,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/276304.276306?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/276304.276306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1998-06-01",
    "authors": [
      {
        "authorId": "1696775",
        "name": "S. Tsur"
      },
      {
        "authorId": "1742391",
        "name": "J. Ullman"
      },
      {
        "authorId": "69026873",
        "name": "S. Abiteboul"
      },
      {
        "authorId": "2935513",
        "name": "Chris Clifton"
      },
      {
        "authorId": "84095744",
        "name": "R. Motwani"
      },
      {
        "authorId": "1771093",
        "name": "Svetlozar Nestorov"
      },
      {
        "authorId": "144425383",
        "name": "A. Rosenthal"
      }
    ]
  },
  {
    "paperId": "64cabac9ca53a6a31dac409d2d8c5a69c5a7da40",
    "title": "The effect multiple query representations on information retrieval system performance",
    "abstract": "Five independently generated Boolean query formulations for ten different TREC topics were produced by ten different expert online searchers. These different formulations were grouped, and the groups, and combinations of them, were used as searches against the TREC test collection, using the INQUERY probabilistic inference network retrieval engine, Results show that progressive combination of query formulations leads to progressively improving retrieval performance, Results were compared against the performance of INQUERY natural language based queries, and in combination with them. The issue of recall as a performance measure in large databases was raised, since overlap between the searches conducted in this study, and the TREC-1 searches, was smaller than expected.",
    "year": 1993,
    "citationCount": 212,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/160688.160760?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/160688.160760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1993-07-01",
    "authors": [
      {
        "authorId": "2095083",
        "name": "N. Belkin"
      },
      {
        "authorId": "145292542",
        "name": "Colleen Cool"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      },
      {
        "authorId": "144987107",
        "name": "Jamie Callan"
      }
    ]
  },
  {
    "paperId": "a3995d3ed5673effddc78b749c29da1d2e2fe9f6",
    "title": "Reverse Query Processing",
    "abstract": "Generating databases for testing database applications (e.g., OLAP or business objects) is a daunting task in practice. There are a number of commercial tools to automatically generate test databases. These tools take a database schema (table layouts plus integrity constraints) and table sizes as input in order to generate new tuples. However, the databases generated by these tools are not adequate for testing a database application. If an application query is executed against such a synthetic database, then the result of that application query is likely to be empty or contain weird results, such as a report on the performance of a sales person that contains negative sales. To solve this problem, this paper proposes a new technique called reverse query processing (RQP). RQP gets a query and a result as input and returns a possible database instance that could have produced that result for that query. RQP also has other applications; most notably, testing the performance of DBMS and debugging SQL queries.",
    "year": 2007,
    "citationCount": 116,
    "openAccessPdf": {
      "url": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/69571/1/eth-4827-01.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDE.2007.367896?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDE.2007.367896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2007-04-15",
    "authors": [
      {
        "authorId": "2691974",
        "name": "Carsten Binnig"
      },
      {
        "authorId": "1691108",
        "name": "Donald Kossmann"
      },
      {
        "authorId": "145373095",
        "name": "Eric Lo"
      }
    ]
  },
  {
    "paperId": "c64882280828801831b06f02b49416c257d68204",
    "title": "Quantifying query ambiguity",
    "abstract": "We develop a measure of a query with respect to a collection of documents with the aim of quantifying the query's ambiguity with respect to those documents. This measure, the clarity score, is the relative entropy between a query language model and the corresponding collection language model. We substantiate that the clarity score measures the coherence and specificity of the language used in documents likely to satisfy the query. We also argue that it provides a suitable quantification of the (lack of) ambiguity of a query with respect to a collection of documents and has potential applications throughout the field of information retrieval. In particular, the clarity score is shown to correlate positively with average precision in evaluations using TREC test collections. Hence, as one example, the clarity score could serve as a predictor of query performance. Systems would then be able to identify vague information requests and respond differently than they would to clear and specific requests.",
    "year": 2002,
    "citationCount": 152,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.5555/1289189.1289266",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.3115/1289189.1289266?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3115/1289189.1289266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2002-03-24",
    "authors": [
      {
        "authorId": "1405333137",
        "name": "Steve Cronen-Townsend"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      }
    ]
  },
  {
    "paperId": "2c1b4cbce83a60b351a34c46ea67a71448aa51cc",
    "title": "Coverage, relevance, and ranking: The impact of query operators on Web search engine results",
    "abstract": "Research has reported that about 10% of Web searchers utilize advanced query operators, with the other 90% using extremely simple queries. It is often assumed that the use of query operators, such as Boolean operators and phrase searching, improves the effectiveness of Web searching. We test this assumption by examining the effects of query operators on the performance of three major Web search engines. We selected one hundred queries from the transaction log of a Web search service. Each of these original queries contained query operators such as AND, OR, MUST APPEAR (+), or PHRASE (\" \"). We then removed the operators from these one hundred advanced queries. We submitted both the original and modified queries to three major Web search engines; a total of 600 queries were submitted and 5,748 documents evaluated. We compared the results from the original queries with the operators to the results from the modified queries without the operators. We examined the results for changes in coverage, relative precision, and ranking of relevant documents. The use of most query operators had no significant effect on coverage, relative precision, or ranking, although the effect varied depending on the search engine. We discuss implications for the effectiveness of searching techniques as currently taught, for future information retrieval system design, and for future research.",
    "year": 2003,
    "citationCount": 133,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/944012.944015?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/944012.944015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2003-10-01",
    "authors": [
      {
        "authorId": "1711397",
        "name": "C. Eastman"
      },
      {
        "authorId": "144715575",
        "name": "B. Jansen"
      }
    ]
  },
  {
    "paperId": "e0bd6e0c494ca277aeaa5ef00da41da5b2600b9c",
    "title": "Query-based learning applied to partially trained multilayer perceptrons",
    "abstract": "An approach is presented for query-based neural network learning. A layered perceptron partially trained for binary classification is considered. The single-output neuron is trained to be either a zero or a one. A test decision is made by thresholding the output at, for example, one-half. The set of inputs that produce an output of one-half forms the classification boundary. The authors adopted an inversion algorithm for the neural network that allows generation of this boundary. For each boundary point, the classification gradient can be generated. The gradient provides a useful measure of the steepness of the multidimensional decision surfaces. Conjugate input pairs are generated using the boundary point and gradient information and presented to an oracle for proper classification. These data are used to refine further the classification boundary, thereby increasing the classification accuracy. The result can be a significant reduction in the training set cardinality in comparison with, for example, randomly generated data points. An application example to power system security assessment is given.",
    "year": 1991,
    "citationCount": 176,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/72.80299?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/72.80299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "145159381",
        "name": "Jenq-Neng Hwang"
      },
      {
        "authorId": "28184513",
        "name": "Jai J. Choi"
      },
      {
        "authorId": "3235479",
        "name": "Seho Oh"
      },
      {
        "authorId": "47398186",
        "name": "R. Marks"
      }
    ]
  },
  {
    "paperId": "a770ec2d9850893a58854564f50e2e4c715f73c0",
    "title": "Pseudo test collections for training and tuning microblog rankers",
    "abstract": "Recent years have witnessed a persistent interest in generating pseudo test collections, both for training and evaluation purposes. We describe a method for generating queries and relevance judgments for microblog search in an unsupervised way. Our starting point is this intuition: tweets with a hashtag are relevant to the topic covered by the hashtag and hence to a suitable query derived from the hashtag. Our baseline method selects all commonly used hashtags, and all associated tweets as relevance judgments; we then generate a query from these tweets. Next, we generate a timestamp for each query, allowing us to use temporal information in the training process. We then enrich the generation process with knowledge derived from an editorial test collection for microblog search. We use our pseudo test collections in two ways. First, we tune parameters of a variety of well known retrieval methods on them. Correlations with parameter sweeps on an editorial test collection are high on average, with a large variance over retrieval algorithms. Second, we use the pseudo test collections as training sets in a learning to rank scenario. Performance close to training on an editorial test collection is achieved in many cases. Our results demonstrate the utility of tuning and training microblog search algorithms on automatically generated training material.",
    "year": 2013,
    "citationCount": 35,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2484028.2484063?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2484028.2484063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2013-07-28",
    "authors": [
      {
        "authorId": "2630627",
        "name": "R. Berendsen"
      },
      {
        "authorId": "1732056",
        "name": "M. Tsagkias"
      },
      {
        "authorId": "1780941",
        "name": "W. Weerkamp"
      },
      {
        "authorId": "1696030",
        "name": "M. de Rijke"
      }
    ]
  },
  {
    "paperId": "d66a826ad8f66419292cf5a5c7ed22f114681b40",
    "title": "APPLYING GENETIC ALGORITHMS TO INFORMATION RETRIEVAL USING VECTOR SPACE MODEL",
    "abstract": "Genetic algorithms are usually used in information retrieval systems (IRs) to enhance the information retrieval process, and to increase the efficiency of the optimal information retrieval in order to meet the users' needs and help them find what they want exactly among the growing numbers of available information. The improvement of adaptive genetic algorithms helps to retrieve the information needed by the user accurately, reduces the retrieved relevant files and excludes irrelevant files. In this study, the researcher explored the problems embedded in this process, attempted to find solutions such as the way of choosing mutation probability and fitness function, and chose Cranfield English Corpus test collection on mathematics. Such collection was conducted by Cyrial Cleverdon and used at the University of Cranfield in 1960 containing 1400 documents, and 225 queries for simulation purposes. The researcher also used cosine similarity and jaccards to compute similarity between the query and documents, and used two proposed adaptive fitness function, mutation operators as well as adaptive crossover. The process aimed at evaluating the effectiveness of results according to the measures of precision and recall. Finally, the study concluded that we might have several improvements when using adaptive genetic algorithms. \ufffd",
    "year": 2015,
    "citationCount": 212,
    "openAccessPdf": {
      "url": "https://doi.org/10.5121/ijcsea.2015.5102",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.5121/IJCSEA.2015.5102?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5121/IJCSEA.2015.5102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2015-02-28",
    "authors": [
      {
        "authorId": "7833679",
        "name": "L. Abualigah"
      },
      {
        "authorId": "25133390",
        "name": "Essam Said Hanandeh"
      }
    ]
  },
  {
    "paperId": "e2303a3f8a508ea0a4f3baf7abffa04aa89dd6fe",
    "title": "Use of Psychological Experimentation as an Aid to Development of a Query Language",
    "abstract": "This paper describes a series of psychological experiments used to test a new data base query language. The intent is to make psychological testing of a language part of the design and development process. By testing a language while it is still under development, features that require changing can bs identified and the changes made.",
    "year": 1977,
    "citationCount": 186,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/TSE.1977.231131?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TSE.1977.231131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1977-05-01",
    "authors": [
      {
        "authorId": "2271803610",
        "name": "Phyllis Reisner"
      }
    ]
  },
  {
    "paperId": "ee45dd45b8937c565ad7098204d5b98749609463",
    "title": "INTRUSION DETECTION BY BACKPROPAGATION NEURAL NETWORKS WITH SAMPLE-QUERY AND ATTRIBUTE-QUERY",
    "abstract": "The growing network intrusions have put companies and organizations at a much greater risk of loss. In this paper, we propose a new learning methodology towards developing a novel intrusion detection system (IDS) by backpropagation neural networks (BPN) with sample-query and attribute-query. We test the proposed method by a benchmark intrusion dataset to verify its feasibility and effectiveness. Results show that choosing good attributes and samples will not only have impact on the performance, but also on the overall execution efficiency. The proposed method can significantly reduce the training time required. Additionally, the training results are good. It provides a powerful tool to help supervisors analyze, model and understand the complex attack behavior of electronic crime.",
    "year": 2007,
    "citationCount": 64,
    "openAccessPdf": {
      "url": "http://ntur.lib.ntu.edu.tw/news/agent_contract.pdf",
      "status": "GREEN",
      "license": "mit",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.5019/J.IJCIR.2007.76?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5019/J.IJCIR.2007.76, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2112567007",
        "name": "Ray-I Chang"
      },
      {
        "authorId": "3195399",
        "name": "Liang-Bin Lai"
      },
      {
        "authorId": "153032841",
        "name": "Wenhung Su"
      },
      {
        "authorId": "2227990947",
        "name": "Jen-Chieh Wang"
      },
      {
        "authorId": "2972164",
        "name": "J. Kouh"
      }
    ]
  },
  {
    "paperId": "34b87d2e5b476065861106659f36146e2e32e788",
    "title": "Query-independent evidence in home page finding",
    "abstract": "Hyperlink recommendation evidence, that is, evidence based on the structure of a web's link graph, is widely exploited by commercial Web search systems. However there is little published work to support its popularity. Another form of query-independent evidence, URL-type, has been shown to be beneficial on a home page finding task. We compared the usefulness of these types of evidence on the home page finding task, combined with both content and anchor text baselines. Our experiments made use of five query sets spanning three corpora---one enterprise crawl, and the WT10g and VLC2 Web test collections.We found that, in optimal conditions, all of the query-independent methods studied (in-degree, URL-type, and two variants of PageRank) offered a better than random improvement on a content-only baseline. However, only URL-type offered a better than random improvement on an anchor text baseline. In realistic settings, for either baseline, only URL-type offered consistent gains. In combination with URL-type the anchor text baseline was more useful for finding popular home pages, but URL-type with content was more useful for finding randomly selected home pages. We conclude that a general home page finding system should combine evidence from document content, anchor text, and URL-type classification.",
    "year": 2003,
    "citationCount": 83,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/858476.858479?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/858476.858479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2003-07-01",
    "authors": [
      {
        "authorId": "2597168",
        "name": "Trystan Upstill"
      },
      {
        "authorId": "1703980",
        "name": "Nick Craswell"
      },
      {
        "authorId": "1718817",
        "name": "D. Hawking"
      }
    ]
  },
  {
    "paperId": "1d83240e8c82e14ffacfe60534dcdef550e3cbcb",
    "title": "Relational expressive power of constraint query languages",
    "abstract": "The expressive power of first-order query languages with several classes of equality and inequality constraints is studied in this paper. We settle the conjecture that recursive queries such as parit y test and transitive closure cannot be expressed in the relational calculus augmented with polynomial inequality constraints over the reals. Furthermore, noting that relational queries exhibit several forms of genericity, we establish a number of collapse results of the following form: The class of generic boolean queries expressible in the relational calculus augmented with a given class of constraints coincides with the class of queries expressible in the relational calculus (with or without an order relation). We prove such results for both the natural and active-domain semantics. As a consequence, the relational calculus augmented with polynomial inequalities expresses the same classes of generic boolean queries under both the natural and active-domain semantics. In the course of proving these results for the active-domain semantics, we establish Ramsey-type theorems saying that any query involving certain kinds of constraints coincides with a constraint-free query on databases whose elements come from a certain infinite subset of the domain. To prove the collapse results for the natural semantics, we make use of techniques from nonstandard analysis and from the model theory of ordered structures.",
    "year": 1996,
    "citationCount": 95,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/237661.237667?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/237661.237667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1996-06-03",
    "authors": [
      {
        "authorId": "1750856",
        "name": "Michael Benedikt"
      },
      {
        "authorId": "2147292665",
        "name": "Guozhu Dong"
      },
      {
        "authorId": "1681226",
        "name": "L. Libkin"
      },
      {
        "authorId": "143670300",
        "name": "L. Wong"
      }
    ]
  },
  {
    "paperId": "263e6e91d5ac764f899c766bee83f156f483fad7",
    "title": "Design and Implementation of a Semantic Query Optimizer",
    "abstract": "The authors describe a scheme to utilize semantic knowledge in optimizing a user-specified query. The semantics is represented as function-free clauses in predicate logic. The scheme uses a graph-theoretic approach to identify redundant joins and restrictions present in a given query while adding additional profitable specifications to it. Dynamic and heuristic interaction of three entities-schema, semantics, and query-forms the basis of the algorithm. The implementation architecture of the algorithm and test results on a representative set of data are presented. Issues associated with updating of semantic constraints are addressed, and an algorithm for semantic maintenance is introduced. >",
    "year": 1989,
    "citationCount": 111,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/69.87980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/69.87980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1989-09-01",
    "authors": [
      {
        "authorId": "47533578",
        "name": "Sreekumar T. Shenoy"
      },
      {
        "authorId": "1695700",
        "name": "Z. M. \u00d6zsoyoglu"
      }
    ]
  },
  {
    "paperId": "1622a632a8ac22f46a6359a8e628d7377b3ac095",
    "title": "Investigation of partial query proximity in web search",
    "abstract": "Proximity of query terms in a document is an important criterion in IR. However, no investigation has been made to determine the most useful term sequences for which proximity should be considered. In this study, we test the effectiveness of using proximity of partial term sequences (n-grams) for Web search. We observe that the proximity of sequences of 3 to 5 terms is most effective for long queries, while shorter or longer sequences appear less useful. This suggests that combinations of 3 to 5 terms can best capture the intention in user queries. In addition, we also experiment with weighing the importance of query sub-sequences using query log frequencies. Our preliminary tests show promising empirical results.",
    "year": 2008,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1367497.1367717?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1367497.1367717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2008-04-21",
    "authors": [
      {
        "authorId": "2113829681",
        "name": "Jing Bai"
      },
      {
        "authorId": "145882798",
        "name": "Yi Chang"
      },
      {
        "authorId": "3129614",
        "name": "H. Cui"
      },
      {
        "authorId": "1749245",
        "name": "Zhaohui Zheng"
      },
      {
        "authorId": "2148463679",
        "name": "Gordon Sun"
      },
      {
        "authorId": "2153900202",
        "name": "Xin Li"
      }
    ]
  },
  {
    "paperId": "703e5d39d6f48748f7cf87c00e910de212a973f5",
    "title": "What the query told the link: the integration of hypertext and information retrieval",
    "abstract": "Traditionally hypertexts have been limited in size by the manual effort required to create hypertext links. In addition, large hyper\u2013linked collections may overwhelm users with the range of possible links from any node, only a fraction of which may be appropriate for a given user at any time. This work explores automatic methods of link construction based on feedback from users collected during browsing. A fulltext search engine mediates the linking process. Query terms that distinguish well among documents in the database become candidate anchors; links are mediated by passage\u2013based relevance feedback queries. The newspaper metaphor is used to organize the retrieval results. VOIR, a software prototype that implements these algorithms has been used to browse a 74,500 node (250MB) database of newspaper articles. An experiment has been conducted to test the relative effectiveness of dynamic links and user\u2013specified queries. Experimental results suggest that link\u2013 mediated queries are more effective than user\u2013 specified queries in retrieving relevant information. The paper concludes with a discussion of possible extensions to the linking algorithms.",
    "year": 1997,
    "citationCount": 97,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/267437.267445?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/267437.267445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1997-04-15",
    "authors": [
      {
        "authorId": "2724097",
        "name": "G. Golovchinsky"
      }
    ]
  },
  {
    "paperId": "4c564dc8c15c6c2ec48c62dcf14139db16253cc9",
    "title": "An Iconic Query Language for Topological Relationships in GIS",
    "abstract": "Abstract The study of query languages for spatial databases is an active research area. This paper describes a new spatial query language that uses a visual grammar to express topological relationships. It is supplemented by text and icons to handle other spatial and non-spatial queries. A graphical user interface is also developed to provide an interactive environment for composing the iconic query command. To test the language, the interface is implemented on a SUN 4 Workstation and linked to Ingres, a relational DBMS. Preliminary tests show that the iconic query language is more convenient for expressing spatial concepts than conventional textual languages. This is due mainly to the two-dimensionality of iconic languages in contrast with the linear nature of conventional languages.",
    "year": 1995,
    "citationCount": 69,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1080/02693799508902023?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/02693799508902023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "47868571",
        "name": "Y. C. Lee"
      },
      {
        "authorId": "153119719",
        "name": "F. L. Chin"
      }
    ]
  },
  {
    "paperId": "228d7517b3cff9ae12c84f9a92237081ebc6976c",
    "title": "Addressing Complex and Subjective Product-Related Queries with Customer Reviews",
    "abstract": "Online reviews are often our first port of call when considering products and purchases online. When evaluating a potential purchase, we may have a specific query in mind, e.g. `will this baby seat fit in the overhead compartment of a 747?' or `will I like this album if I liked Taylor Swift's 1989?'. To answer such questions we must either wade through huge volumes of consumer reviews hoping to find one that is relevant, or otherwise pose our question directly to the community via a Q/A system. In this paper we hope to fuse these two paradigms: given a large volume of previously answered queries about products, we hope to automatically learn whether a review of a product is relevant to a given query. We formulate this as a machine learning problem using a mixture-of-experts-type framework---here each review is an `expert' that gets to vote on the response to a particular query; simultaneously we learn a relevance function such that `relevant' reviews are those that vote correctly. At test time this learned relevance function allows us to surface reviews that are relevant to new queries on-demand. We evaluate our system, Moqa, on a novel corpus of 1.4 million questions (and answers) and 13 million reviews. We show quantitatively that it is effective at addressing both binary and open-ended queries, and qualitatively that it surfaces reviews that human evaluators consider to be relevant.",
    "year": 2015,
    "citationCount": 184,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1512.06863",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1512.06863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2015-12-21",
    "authors": [
      {
        "authorId": "35660011",
        "name": "Julian McAuley"
      },
      {
        "authorId": "2057541565",
        "name": "Alex Yang"
      }
    ]
  },
  {
    "paperId": "19b77681f549b37f90f4b050bc69a8a0d6b1bc4f",
    "title": "Magnetoacoustic remote query temperature and humidity sensors",
    "abstract": "In response to an externally applied time-varying magnetic field, freestanding sensors made of magnetoelastic thick or thin films mechanically oscillate. These oscillations are strongest at the characteristic resonant frequency of the sensor. Depending upon the physical geometry and the surface roughness of the magnetoelastic sensor, these mechanical deformations launch an acoustic wave that can be detected remotely from the test area by a microphone. By monitoring changes in the characteristic resonant frequency of a magnetoacoustic sensor, multiple environmental parameters can be measured. In this work we report on the application of magnetoacoustic sensors for the remote query measurement of temperature, the monitoring of phase transitions and, in combination with a humidity-responsive mass-changing Al2O3 ceramic thin film, the in situ measurement of humidity levels.",
    "year": 2000,
    "citationCount": 70,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1088/0964-1726/9/4/314?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/0964-1726/9/4/314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Materials Science",
      "Physics"
    ],
    "publicationDate": "2000-08-01",
    "authors": [
      {
        "authorId": "2150987404",
        "name": "Mahaveer K. Jain"
      },
      {
        "authorId": "2067704390",
        "name": "Stefan Schmidt"
      },
      {
        "authorId": "1929277",
        "name": "K. G. Ong"
      },
      {
        "authorId": "31192758",
        "name": "C. Mungle"
      },
      {
        "authorId": "2465802",
        "name": "C. Grimes"
      }
    ]
  },
  {
    "paperId": "e1fa8150a37c0bf34f3e6da402b6d8e3081c33e2",
    "title": "Temporal Logic Query Checking: A Tool for Model Exploration",
    "abstract": "Temporal logic query checking was first introduced by W. Chan in order to speed up design understanding by discovering properties not known a priori. A query is a temporal logic formula containing a special symbol ?/sub 1/, known as a placeholder. Given a Kripke structure and a propositional formula /spl phi/, we say that /spl phi/ satisfies the query if replacing the placeholder by /spl phi/ results in a temporal logic formula satisfied by the Kripke structure. A solution to a temporal logic query on a Kripke structure is the set of all propositional formulas that satisfy the query. Query checking helps discover temporal properties of a system and, as such, is a useful tool for model exploration. In this paper, we show that query checking is applicable to a variety of model exploration tasks, ranging from invariant computation to test case generation. We illustrate these using a Cruise Control System. Additionally, we show that query checking is an instance of a multi-valued model checking of Chechik et al. This approach enables us to build an implementation of a temporal logic query checker, TLQSolver, on top of our existing multi-valued model checker /sub /spl chi//Chek. It also allows us to decide a large class of queries and introduce witnesses for temporal logic queries-an essential notion for effective model exploration.",
    "year": 2003,
    "citationCount": 49,
    "openAccessPdf": {
      "url": "https://tspace.library.utoronto.ca/bitstream/1807/16887/1/Chechik_2847_2979.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/TSE.2003.1237171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TSE.2003.1237171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2003-10-01",
    "authors": [
      {
        "authorId": "2850298",
        "name": "A. Gurfinkel"
      },
      {
        "authorId": "1854323",
        "name": "M. Chechik"
      },
      {
        "authorId": "34724460",
        "name": "B. Devereux"
      }
    ]
  },
  {
    "paperId": "c0c17e89c6eac1d9b1c90c22937ecb47a792fedd",
    "title": "Keyphrase extraction-based query expansion in digital libraries",
    "abstract": "In pseudo-relevance feedback, the two key factors affecting the retrieval performance most are the source from which expansion terms are generated and the method of ranking those expansion terms. In this paper, we present a novel unsupervised query expansion technique that utilizes keyphrases and POS phrase categorization. The keyphrases are extracted from the retrieved documents and weighted with an algorithm based on information gain and co-occurrence of phrases. The selected keyphrases are translated into disjunctive normal form (DNF) based on the POS phrase categorization technique for better query refomulation. Furthermore, we study whether ontologies such as WordNet and MeSH improve the retrieval performance in conjunction with the keyphrases. We test our techniques on TREC 5, 6, and 7 as well as a MEDLINE collection. The experimental results show that the use of keyphrases with POS phrase categorization produces the best average precision",
    "year": 2006,
    "citationCount": 49,
    "openAccessPdf": {
      "url": "http://www.dabi.temple.edu/%7Ezoran/papers/song06.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1141753.1141800?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1141753.1141800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2006-06-11",
    "authors": [
      {
        "authorId": "144982626",
        "name": "Min Song"
      },
      {
        "authorId": "144436171",
        "name": "I. Song"
      },
      {
        "authorId": "143956638",
        "name": "R. Allen"
      },
      {
        "authorId": "3844766",
        "name": "Z. Obradovic"
      }
    ]
  },
  {
    "paperId": "ce549a59f62cc5ea07d5521fc1e977e18c33d112",
    "title": "XPath query containment",
    "abstract": "Consider an XML publish-subscribe scenario with hundreds of subscribers and tens of thousands of XML documents to be delivered per day. Subscribers specify the documents in which they are interested in by means of XPath [8] expressions. If an expression matches a (part of a) document it is delivered to the subscriber. Naturally, it is desired that the decision to which subscriber a document must be sent should be taken quickly. Although the test whether a single XPath expression matches can be done in polynomial time, it is not efficient to test every such expression for every document. Fortunately, there is a partial order on expressions, i.e., for some expressions p, q it might hold that whenever a document matches p it also matches q (denoted p \u22860 q). If we already know that a document matches p, we do not need to test q anymore, as it matches automatically. Correspondingly, if we know that q does not match then p will not match either. Hence, the inclusion structure of the XPath expressions should be computed in advance to decrease online computation time. This leads to the algorithmic problem of XPath Query Containment, i.e., checking whether p \u22860 q (for a different, indexbased approach see, e.g., [6]). The main idea of this article is to describe some of the main algorithmic techniques that have been proposed for XPath Query Containment. These techniques are described in Section 5. Before that, in Sections 2 and 3 the basic definitions on XPath and the",
    "year": 2004,
    "citationCount": 142,
    "openAccessPdf": {
      "url": "http://www.sigmod.org/publications/sigmod-record/0403/C.LLibkin.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/974121.974140?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/974121.974140, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2004-03-01",
    "authors": [
      {
        "authorId": "1736032",
        "name": "T. Schwentick"
      }
    ]
  },
  {
    "paperId": "0ad4918cfc22eedda08b107a6061aa2a7df1c1e1",
    "title": "Relational expressive power of constraint query languages",
    "abstract": "The expressive power of first-order query languages with several classes of equality and inequality constraints is studied in this paper. We settle the conjecture that recursive queries such as parity test and transitive closure cannot be expressed in the relational calculus augmented with polynomial inequality constraints over the reals. Furthermore, noting that relational queries exhibit several forms of genericity, we establish a number of collapse results of the following form: The class of generic Boolean queries expressible in the relational calculus augmented with a given class of constraints coincides with the class of queries expressible in the relational calculus (with or without an order relation). We prove such results for both the natural and active-domain semantics. As a consequence, the relational calculus augmented with polynomial inequalities expresses the same classes of generic Boolean queries under both the natural and active-domain semantics.\nIn the course of proving these results for the active-domin semantics, we establish Ramsey-type theorems saying that any query involving certain kinds of constraints coincides with a constraint-free query on databases whose elements come from a certain infinite subset of the domain. To prove the collapse results for the natural semantics, we make use of techniques from nonstandard analysis and from the model theory of ordered structures.",
    "year": 1998,
    "citationCount": 61,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/273865.273870",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/273865.273870?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/273865.273870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1750856",
        "name": "Michael Benedikt"
      },
      {
        "authorId": "2147292665",
        "name": "Guozhu Dong"
      },
      {
        "authorId": "1681226",
        "name": "L. Libkin"
      },
      {
        "authorId": "143670300",
        "name": "L. Wong"
      }
    ]
  },
  {
    "paperId": "d59abc5aba56371c1af60d9a3cc87c3cd0396c50",
    "title": "Mining term association rules for automatic global query expansion: methodology and preliminary results",
    "abstract": "The authors are looking at the mining of association between terms for the automatic expansion of queries. The technique used for the discovery of the associations is association rule mining (R. Agrawal et al., 1996). The technique proposed is more flexible than previous techniques based on term co-occurrence since it takes into account not only the co-occurrence frequency but also the confidence and direction of the association rules. We have been able to consistently improve the effectiveness of the retrieval over the set of 48 test queries on the Associated Press 1990 news wires corpus of the TREC4 benchmark by query expansion using term association rules.",
    "year": 2000,
    "citationCount": 49,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/WISE.2000.882414?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WISE.2000.882414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2000-06-19",
    "authors": [
      {
        "authorId": "2111378032",
        "name": "Jie Wei"
      },
      {
        "authorId": "1735321",
        "name": "S. Bressan"
      },
      {
        "authorId": "1693070",
        "name": "B. Ooi"
      }
    ]
  },
  {
    "paperId": "7dcf693cd289b00a4c1bf81fc213d4dd9989c3d8",
    "title": "Effects of query complexity and learning on novice user query performance with conceptual and logical database interfaces",
    "abstract": "Users see the database interface as the database system. A good interface enables them to formulate queries better. The semantics communicated through the interface can be classified according to abstraction levels, such as the conceptual and logical levels. With the conceptual interface, interaction is in terms of real-world concepts such as entities, objects and relationships. Current user-database interaction is mainly based on the logical interface, where interaction is in terms of abstract database concepts such as relations and joins. Many researchers argue that end users will perform better with the conceptual interface. This research tested this claim, as well as the effects of query complexity and learning, on the visual query performance of users. The experiment involved three tests: an initial test, a retention test and a relearning test. The results showed that, for complex queries, conceptual interface users achieved higher accuracy, were more confident in their answers, and spent less time on the queries. This is persistent across retention and relearning tests.",
    "year": 2004,
    "citationCount": 48,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/TSMCA.2003.820581?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TSMCA.2003.820581, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2004-03-01",
    "authors": [
      {
        "authorId": "1746885",
        "name": "K. Siau"
      },
      {
        "authorId": "33892292",
        "name": "H. Chan"
      },
      {
        "authorId": "1792514",
        "name": "K. Wei"
      }
    ]
  },
  {
    "paperId": "84e0e31ab45b14b6529c27ebc822a60cdea607c4",
    "title": "A study of Poisson query generation model for information retrieval",
    "abstract": "Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson distribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent. However, based on different smoothing methods, the two families of models behave differently. We show that the Poisson model has several advantages, including naturally accommodating per-term smoothing and modeling accurate background more efficiently. We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections. The results show that while their basic models perform comparably, the Poisson model can out perform multinomial model with per-term smoothing. The performance can be further improved with two-stage smoothing.",
    "year": 2007,
    "citationCount": 47,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1277741.1277797?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1277741.1277797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2007-07-23",
    "authors": [
      {
        "authorId": "1743469",
        "name": "Qiaozhu Mei"
      },
      {
        "authorId": "145344526",
        "name": "Hui Fang"
      },
      {
        "authorId": "1736467",
        "name": "ChengXiang Zhai"
      }
    ]
  },
  {
    "paperId": "626b8452a957c4bd2b5039a43d7738eff590913d",
    "title": "A survey of pre-retrieval query performance predictors",
    "abstract": "The focus of research on query performance prediction is to predict the effectiveness of a query given a search system and a collection of documents. If the performance of queries can be estimated in advance of, or during the retrieval stage, specific measures can be taken to improve the overall performance of the system. In particular, pre-retrieval predictors predict the query performance before the retrieval step and are thus independent of the ranked list of results; such predictors base their predictions solely on query terms, the collection statistics and possibly external sources such as WordNet. In this poster, 22 pre-retrieval predictors are categorized and assessed on three different TREC test collections.",
    "year": 2008,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1458082.1458311?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1458082.1458311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2008-10-26",
    "authors": [
      {
        "authorId": "2244631782",
        "name": "Claudia Hauff"
      },
      {
        "authorId": "4021174",
        "name": "D. Hiemstra"
      },
      {
        "authorId": "144097974",
        "name": "F. D. Jong"
      }
    ]
  },
  {
    "paperId": "9fa45b0706fce829da0bfb846ba5c10a5767ec24",
    "title": "Efficient interactive query expansion with complete search",
    "abstract": "We present an efficient realization of the following interactive search engine feature: as the user is typing the query, words that are related to the last query word and that would lead to good hits are suggested, as well as selected such hits. The realization has three parts: (i) building clusters of related terms, (ii) adding this information as artificial words to the index such that (iii) the described feature reduces to an instance of prefix search and completion. An efficient solution for the latter is provided by the CompleteSearch engine, with which we have integrated the proposed feature. For building the clusters of related terms we propose a variant of latent semantic indexing that, unlike standard approaches, is completely transparent to the user. By experiments on two large test-collections, we demonstrate that the feature is provided at only a slight increase in query processing time and index size.",
    "year": 2007,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1321440.1321560?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1321440.1321560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2007-11-06",
    "authors": [
      {
        "authorId": "1936883",
        "name": "H. Bast"
      },
      {
        "authorId": "1956144",
        "name": "Debapriyo Majumdar"
      },
      {
        "authorId": "1684687",
        "name": "Ingmar Weber"
      }
    ]
  },
  {
    "paperId": "1ac790a9be9e7af51495688231dae7eb9751b78d",
    "title": "Multidimensional indexing and query coordination for tertiary storage management",
    "abstract": "In many scientific domains, experimental devices or simulation programs generate large volumes of data. The volumes of data may reach hundreds of terabytes and therefore it is impractical to store them on disk systems. Rather they are stored on robotic tape systems that are managed by some mass storage system (MSS). A major bottleneck in analyzing the simulated/collected data is the retrieval of subsets from the tertiary storage system. We describe the architecture and implementation of a Storage Access Coordination System (STACS) designed to optimize the use of a disk cache, and thus minimize the number of files read from tape. We achieve this by using a specialized index to locate the relevant data on tapes, and by coordinating file caching over multiple queries. We focus on a specific application area, a high energy physics data management and analysis environment. STACS was implemented and is being incorporated in an operational system, scheduled to go online at the end of 1999. We also include the results of various tests that demonstrate the benefits and efficiency gained of using the STACS.",
    "year": 1999,
    "citationCount": 80,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/SSDM.1999.787637?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SSDM.1999.787637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1999-07-28",
    "authors": [
      {
        "authorId": "1747737",
        "name": "A. Shoshani"
      },
      {
        "authorId": "2062880264",
        "name": "L. Bernardo"
      },
      {
        "authorId": "2260526",
        "name": "H. Nordberg"
      },
      {
        "authorId": "1700419",
        "name": "D. Rotem"
      },
      {
        "authorId": "143891223",
        "name": "A. Sim"
      }
    ]
  },
  {
    "paperId": "364e69c9e3a25480d99b7a6544e9bae518e10496",
    "title": "Articulating information needs in XML query languages",
    "abstract": "Document-centric XML is a mixture of text and structure. With the increased availability of document-centric XML documents comes a need for query facilities in which both structural constraints and constraints on the content of the documents can be expressed. How does the expressiveness of languages for querying XML documents help users to express their information needs? We address this question from both an experimental and a theoretical point of view. Our experimental analysis compares a structure-ignorant with a structure-aware retrieval approach using the test suite of the INEX XML Retrieval Evaluation Initiative. Theoretically, we create two mathematical models of users' knowledge of a set of documents and define query languages which exactly fit these models. One of these languages corresponds to an XML version of fielded search, the other to the INEX query language.Our main experimental findings are: First, while structure is used in varying degrees of complexity, two-thirds of the queries can be expressed in a fielded-search-like format which does not use the hierarchical structure of the documents. Second, three-quarters of the queries use constraints on the context of the elements to be returned; these contextual constraints cannot be captured by ordinary keyword queries. Third, structure is used as a search hint, and not as a strict requirement, when judged against the underlying information need. Fourth, the use of structure in queries functions as a precision enhancing device.",
    "year": 2006,
    "citationCount": 44,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1185877.1185879?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1185877.1185879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2006-10-01",
    "authors": [
      {
        "authorId": "1753628",
        "name": "J. Kamps"
      },
      {
        "authorId": "143660572",
        "name": "maarten marx"
      },
      {
        "authorId": "1696030",
        "name": "M. de Rijke"
      },
      {
        "authorId": "1780952",
        "name": "B\u00f6rkur Sigurbj\u00f6rnsson"
      }
    ]
  },
  {
    "paperId": "48048fecbddcdc298367ffaac853749a3c0bc0c5",
    "title": "A simple, focused, computerized query to detect overutilization of laboratory tests.",
    "abstract": "CONTEXT\nAlthough there is nearly universal agreement that laboratory tests are overutilized, the degree of overutilization in a given institution is difficult to quantify and monitor across time.\n\n\nOBJECTIVE\nTo detect and clearly document repetitive daily ordering of a commonly ordered laboratory test (serum sodium) by employing a simple, focused, computerized query of a test result database followed by chart review and validation.\n\n\nDESIGN\nA retrospective computerized query of our clinical data repository was performed to find inpatients who displayed normal serum sodium test results on 4 or more consecutive days, without any abnormal values during the same admission. The search was limited to a 1-month period. A subset of these patients was selected for chart review.\n\n\nRESULTS\nOne hundred sixteen patients met our criteria, and the tests ordered for those patients comprised 5.1% of the monthly volume of serum sodium tests ordered in our institution. Chart review revealed a consistent lack of documentation of medical necessity for repeat testing as well as persistence of repeat serum sodium orders until the end of the patients' hospital course.\n\n\nCONCLUSIONS\nWe conclude that a focused query of data derived from a clinical data repository can detect and document overutilization of a common laboratory test in a convincing fashion within a given institution.",
    "year": 2005,
    "citationCount": 40,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1043/1543-2165(2005)129[1141:ASFCQT]2.0.CO;2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1043/1543-2165(2005)129[1141:ASFCQT]2.0.CO;2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationDate": "2005-09-01",
    "authors": [
      {
        "authorId": "143996316",
        "name": "J. Weydert"
      },
      {
        "authorId": "12718040",
        "name": "Newell D Nobbs"
      },
      {
        "authorId": "2076980507",
        "name": "Ronald Feld"
      },
      {
        "authorId": "31951426",
        "name": "J. Kemp"
      }
    ]
  },
  {
    "paperId": "ff9c819e4425a20627570146f4cc666fbccd5790",
    "title": "Ad Hoc Table Retrieval using Semantic Similarity",
    "abstract": "We introduce and address the problem of ad hoc table retrieval: answering a keyword query with a ranked list of tables. This task is not only interesting on its own account, but is also being used as a core component in many other table-based information access scenarios, such as table completion or table mining. The main novel contribution of this work is a method for performing semantic matching between queries and tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using a purpose-built test collection based on Wikipedia tables, we demonstrate significant and substantial improvements over a state-of-the-art baseline.",
    "year": 2018,
    "citationCount": 118,
    "openAccessPdf": {
      "url": "https://uis.brage.unit.no/uis-xmlui/bitstream/11250/3038981/1/www2018-table.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1802.06159, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2018-02-16",
    "authors": [
      {
        "authorId": "1390852432",
        "name": "Shuo Zhang"
      },
      {
        "authorId": "1680484",
        "name": "K. Balog"
      }
    ]
  },
  {
    "paperId": "d7cd0949f5c30762c50fdad6d8009132130ccec8",
    "title": "User errors in database query composition",
    "abstract": "This research reports on the experimental test of several causes of user errors while composing database queries. The query language under consideration is Structured Query Language (SQL), the industry standard language for querying databases. Unfortunately, users commit many errors when using SQL. To understand user errors, a model of query writing was developed that integrated a GOMS-type analysis of query writing with the characteristics of human cognition. This model revealed multiple cognitive causes of a frequent and troublesome error, join clause omission. This semantic user error returns answers from the database that may be undetectably wrong, affecting users, decision makers, and programmers.The model predicted four possible causes of join clause omission, and empirical testing revealed that all four contributed to the error. Specifically, the frequency of this error increased because (1) the load on working memory caused by writing intervening clauses made the users forget to include the join clause, (2) an explicit clue to write the join clause was absent from the problem statement, (3) users inappropriately reused the procedure appropriate for a single table query, which requires no join clause, when a join clause is indeed necessary, and (4) some users never learned the correct procedure. These results are significant for understanding user errors in general and for developing new interfaces and training schemes for the task of writing database queries.",
    "year": 1995,
    "citationCount": 49,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1006/ijhc.1995.1017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1006/ijhc.1995.1017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1995-04-01",
    "authors": [
      {
        "authorId": "3143779",
        "name": "J. Smelcer"
      }
    ]
  },
  {
    "paperId": "ca7dd279274916062a981c544da730e0e84abb87",
    "title": "Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask",
    "abstract": "The query engines of most modern database systems are either based on vectorization or data-centric code generation. These two state-of-the-art query processing paradigms are fundamentally different in terms of system structure and query execution code. Both paradigms were used to build fast systems. However, until today it is not clear which paradigm yields faster query execution, as many implementation-specific choices obstruct a direct comparison of architectures. In this paper, we experimentally compare the two models by implementing both within the same test system. This allows us to use for both models the same query processing algorithms, the same data structures, and the same parallelization framework to ultimately create an apples-to-apples comparison. We find that both are efficient, but have different strengths and weaknesses. Vectorization is better at hiding cache miss latency, whereas data-centric compilation requires fewer CPU instructions, which benefits cache-resident workloads. Besides raw, single-threaded performance, we also investigate SIMD as well as multi-core parallelization and different hardware architectures. Finally, we analyze qualitative differences as a guide for system architects.",
    "year": 2018,
    "citationCount": 118,
    "openAccessPdf": {
      "url": "https://ir.cwi.nl/pub/28470/28470.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.14778/3275366.3275370?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14778/3275366.3275370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2018-09-01",
    "authors": [
      {
        "authorId": "40266474",
        "name": "T. Kersten"
      },
      {
        "authorId": "1787012",
        "name": "Viktor Leis"
      },
      {
        "authorId": "144122431",
        "name": "A. Kemper"
      },
      {
        "authorId": "143993045",
        "name": "Thomas Neumann"
      },
      {
        "authorId": "1774210",
        "name": "Andrew Pavlo"
      },
      {
        "authorId": "1687211",
        "name": "P. Boncz"
      }
    ]
  },
  {
    "paperId": "e524b0ac3271d962a893b740197700c652fa657f",
    "title": "An Experimental Comparison of Natural and Structured Query Languages",
    "abstract": "In order to test the widely held belief that natural language is the ideal user interface for computers, subjects solved data retrieval problems using English and using SEQUEL, a structured query language. There were no significant differences in solution accuracy, but subjects worked faster using SEQUEL. Speed differences were greatest for problems having a solution congruent with the syntactic structure of SEQUEL, suggesting an advantage for special-purpose languages tuned to the requirements of specific tasks",
    "year": 1983,
    "citationCount": 57,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1177/001872088302500301?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/001872088302500301, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1983-06-01",
    "authors": [
      {
        "authorId": "66408706",
        "name": "Duane W. Small"
      },
      {
        "authorId": "32151260",
        "name": "Linda J. Weldon"
      }
    ]
  },
  {
    "paperId": "5af49fc44d7988562fe7c216562eb9e8f6742400",
    "title": "Efficient state merging in symbolic execution",
    "abstract": "Symbolic execution has proven to be a practical technique for building automated test case generation and bug finding tools. Nevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, one way to reduce the number of states that the tools need to explore is to merge states obtained on different paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the underlying constraint solver) and interferes with optimizations of the exploration process (also referred to as search strategies). The net effect is that state merging may actually lower performance rather than increase it. We present a way to automatically choose when and how to merge states such that the performance of symbolic execution is significantly increased. First, we present query count estimation, a method for statically estimating the impact that each symbolic variable has on solver queries that follow a potential merge point; states are then merged only when doing so promises to be advantageous. Second, we present dynamic state merging, a technique for merging states that interacts favorably with search strategies in automated test case generation and bug finding tools. Experiments on the 96 GNU Coreutils show that our approach consistently achieves several orders of magnitude speedup over previously published results. Our code and experimental data are publicly available at http://cloud9.epfl.ch.",
    "year": 2012,
    "citationCount": 258,
    "openAccessPdf": {
      "url": "https://infoscience.epfl.ch/record/176487/files/stateMerging_pldi12.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2254064.2254088?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2254064.2254088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2012-06-11",
    "authors": [
      {
        "authorId": "49990119",
        "name": "Volodymyr Kuznetsov"
      },
      {
        "authorId": "1764655",
        "name": "Johannes Kinder"
      },
      {
        "authorId": "32259993",
        "name": "Stefan Bucur"
      },
      {
        "authorId": "2465036",
        "name": "George Candea"
      }
    ]
  },
  {
    "paperId": "59d5de157c3e7b6331495e276d0d526450b94574",
    "title": "Query Expansion in Information Retrieval Systems using a Bayesian Network-Based Thesaurus",
    "abstract": "Information Retrieval (IR) is concerned with the identification of documents in a collection that are relevant to a given information need, usually represented as a query containing terms or keywords, which are supposed to be a good description of what the user is looking for. IR systems may improve their effectiveness (i.e., increasing the number of relevant documents retrieved) by using a process of query expansion, which automatically adds new terms to the original query posed by an user. In this paper we develop a method of query expansion based on Bayesian networks. IJsing a learning algorithm, we construct a Bayesian network that represents some of the relationships among the terms appearing in a given document collection; this network is then used as a thesaurus (specific for that collection). We also report the results obtained by our method on three standard test collections.",
    "year": 1998,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1301.7364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1998-07-24",
    "authors": [
      {
        "authorId": "1728509",
        "name": "L. M. D. Campos"
      },
      {
        "authorId": "1397395901",
        "name": "J. M. Fern\u00e1ndez-Luna"
      },
      {
        "authorId": "1690193",
        "name": "J. Huete"
      }
    ]
  },
  {
    "paperId": "4125d446f437933a7d639cc1eb2d43302ee679a8",
    "title": "Hyperpoints and Fine Vocabularies for Large-Scale Location Recognition",
    "abstract": "Structure-based localization is the task of finding the absolute pose of a given query image w.r.t. a pre-computed 3D model. While this is almost trivial at small scale, special care must be taken as the size of the 3D model grows, because straight-forward descriptor matching becomes ineffective due to the large memory footprint of the model, as well as the strictness of the ratio test in 3D. Recently, several authors have tried to overcome these problems, either by a smart compression of the 3D model or by clever sampling strategies for geometric verification. Here we explore an orthogonal strategy, which uses all the 3D points and standard sampling, but performs feature matching implicitly, by quantization into a fine vocabulary. We show that although this matching is ambiguous and gives rise to 3D hyperpoints when matching each 2D query feature in isolation, a simple voting strategy, which enforces the fact that the selected 3D points shall be co-visible, can reliably find a locally unique 2D-3D point assignment. Experiments on two large-scale datasets demonstrate that our method achieves state-of-the-art performance, while the memory footprint is greatly reduced, since only visual word labels but no 3D point descriptors need to be stored.",
    "year": 2015,
    "citationCount": 160,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2015.243?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2015.243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationDate": "2015-12-07",
    "authors": [
      {
        "authorId": "1959475",
        "name": "Torsten Sattler"
      },
      {
        "authorId": "3276694",
        "name": "M. Havlena"
      },
      {
        "authorId": "2708577",
        "name": "Filip Radenovic"
      },
      {
        "authorId": "144810819",
        "name": "K. Schindler"
      },
      {
        "authorId": "1742208",
        "name": "M. Pollefeys"
      }
    ]
  },
  {
    "paperId": "d26c304a29552923e65a52e1331c0bdce8c70cb2",
    "title": "Distributed query processing strategies in Mermaid, a frontend to data management systems",
    "abstract": "The Mermaid testbed system has been developed as part of an ongoing research program at SDC to explore issues in distributed data management. The Mermaid testbed provides a uniform front end that makes the complexity of manipulating data in distributed heterogeneous databases under various data management systems (DBMSs) transparent to the user. It is being used to test query optimization algorithms as well as user interface methodology.",
    "year": 1984,
    "citationCount": 34,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDE.1984.7271274?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDE.1984.7271274, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1984-04-24",
    "authors": [
      {
        "authorId": "2072072218",
        "name": "David Brill"
      },
      {
        "authorId": "2468885",
        "name": "M. Templeton"
      },
      {
        "authorId": "2642131",
        "name": "Clement T. Yu"
      }
    ]
  },
  {
    "paperId": "f314339651cb25e4234e0b96fe8bd87206847993",
    "title": "Iterative Alternating Neural Attention for Machine Reading",
    "abstract": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.",
    "year": 2016,
    "citationCount": 117,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1606.02245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2016-06-07",
    "authors": [
      {
        "authorId": "2041695",
        "name": "Alessandro Sordoni"
      },
      {
        "authorId": "143902541",
        "name": "Philip Bachman"
      },
      {
        "authorId": "1751762",
        "name": "Yoshua Bengio"
      }
    ]
  },
  {
    "paperId": "75c723be37aec0052e17e64af5c532389215dc0e",
    "title": "Benchmarking the Chase",
    "abstract": "The chase is a family of algorithms used in a number of data management tasks, such as data exchange, answering queries under dependencies, query reformulation with constraints, and data cleaning. It is well established as a theoretical tool for understanding these tasks, and in addition a number of prototype systems have been developed. While individual chase-based systems and particular optimizations of the chase have been experimentally evaluated in the past, we provide the first comprehensive and publicly available benchmark---test infrastructure and a set of test scenarios---for evaluating chase implementations across a wide range of assumptions about the dependencies and the data. We used our benchmark to compare chase-based systems on data exchange and query answering tasks with one another, as well as with systems that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of chase implementations.",
    "year": 2017,
    "citationCount": 111,
    "openAccessPdf": {
      "url": "https://iris.unibas.it/bitstream/11563/131719/3/29.PODS2017.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3034786.3034796?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3034786.3034796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2017-05-09",
    "authors": [
      {
        "authorId": "1750856",
        "name": "Michael Benedikt"
      },
      {
        "authorId": "152576198",
        "name": "G. Konstantinidis"
      },
      {
        "authorId": "1785690",
        "name": "G. Mecca"
      },
      {
        "authorId": "1703160",
        "name": "B. Motik"
      },
      {
        "authorId": "1802817",
        "name": "Paolo Papotti"
      },
      {
        "authorId": "145530628",
        "name": "Donatello Santoro"
      },
      {
        "authorId": "2181980",
        "name": "Efthymia Tsamoura"
      }
    ]
  },
  {
    "paperId": "8d371244b2f43aad2862c0592a0332598baac426",
    "title": "Query Processing In A Relational Database Management System",
    "abstract": "In this paper the various tactics for query processing in INGRESS are empirically evaluated on a test bed of sample queries.",
    "year": 1979,
    "citationCount": 44,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/VLDB.1979.718156?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/VLDB.1979.718156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1979-10-03",
    "authors": [
      {
        "authorId": "2443595",
        "name": "K. Youssefi"
      },
      {
        "authorId": "2053678407",
        "name": "E. Wong"
      }
    ]
  },
  {
    "paperId": "983244d888bd974603c1d45a8e613abc71bd06a0",
    "title": "Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent",
    "abstract": "We prove the first superpolynomial lower bounds for learning one-layer neural networks with respect to the Gaussian distribution using gradient descent. We show that any classifier trained using gradient descent with respect to square-loss will fail to achieve small test error in polynomial time given access to samples labeled by a one-layer neural network. For classification, we give a stronger result, namely that any statistical query (SQ) algorithm (including gradient descent) will fail to achieve small test error in polynomial time. Prior work held only for gradient descent run with small batch sizes, required sharp activations, and applied to specific classes of queries. Our lower bounds hold for broad classes of activations including ReLU and sigmoid. The core of our result relies on a novel construction of a simple family of neural networks that are exactly orthogonal with respect to all spherically symmetric distributions.",
    "year": 2020,
    "citationCount": 68,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/2006.12011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2020-06-22",
    "authors": [
      {
        "authorId": "9935792",
        "name": "Surbhi Goel"
      },
      {
        "authorId": "1658704335",
        "name": "Aravind Gollakota"
      },
      {
        "authorId": "1432199349",
        "name": "Zhihan Jin"
      },
      {
        "authorId": "40641900",
        "name": "Sushrut Karmalkar"
      },
      {
        "authorId": "1682471",
        "name": "Adam R. Klivans"
      }
    ]
  },
  {
    "paperId": "dd9d8abdec7d1b95848018717db7ab200faf4df5",
    "title": "Testing the reliability of genetic methods of species identification via simulation.",
    "abstract": "Although genetic methods of species identification, especially DNA barcoding, are strongly debated, tests of these methods have been restricted to a few empirical cases for pragmatic reasons. Here we use simulation to test the performance of methods based on sequence comparison (BLAST and genetic distance) and tree topology over a wide range of evolutionary scenarios. Sequences were simulated on a range of gene trees spanning almost three orders of magnitude in tree depth and in coalescent depth; that is, deep or shallow trees with deep or shallow coalescences. When the query's conspecific sequences were included in the reference alignment, the rate of positive identification was related to the degree to which different species were genetically differentiated. The BLAST, distance, and liberal tree-based methods returned higher rates of correct identification than did the strict tree-based requirement that the query was within, but not sister to, a single-species clade. Under this more conservative approach, ambiguous outcomes occurred in inverse proportion to the number of reference sequences per species. When the query's conspecific sequences were not in the reference alignment, only the strict tree-based approach was relatively immune to making false-positive identifications. Thresholds affected the rates at which false-positive identifications were made when the query's species was unrepresented in the reference alignment but did not otherwise influence outcomes. A conservative approach using the strict tree-based method should be used initially in large-scale identification systems, with effort made to maximize sequence sampling within species. Once the genetic variation within a taxonomic group is well characterized and the taxonomy resolved, then the choice of method used should be dictated by considerations of computational efficiency. The requirement for extensive genetic sampling may render these techniques inappropriate in some circumstances.",
    "year": 2008,
    "citationCount": 339,
    "openAccessPdf": {
      "url": "http://sysbio.oxfordjournals.org/content/57/2/216.full.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1080/10635150802032990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10635150802032990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "publicationDate": "2008-04-01",
    "authors": [
      {
        "authorId": "38876564",
        "name": "H. Ross"
      },
      {
        "authorId": "2142482944",
        "name": "Sumathi Murugan"
      },
      {
        "authorId": "1861413",
        "name": "Wai Lok Sibon Li"
      }
    ]
  },
  {
    "paperId": "cbc93fff80721c6f3e75e65578287442be8b48a3",
    "title": "Ambiguous queries: test collections need more sense",
    "abstract": "Although there are many papers examining ambiguity in Information Retrieval, this paper shows that there is a whole class of ambiguous word that past research has barely explored. It is shown that the class is more ambiguous than other word types and is commonly used in queries. The lack of test collections containing ambiguous queries is highlighted and a method for creating collections from existing resources is described. Tests using the new collection show the impact of query ambiguity on an IR system: it is shown that conventional systems are incapable of dealing effectively with such queries and that current assumptions about how to improve search effectiveness do not hold when searching on this common query type.",
    "year": 2008,
    "citationCount": 162,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390334.1390420?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390334.1390420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2008-07-20",
    "authors": [
      {
        "authorId": "144721996",
        "name": "M. Sanderson"
      }
    ]
  },
  {
    "paperId": "40e235f90ec01fa311feded30462a8e81a8f4b33",
    "title": "The Value of Supplemental Prognostic Tests for the Preoperative Assessment of Idiopathic Normal-pressure Hydrocephalus",
    "abstract": "OBJECTIVE:The diagnosis and management of idiopathic normal-pressure hydrocephalus (INPH) remains unclear. Moreover, the value of supplementary tests to predict which patients would benefit from placement of a shunt has not been established. This report develops evidence-based guidelines for the use of supplementary tests as an aid in prognosis. METHODS:MEDLINE searches from 1966 to the present were undertaken by use of the query NPH, normal-pressure hydrocephalus, lumbar drain, CSF [cerebrospinal fluid] tap test, and external CSF drainage in humans. This resulted in 242 articles. To provide a scientific, evidence-based review, we have chosen to restrict our analysis to clinically relevant studies usually consisting of large numbers of shunted NPH patients. Studies that did not specify INPH or secondary NPH were considered in a separate evidentiary table. RESULTS:Evidence-based guidelines for use in supplementary tests have been developed. A positive response to a 40- to 50-ml tap test has a higher degree of certainty for a favorable response to shunt placement than can be obtained by clinical examination. However, the tap test cannot be used as an exclusionary test because of its low sensitivity (26\u201361%). Determination of the CSF outflow resistance via an infusion test carries a higher sensitivity (57\u2013100%) compared with the tap test and a similar positive predictive value of 75 to 92%. Prolonged external lumbar drainage in excess of 300 ml is associated with high sensitivity (50\u2013100%) and high positive predictive value (80\u2013100%). CONCLUSION:To date, a single standard for the prognostic evaluation of INPH patients is lacking. However, supplemental tests can increase predictive accuracy for prognosis to greater than 90%. Additional multicenter prospective randomized clinical trials are needed.",
    "year": 2005,
    "citationCount": 427,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1227/01.NEU.0000168184.01002.60?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1227/01.NEU.0000168184.01002.60, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationDate": "2005-09-01",
    "authors": [
      {
        "authorId": "145773710",
        "name": "A. Marmarou"
      },
      {
        "authorId": "1804814",
        "name": "M. Bergsneider"
      },
      {
        "authorId": "2103773",
        "name": "P. Klinge"
      },
      {
        "authorId": "2333779",
        "name": "N. Relkin"
      },
      {
        "authorId": "34604331",
        "name": "P. Black"
      }
    ]
  },
  {
    "paperId": "828b95885a129619928d227604b63c477ebf7d0e",
    "title": "Generating test data for killing SQL mutants: A constraint-based approach",
    "abstract": "Complex SQL queries are widely used today, but it is rather difficult to check if a complex query has been written correctly. Formal verification based on comparing a specification with an implementation is not applicable, since SQL queries are essentially a specification without any implementation. Queries are usually checked by running them on sample datasets and checking that the correct result is returned; there is no guarantee that all possible errors are detected. In this paper, we address the problem of test data generation for checking correctness of SQL queries, based on the query mutation approach for modeling errors. Our presentation focuses in particular on a class of join/outer-join mutations, comparison operator mutations, and aggregation operation mutations, which are a common cause of error. To minimize human effort in testing, our techniques generate a test suite containing small and intuitive test datasets. The number of datasets generated, is linear in the size of the query, although the number of mutations in the class we consider is exponential. Under certain assumptions on constraints and query constructs, the test suite we generate is complete for a subclass of mutations that we define, i.e., it kills all non-equivalent mutations in this subclass.",
    "year": 2011,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "http://www.cse.iitb.ac.in/~sudarsha/Pubs-dir/MutantKillingICDE2011.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDE.2011.5767876?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDE.2011.5767876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2011-04-11",
    "authors": [
      {
        "authorId": "2013464495",
        "name": "Shetal Shah"
      },
      {
        "authorId": "2066549925",
        "name": "S. Sudarshan"
      },
      {
        "authorId": "2566341",
        "name": "Suhas Kajbaje"
      },
      {
        "authorId": "50460125",
        "name": "S. Patidar"
      },
      {
        "authorId": "37490757",
        "name": "B. P. Gupta"
      },
      {
        "authorId": "40490679",
        "name": "Devang Vira"
      }
    ]
  },
  {
    "paperId": "1a634a769b7e0ac96253f947f1cfd2892a7eec67",
    "title": "Comparing query logs and pseudo-relevance feedbackfor web-search query refinement",
    "abstract": "Query logs and pseudo-relevance feedback (PRF) offer ways in which terms to refine Web searchers' queries can be selected, offered to searchers, and used to improve search effectiveness. In this poster we present a study of these techniques that aims to characterize the degree of similarity between them across a set of test queries, and the same set broken out by query type. The results suggest that: (i) similarity increases with the amount of evidence provided to the PRF algorithm, (ii) similarity is higherwhen titles/snippets are used for PRF than full-text, and (iii) similarity is higher for navigational than informational queries. The findings have implications for the combined usage of query logs and PRF in generating query refinement alternatives.",
    "year": 2007,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1277741.1277931?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1277741.1277931, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2007-07-23",
    "authors": [
      {
        "authorId": "34286525",
        "name": "Ryen W. White"
      },
      {
        "authorId": "1751287",
        "name": "C. Clarke"
      },
      {
        "authorId": "2672967",
        "name": "Silviu Cucerzan"
      }
    ]
  },
  {
    "paperId": "f520a917dc00f82f7f983ac48c87028962cda523",
    "title": "A framework for testing query transformation rules",
    "abstract": "In order to enable extensibility, modern query optimizers typically leverage a transformation rule based framework. Testing individual rule correctness as well as correctness of rule interactions is crucial in verifying the functionality of a query optimizer. While there has been a lot of work on how to architect optimizers for extensibility using a rule based framework, there has been relatively little work on how to test such optimizers. In this paper we present a framework for testing query transformation rules which enables: (a) efficient generation of queries that exercise a particular transformation rule or a set of rules and (b) efficient execution of corresponding test suites for correctness testing.",
    "year": 2009,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1559845.1559874?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1559845.1559874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-06-29",
    "authors": [
      {
        "authorId": "1691015",
        "name": "Hicham G. Elmongui"
      },
      {
        "authorId": "1765803",
        "name": "Vivek R. Narasayya"
      },
      {
        "authorId": "1708341",
        "name": "Ravishankar Ramamurthy"
      }
    ]
  },
  {
    "paperId": "a3796732b5b338280d33a01a0c80c58310bf9fd1",
    "title": "USE OF A POSTCARD QUERY IN MAIL SURVEYS",
    "abstract": "The primary purpose of a recently completed study was to develop and test methods for the collection of survey data on cases of cystic fibrosis known to physicians and outpatient departments of hospitals. One of the experiments included in the survey was designed to test two alternative mail-survey procedures for collecting data on cystic fibrosis patients known to physicians. Knowledge of the usual pattern of medical care received by patients with this disease made it appear unlikely that many physicians not specializing in pediatrics would have seen such patients. Therefore, it was hoped that a query form which appeared simple to complete and minimized the work required of the physician who had seen no patient with cystic fibrosis would elicit a higher response rate at a lower cost than would a longer questionnaire designed primarily for the physician who might have one or more cases to report. In order to test this procedure, the sample of physicians other than pediatricians (referred to hereafter as \"nonpediatricians\") was randomly subdivided into two subsamples. For one subsample, a twopage query was used. The first page contained a covering letter, as well as two screening questions designed to ascertain whether the physician had seen any cases of cystic fibrosis in his private practice and the number of such cases, if any. Space was then provided on the second page for the reporting of names, demographic characteristics, and diagnostic findings for as many as four separate patients. This form was also used in querying the pediatricians in the sample. The other subsample of nonpediatricians received a single-page covering letter and a return postcard which contained just the two screening questions drawn from the longer form. In the event that a physician who responded on the postcard form reported having seen one or more cases of cystic fibrosis, a subsequent procedure provided for a supplementary query requesting the full set of data needed for each such patient. Three basic questions were raised with regard to this experiment. First, did the postcard form elicit a higher response rate to the first mailing wave, and was the differential response, if any, also observed",
    "year": 1965,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1086/267366?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1086/267366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Sociology"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2111059109",
        "name": "Morton L. Brown"
      }
    ]
  },
  {
    "paperId": "d89fd0d13422e91789a1f37105633056092f361f",
    "title": "Nodding or Needling: Analyzing Delegate Responsiveness in an Authoritarian Parliament",
    "abstract": "Recent scholarship argues that one solution to ensure longevity and economic growth in an authoritarian regime is to co-opt potential opposition by offering them limited policy influence in a national legislature. Although cooptation theory generates a number of predictions for delegate behavior within an authoritarian parliament, the opacity of such regimes has made empirical confirmation difficult. We resolve this problem by exploiting the transcripts of query sessions in the Vietnamese National Assembly, where delegates question the prime minister and Cabinet members on important issues of the day. Using a content analysis of queries, we offer the first empirical test of delegate behavior in nondemocratic parliaments. We find that some delegates exhibit behavior consistent with cooptation theory by actively participating in sessions, demonstrating criticism of authorities, and responding to the needs of local constituents. Such responsiveness, however, is parameterized by regime rules for nominating, electing, and assigning parliamentary responsibilities to individual delegates.",
    "year": 2010,
    "citationCount": 276,
    "openAccessPdf": {
      "url": "http://journals.cambridge.org/abstract_S0003055410000250",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1017/S0003055410000250?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/S0003055410000250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Sociology"
    ],
    "publicationDate": "2010-08-01",
    "authors": [
      {
        "authorId": "152391656",
        "name": "Edmund J. Malesky"
      },
      {
        "authorId": "118429547",
        "name": "P. Schuler"
      }
    ]
  },
  {
    "paperId": "81aea9ac366273a00ba0124b1c1aa5cb44782727",
    "title": "Test Preparation and Performance: A Self-Regulatory Analysis",
    "abstract": "Abstract The effect of self-regulatory processes on test preparation and performance was examined. The author used a structured 1-to-1 interview to query 62 college students enrolled in the same psychology class about their self-regulatory processes. The author expected that (a) high test scorers would use more self-regulatory processes to enhance their test preparation and performance than would low test scorers; (b) self-regulation would positively affect test performance; and (c) self-regulatory skill, self-efficacy beliefs, and perceived instrumentality would predict subsequent test performance. All hypotheses were supported by the data. The results of this study are discussed from a sociocognitive perspective.",
    "year": 2002,
    "citationCount": 184,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1080/00220970209599501?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00220970209599501, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Psychology"
    ],
    "publicationDate": "2002-01-01",
    "authors": [
      {
        "authorId": "1779347",
        "name": "A. Kitsantas"
      }
    ]
  },
  {
    "paperId": "35546ad33a3d877063a6a5e938c8d9e8a34badb9",
    "title": "Pseudo test collections for learning web search ranking functions",
    "abstract": "Test collections are the primary drivers of progress in information retrieval. They provide yardsticks for assessing the effectiveness of ranking functions in an automatic, rapid, and repeatable fashion and serve as training data for learning to rank models. However, manual construction of test collections tends to be slow, labor-intensive, and expensive. This paper examines the feasibility of constructing web search test collections in a completely unsupervised manner given only a large web corpus as input. Within our proposed framework, anchor text extracted from the web graph is treated as a pseudo query log from which pseudo queries are sampled. For each pseudo query, a set of relevant and non-relevant documents are selected using a variety of web-specific features, including spam and aggregated anchor text weights. The automatically mined queries and judgments form a pseudo test collection that can be used for training ranking functions. Experiments carried out on TREC web track data show that learning to rank models trained using pseudo test collections outperform an unsupervised ranking function and are statistically indistinguishable from a model trained using manual judgments, demonstrating the usefulness of our approach in extracting reasonable quality training data \"for free\".",
    "year": 2011,
    "citationCount": 42,
    "openAccessPdf": {
      "url": "http://www.umiacs.umd.edu/%7Ejimmylin/publications/Asadi_etal_SIGIR2011.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2009916.2010058?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2009916.2010058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2011-07-24",
    "authors": [
      {
        "authorId": "1858497",
        "name": "N. Asadi"
      },
      {
        "authorId": "1680617",
        "name": "Donald Metzler"
      },
      {
        "authorId": "143928505",
        "name": "T. Elsayed"
      },
      {
        "authorId": "145580839",
        "name": "Jimmy J. Lin"
      }
    ]
  },
  {
    "paperId": "42414b70fc61def8adcf5c159604c72e4508e9c1",
    "title": "NaLIR: an interactive natural language interface for querying relational databases",
    "abstract": "In this demo, we present NaLIR, a generic interactive natural language interface for querying relational databases. NaLIR can accept a logically complex English language sentence as query input. This query is first translated into a SQL query, which may include aggregation, nesting, and various types of joins, among other things, and then evaluated against an RDBMS. In this demonstration, we show that NaLIR, while far from being able to pass the Turing test, is perfectly usable in practice, and able to handle even quite complex queries in a variety of application domains. In addition, we also demonstrate how carefully designed interactive communication can avoid misinterpretation with minimum user burden.",
    "year": 2014,
    "citationCount": 167,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2588555.2594519?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2588555.2594519, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2014-06-18",
    "authors": [
      {
        "authorId": "2115373983",
        "name": "Fei Li"
      },
      {
        "authorId": "145531067",
        "name": "H. V. Jagadish"
      }
    ]
  },
  {
    "paperId": "4eff01bc0cca342cd6dbc0ee3a9f730939eac78b",
    "title": "Some 3CNF properties are hard to test",
    "abstract": "For a boolean formula \u03c6 on n variables, the associated property P\u03c6 is the collection of n-bit strings that satisfy \u03c6. We prove that there are 3CNF properties that require a linear number of queries, even for adaptive tests. This contrasts with 2CNF properties that are testable with O(\u221an) queries[7]. Notice that for every bad instance (i.e. an assignment that does not satisfy \u03c6) there is a 3-bit query that witnesses this fact. Nevertheless, finding such a short witness requires a linear number of queries, even for assignments that are very far from satisfying.We provide sufficient conditions for linear properties to be hard to test, and in the course of the proof include a couple of observations which are of independent interest.",
    "year": 2003,
    "citationCount": 156,
    "openAccessPdf": {
      "url": "http://ttic.uchicago.edu/~prahladh/papers/BHR/BHR2005.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/780542.780594?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/780542.780594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2003-06-09",
    "authors": [
      {
        "authorId": "1393608183",
        "name": "Eli Ben-Sasson"
      },
      {
        "authorId": "145352587",
        "name": "P. Harsha"
      },
      {
        "authorId": "1797575",
        "name": "Sofya Raskhodnikova"
      }
    ]
  },
  {
    "paperId": "93b9d22e6b1f3fc05feba3c5c3922a23dce09ea9",
    "title": "IR evaluation methods for retrieving highly relevant documents",
    "abstract": "This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modem large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In- Query I) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.",
    "year": 2000,
    "citationCount": 424,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3130348.3130374?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3130348.3130374, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2000-07-01",
    "authors": [
      {
        "authorId": "2768186",
        "name": "K. J\u00e4rvelin"
      },
      {
        "authorId": "2732839",
        "name": "Jaana Kek\u00e4l\u00e4inen"
      }
    ]
  },
  {
    "paperId": "9485764b45a3ed5a443f02428bceb9681e8b196f",
    "title": "Unit-testing query transformation rules",
    "abstract": "The process of validating the internal functionality of a query optimizer includes the selection of appropriate queries to be used as test cases for exercising and validating specific code paths. Specifically, it is desirable to be able to implement unit-tests for small components of the query optimizer such as the query transformation rules.\n In this paper, we present a practical method that simplifies the creation of test cases for validating query transformation rules in a query optimizer. We present the QRel programming framework, which allows designing test cases based on relational algebra expressions. We show how such a framework can be used to create classes of similar test cases that exercise transformation rules over a variety of relational algebra expressions. Finally, we provide some examples of how QRel is used to validate SQL Server's optimizations for subqueries.",
    "year": 2008,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1385269.1385273?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1385269.1385273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2008-06-13",
    "authors": [
      {
        "authorId": "1989902",
        "name": "Mostafa Elhemali"
      },
      {
        "authorId": "2937106",
        "name": "Leo Giakoumakis"
      }
    ]
  },
  {
    "paperId": "f57783d5611989cb5f4e4eb2ff6b2e06bbadf7f2",
    "title": "Hypertree Decompositions: Questions and Answers",
    "abstract": "In the database context, the hypertree decomposition method is used for query optimization, whereby conjunctive queries having a low degree of cyclicity can be recognized and decomposed automatically, and efficiently evaluated. Hypertree decompositions were introduced at ACM PODS 1999. The present paper reviews' in form of questions and answers' the main relevant concepts and algorithms and surveys selected related work including applications and test results.",
    "year": 2016,
    "citationCount": 78,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2902251.2902309?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2902251.2902309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2016-06-15",
    "authors": [
      {
        "authorId": "1684745",
        "name": "G. Gottlob"
      },
      {
        "authorId": "145236328",
        "name": "G. Greco"
      },
      {
        "authorId": "144491891",
        "name": "N. Leone"
      },
      {
        "authorId": "1764248",
        "name": "Francesco Scarcello"
      }
    ]
  },
  {
    "paperId": "04ed97dd9cc9d589a7f60604e440f0ef99487354",
    "title": "Quantum algorithms for nearest-neighbor methods for supervised and unsupervised learning",
    "abstract": "We present quantum algorithms for performing nearest-neighbor learning and k-means clustering. At the core of our algorithms are fast and coherent quantum methods for computing the Euclidean distance both directly and via the inner product which we couple with methods for performing amplitude estimation that do not require measurement. We prove upper bounds on the number of queries to the input data required to compute such distances and find the nearest vector to a given test example. In the worst case, our quantum algorithms lead to polynomial reductions in query complexity relative to Monte Carlo algorithms. We also study the performance of our quantum nearest-neighbor algorithms on several real-world binary classification tasks and find that the classification accuracy is competitive with classical methods.",
    "year": 2014,
    "citationCount": 137,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/1401.2142",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.26421/QIC15.3-4-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.26421/QIC15.3-4-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationDate": "2014-01-09",
    "authors": [
      {
        "authorId": "2265529875",
        "name": "Nathan Wiebe"
      },
      {
        "authorId": "2265811502",
        "name": "Ashish Kapoor"
      },
      {
        "authorId": "1742466",
        "name": "K. Svore"
      }
    ]
  },
  {
    "paperId": "16f1d0d7aa148dc6f64442130cfe589e7ee0e297",
    "title": "A comparison of algorithms for the identification of specimens using DNA barcodes: examples from gymnosperms",
    "abstract": "In order to use DNA sequences for specimen identification (e.g., barcoding, fingerprinting) an algorithm to compare query sequences with a reference database is needed. Precision and accuracy of query sequence identification was estimated for hierarchical clustering (parsimony and neighbor joining), similarity methods (BLAST, BLAT and megaBLAST), combined clustering/similarity methods (BLAST/parsimony and BLAST/neighbor joining), diagnostic methods (DNA\u2013BAR and DOME ID), and a new method (ATIM). We offer two novel alignment\u2010free algorithmic solutions (DOME ID and ATIM) to identify query sequences for the purposes of DNA barcoding. Publicly available gymnosperm nrITS 2 and plastid matK sequences were used as test data sets. On the test data sets, almost all of the methods were able to accurately identify sequences to genus; however, no method was able to accurately identify query sequences to species at a frequency that would be considered useful for routine specimen identification (42\u201371% unambiguously correct). Clustering methods performed the worst (perhaps due to alignment issues). Similarity methods, ATIM, DNA\u2013BAR, and DOME ID all performed at approximately the same level. Given the relative precision of the algorithms (median\u2003=\u200367% unambiguous), the low accuracy of species\u2010level identification observed could be ascribed to the lack of correspondence between patterns of allelic similarity and species delimitations. Application of DNA barcoding to sequences of CITES listed cycads (Cycadopsida) provides an example of the potential application of DNA barcoding to enforcement of conservation laws.",
    "year": 2007,
    "citationCount": 338,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1096-0031.2006.00126.x",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1111/j.1096-0031.2006.00126.x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1111/j.1096-0031.2006.00126.x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "publicationDate": "2007-02-01",
    "authors": [
      {
        "authorId": "33004873",
        "name": "D. Little"
      },
      {
        "authorId": "145007843",
        "name": "D. Stevenson"
      }
    ]
  },
  {
    "paperId": "e926b32fae2baaa9c879b43e7b2a5334d31a3931",
    "title": "Gleaning non-trivial structural, functional and evolutionary information about proteins by iterative database searches.",
    "abstract": "Using a number of diverse protein families as test cases, we investigate the ability of the recently developed iterative sequence database search method, PSI-BLAST, to identify subtle relationships between proteins that originally have been deemed detectable only at the level of structure-structure comparison. We show that PSI-BLAST can detect many, though not all, of such relationships, but the success critically depends on the optimal choice of the query sequence used to initiate the search. Generally, there is a correlation between the diversity of the sequences detected in the first pass of database screening and the ability of a given query to detect subtle relationships in subsequent iterations. Accordingly, a thorough analysis of protein superfamilies at the sequence level is necessary in order to maximize the chances of gleaning non-trivial structural and functional inferences, as opposed to a single search, initiated, for example, with the sequence of a protein whose structure is available. This strategy is illustrated by several findings, each of which involves an unexpected structural prediction: (i) a number of previously undetected proteins with the HSP70-actin fold are identified, including a highly conserved and nearly ubiquitous family of metal-dependent proteases (typified by bacterial O-sialoglycoprotease) that represent an adaptation of this fold to a new type of enzymatic activity; (ii) we show that, contrary to the previous conclusions, ATP-dependent and NAD-dependent DNA ligases are confidently predicted to possess the same fold; (iii) the C-terminal domain of 3-phosphoglycerate dehydrogenase, which binds serine and is involved in allosteric regulation of the enzyme activity, is shown to typify a new superfamily of ligand-binding, regulatory domains found primarily in enzymes and regulators of amino acid and purine metabolism; (iv) the immunoglobulin-like DNA-binding domain previously identified in the structures of transcription factors NFkappaB and NFAT is shown to be a member of a distinct superfamily of intracellular and extracellular domains with the immunoglobulin fold; and (v) the Rag-2 subunit of the V-D-J recombinase is shown to contain a kelch-type beta-propeller domain which rules out its evolutionary relationship with bacterial transposases.",
    "year": 1999,
    "citationCount": 451,
    "openAccessPdf": {
      "url": "http://pdfs.semanticscholar.org/e926/b32fae2baaa9c879b43e7b2a5334d31a3931.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1006/JMBI.1999.2653?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1006/JMBI.1999.2653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "publicationDate": "1999-04-16",
    "authors": [
      {
        "authorId": "144750405",
        "name": "L. Aravind"
      },
      {
        "authorId": "2967695",
        "name": "E. Koonin"
      }
    ]
  },
  {
    "paperId": "6f3fce7cecdd9d83421ecb87788393474aaac7dc",
    "title": "Lexical ambiguity and information retrieval",
    "abstract": "Lexical ambiguity is a pervasive problem in natural language processing. However, little quantitative information is available about the extent of the problem or about the impact that it has on information retrieval systems. We report on an analysis of lexical ambiguity in information retrieval test collections and on experiments to determine the utility of word meanings for separating relevant from nonrelevant documents. The experiments show that there is considerable ambiguity even in a specialized database. Word senses provide a significant separation between relevant and nonrelevant documents, but several factors contribute to determining whether disambiguation will make an improvement in performance. For example, resolving lexical ambiguity was found to have little impact on retrieval effectiveness for documents that have many words in common with the query. Other uses of word sense disambiguation in an information retrieval context are discussed.",
    "year": 1992,
    "citationCount": 496,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/146802.146810",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/146802.146810?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/146802.146810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1992-04-01",
    "authors": [
      {
        "authorId": "1872595",
        "name": "Robert Krovetz"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      }
    ]
  },
  {
    "paperId": "8751bf53c5f32db3e040516433a49b991aba62ac",
    "title": "IncSQL: Training Incremental Text-to-SQL Parsers with Non-Deterministic Oracles",
    "abstract": "We present a sequence-to-action parsing approach for the natural language to SQL task that incrementally fills the slots of a SQL query with feasible actions from a pre-defined inventory. To account for the fact that typically there are multiple correct SQL queries with the same or very similar semantics, we draw inspiration from syntactic parsing techniques and propose to train our sequence-to-action models with non-deterministic oracles. We evaluate our models on the WikiSQL dataset and achieve an execution accuracy of 83.7% on the test set, a 2.1% absolute improvement over the models trained with traditional static oracles assuming a single correct target SQL query. When further combined with the execution-guided decoding strategy, our model sets a new state-of-the-art performance at an execution accuracy of 87.1%.",
    "year": 2018,
    "citationCount": 63,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1809.05054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2018-09-13",
    "authors": [
      {
        "authorId": "2929835",
        "name": "Tianze Shi"
      },
      {
        "authorId": "145847769",
        "name": "Kedar Tatwawadi"
      },
      {
        "authorId": "145328899",
        "name": "K. Chakrabarti"
      },
      {
        "authorId": "145469202",
        "name": "Yi Mao"
      },
      {
        "authorId": "2636739",
        "name": "Oleksandr Polozov"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      }
    ]
  },
  {
    "paperId": "0496b636c5304ccd37321b5e04c13aadb9a5fbc0",
    "title": "An exploration of proximity measures in information retrieval",
    "abstract": "In most existing retrieval models, documents are scored primarily based on various kinds of term statistics such as within-document frequencies, inverse document frequencies, and document lengths. Intuitively, the proximity of matched query terms in a document can also be exploited to promote scores of documents in which the matched query terms are close to each other. Such a proximity heuristic, however, has been largely under-explored in the literature; it is unclear how we can model proximity and incorporate a proximity measure into an existing retrieval model. In this paper,we systematically explore the query term proximity heuristic. Specifically, we propose and study the effectiveness of five different proximity measures, each modeling proximity from a different perspective. We then design two heuristic constraints and use them to guide us in incorporating the proposed proximity measures into an existing retrieval model. Experiments on five standard TREC test collections show that one of the proposed proximity measures is indeed highly correlated with document relevance, and by incorporating it into the KL-divergence language model and the Okapi BM25 model, we can significantly improve retrieval performance.",
    "year": 2007,
    "citationCount": 272,
    "openAccessPdf": {
      "url": "http://cmapsconverted.ihmc.us/rid=1228287458424_589005935_15512/exploration of proximity measures.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1277741.1277794?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1277741.1277794, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2007-07-23",
    "authors": [
      {
        "authorId": "2056234035",
        "name": "Tao Tao"
      },
      {
        "authorId": "1736467",
        "name": "ChengXiang Zhai"
      }
    ]
  },
  {
    "paperId": "ea67d1167bd93f0ccbe061e144e6f23069a182aa",
    "title": "User Variability and IR System Evaluation",
    "abstract": "Test collection design eliminates sources of user variability to make statistical comparisons among information retrieval (IR) systems more affordable. Does this choice unnecessarily limit generalizability of the outcomes to real usage scenarios? We explore two aspects of user variability with regard to evaluating the relative performance of IR systems, assessing effectiveness in the context of a subset of topics from three TREC collections, with the embodied information needs categorized against three levels of increasing task complexity. First, we explore the impact of widely differing queries that searchers construct for the same information need description. By executing those queries, we demonstrate that query formulation is critical to query effectiveness. The results also show that the range of scores characterizing effectiveness for a single system arising from these queries is comparable or greater than the range of scores arising from variation among systems using only a single query per topic. Second, our experiments reveal that searchers display substantial individual variation in the numbers of documents and queries they anticipate needing to issue, and there are underlying significant differences in these numbers in line with increasing task complexity levels. Our conclusion is that test collection design would be improved by the use of multiple query variations per topic, and could be further improved by the use of metrics which are sensitive to the expected numbers of useful documents.",
    "year": 2015,
    "citationCount": 77,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2766462.2767728?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2766462.2767728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2015-08-09",
    "authors": [
      {
        "authorId": "144753756",
        "name": "P. Bailey"
      },
      {
        "authorId": "144448479",
        "name": "Alistair Moffat"
      },
      {
        "authorId": "1732541",
        "name": "Falk Scholer"
      },
      {
        "authorId": "41202995",
        "name": "Paul Thomas"
      }
    ]
  },
  {
    "paperId": "f8bccc19aa77ccda25b8e09097e3fe76dde5a3bc",
    "title": "DNA Barcoding for Species Assignment: The Case of Mediterranean Marine Fishes",
    "abstract": "Background DNA barcoding enhances the prospects for species-level identifications globally using a standardized and authenticated DNA-based approach. Reference libraries comprising validated DNA barcodes (COI) constitute robust datasets for testing query sequences, providing considerable utility to identify marine fish and other organisms. Here we test the feasibility of using DNA barcoding to assign species to tissue samples from fish collected in the central Mediterranean Sea, a major contributor to the European marine ichthyofaunal diversity. Methodology/Principal Findings A dataset of 1278 DNA barcodes, representing 218 marine fish species, was used to test the utility of DNA barcodes to assign species from query sequences. We tested query sequences against 1) a reference library of ranked DNA barcodes from the neighbouring North East Atlantic, and 2) the public databases BOLD and GenBank. In the first case, a reference library comprising DNA barcodes with reliability grades for 146 fish species was used as diagnostic dataset to screen 486 query DNA sequences from fish specimens collected in the central basin of the Mediterranean Sea. Of all query sequences suitable for comparisons 98% were unambiguously confirmed through complete match with reference DNA barcodes. In the second case, it was possible to assign species to 83% (BOLD-IDS) and 72% (GenBank) of the sequences from the Mediterranean. Relatively high intraspecific genetic distances were found in 7 species (2.2%\u201318.74%), most of them of high commercial relevance, suggesting possible cryptic species. Conclusion/Significance We emphasize the discriminatory power of COI barcodes and their application to cases requiring species level resolution starting from query sequences. Results highlight the value of public reference libraries of reliability grade-annotated DNA barcodes, to identify species from different geographical origins. The ability to assign species with high precision from DNA samples of disparate quality and origin has major utility in several fields, from fisheries and conservation programs to control of fish products authenticity.",
    "year": 2014,
    "citationCount": 116,
    "openAccessPdf": {
      "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0106135&type=printable",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC4164363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Medicine",
      "Biology"
    ],
    "publicationDate": "2014-09-15",
    "authors": [
      {
        "authorId": "36101817",
        "name": "M. Landi"
      },
      {
        "authorId": "6918116",
        "name": "M. Dimech"
      },
      {
        "authorId": "5898323",
        "name": "M. Arculeo"
      },
      {
        "authorId": "37524060",
        "name": "G. Biondo"
      },
      {
        "authorId": "34026772",
        "name": "R. Martins"
      },
      {
        "authorId": "145980574",
        "name": "M. Carneiro"
      },
      {
        "authorId": "144132083",
        "name": "G. Carvalho"
      },
      {
        "authorId": "4716323",
        "name": "Sabrina Lo Brutto"
      },
      {
        "authorId": "47946393",
        "name": "F. Costa"
      }
    ]
  },
  {
    "paperId": "cac053312603a785b23abe9b7998910af7485d6b",
    "title": "Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries",
    "abstract": "We focus on learning the desired objective function for a robot. Although trajectory demonstrations can be very informative of the desired objective, they can also be difficult for users to provide. Answers to comparison queries, asking which of two trajectories is preferable, are much easier for users, and have emerged as an effective alternative. Unfortunately, comparisons are far less informative. We propose that there is much richer information that users can easily provide and that robots ought to leverage. We focus on augmenting comparisons with feature queries, and introduce a unified formalism for treating all answers as observations about the true desired reward. We derive an active query selection algorithm, and test these queries in simulation and on real users. We find that richer, feature-augmented queries can extract more information faster, leading to robots that better match user preferences in their behavior.",
    "year": 2018,
    "citationCount": 56,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1802.01604",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1802.01604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2018-02-05",
    "authors": [
      {
        "authorId": "7843744",
        "name": "Chandrayee Basu"
      },
      {
        "authorId": "144772132",
        "name": "M. Singhal"
      },
      {
        "authorId": "2745001",
        "name": "A. Dragan"
      }
    ]
  },
  {
    "paperId": "d891409cfaddf6f9f2203d24e088731010f9a6fe",
    "title": "Using Semi-Joins to Solve Relational Queries",
    "abstract": "The semi-join is a relational algebraic operation that selects a set of tuples in one relation that match one or more tuples of another relation on the joining domains. Semi-joins have been used as a basic ingredient in query processing strategies for a number of hardware and software database systems. However, not all queries can be solved entirely using semi-joins. In this paper the exact class of relational queries that can be solved using semi-joins is shown. It is also shown that queries outside of this class may not even be partially solvable using \"short\" semi-join programs. In addition, a linear-time membership test for this class is presented.",
    "year": 1981,
    "citationCount": 490,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/322234.322238",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/322234.322238?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/322234.322238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1737944",
        "name": "P. Bernstein"
      },
      {
        "authorId": "144770850",
        "name": "D. Chiu"
      }
    ]
  },
  {
    "paperId": "18d62040534012818abb90e37eade5dab6dca716",
    "title": "Identifying Well-formed Natural Language Questions",
    "abstract": "Understanding search queries is a hard problem as it involves dealing with \u201cword salad\u201d text ubiquitously issued by users. However, if a query resembles a well-formed question, a natural language processing pipeline is able to perform more accurate interpretation, thus reducing downstream compounding errors. Hence, identifying whether or not a query is well formed can enhance query understanding. Here, we introduce a new task of identifying a well-formed natural language question. We construct and release a dataset of 25,100 publicly available questions classified into well-formed and non-wellformed categories and report an accuracy of 70.7% on the test set. We also show that our classifier can be used to improve the performance of neural sequence-to-sequence models for generating questions for reading comprehension.",
    "year": 2018,
    "citationCount": 49,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/D18-1091.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1808.09419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2018-08-01",
    "authors": [
      {
        "authorId": "1779225",
        "name": "Manaal Faruqui"
      },
      {
        "authorId": "143790066",
        "name": "Dipanjan Das"
      }
    ]
  },
  {
    "paperId": "4cb2f596c796fda6d12248824f68460db84100e4",
    "title": "Unconstrained Pose-Invariant Face Recognition Using 3D Generic Elastic Models",
    "abstract": "Classical face recognition techniques have been successful at operating under well-controlled conditions; however, they have difficulty in robustly performing recognition in uncontrolled real-world scenarios where variations in pose, illumination, and expression are encountered. In this paper, we propose a new method for real-world unconstrained pose-invariant face recognition. We first construct a 3D model for each subject in our database using only a single 2D image by applying the 3D Generic Elastic Model (3D GEM) approach. These 3D models comprise an intermediate gallery database from which novel 2D pose views are synthesized for matching. Before matching, an initial estimate of the pose of the test query is obtained using a linear regression approach based on automatic facial landmark annotation. Each 3D model is subsequently rendered at different poses within a limited search space about the estimated pose, and the resulting images are matched against the test query. Finally, we compute the distances between the synthesized images and test query by using a simple normalized correlation matcher to show the effectiveness of our pose synthesis method to real-world data. We present convincing results on challenging data sets and video sequences demonstrating high recognition accuracy under controlled as well as unseen, uncontrolled real-world scenarios using a fast implementation.",
    "year": 2011,
    "citationCount": 154,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2011.123?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2011.123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationDate": "2011-10-01",
    "authors": [
      {
        "authorId": "39309000",
        "name": "Utsav Prabhu"
      },
      {
        "authorId": "39533703",
        "name": "J. Heo"
      },
      {
        "authorId": "1794486",
        "name": "M. Savvides"
      }
    ]
  },
  {
    "paperId": "abbfe3276cc57824b349e2a6141fcb958c2c9921",
    "title": "Sub-constant error low degree test of almost-linear size",
    "abstract": "Given a function f:Fm\u2192F over a finite field F, a low degree tester tests its agreement with an m-variate polynomial of total degree at most d over F. The tester is usually given access to an oracle A providing the supposed restrictions of f to affine subspaces of constant dimension (e.g., lines, planes, etc.). The tester makes very few (probabilistic) queries to f and to A (say, one query to f and one query to A), and decides whether to accept or reject based on the replies.We wish to minimize two parameters of a tester: its error and its size. The error bounds the probability that the tester accepts although the function is far from a low degree polynomial. The size is the number of bits required to write the oracle replies on all possible tester's queries.Low degree testing is a central ingredient in most constructions of probabilistically checkable proofs (PCPs) and locally testable codes (LTCs). The error of the low degree tester is related to the soundness of the PCP and its size is related to the size of the PCP (or the length of the LTC).We design and analyze new low degree testers that have both sub-constant error o(1) and almost-linear size n1+o(1) (where n=|F|m). Previous constructions of sub-constant error testers had polynomial size [13, 16]. These testers enabled the construction of PCPs with sub-constant soundness, but polynomial size [13, 16, 9]. Previous constructions of almost-linear size testers obtained only constant error [13, 7]. These testers were used to construct almost-linear size LTCs and almost-linear size PCPs with constant soundness [13, 7, 5, 6, 8].",
    "year": 2006,
    "citationCount": 57,
    "openAccessPdf": {
      "url": "http://www.wisdom.weizmann.ac.il/~danamo/papers/low degree test/ldt-conf.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1132516.1132520?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1132516.1132520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2006-05-21",
    "authors": [
      {
        "authorId": "1736268",
        "name": "Dana Moshkovitz"
      },
      {
        "authorId": "1742705",
        "name": "R. Raz"
      }
    ]
  },
  {
    "paperId": "245675a3a711dd3e4c48b1effd3f678868fb259d",
    "title": "Evaluating Retrieval over Sessions: The TREC Session Track 2011-2014",
    "abstract": "Information Retrieval (IR) research has traditionally focused on serving the best results for a single query - so-called ad hoc retrieval. However, users typically search iteratively, refining and reformulating their queries during a session. A key challenge in the study of this interaction is the creation of suitable evaluation resources to assess the effectiveness of IR systems over sessions. This paper describes the TREC Session Track, which ran from 2010 through to 2014, which focussed on forming test collections that included various forms of implicit feedback. We describe the test collections; a brief analysis of the differences between datasets over the years; and the evaluation results that demonstrate that the use of user session data significantly improved effectiveness.",
    "year": 2016,
    "citationCount": 51,
    "openAccessPdf": {
      "url": "http://dl.acm.org/ft_gateway.cfm?id=2914675&type=pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2911451.2914675?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2911451.2914675, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2016-07-07",
    "authors": [
      {
        "authorId": "1750995",
        "name": "Ben Carterette"
      },
      {
        "authorId": "1704149",
        "name": "Paul D. Clough"
      },
      {
        "authorId": "27577175",
        "name": "M. Hall"
      },
      {
        "authorId": "1713134",
        "name": "E. Kanoulas"
      },
      {
        "authorId": "144721996",
        "name": "M. Sanderson"
      }
    ]
  },
  {
    "paperId": "0e73d2b0f943cf8559da7f5002414ccc26bc77cd",
    "title": "Similarity Comparisons for Interactive Fine-Grained Categorization",
    "abstract": "Current human-in-the-loop fine-grained visual categorization systems depend on a predefined vocabulary of attributes and parts, usually determined by experts. In this work, we move away from that expert-driven and attribute-centric paradigm and present a novel interactive classification system that incorporates computer vision and perceptual similarity metrics in a unified framework. At test time, users are asked to judge relative similarity between a query image and various sets of images, these general queries do not require expert-defined terminology and are applicable to other domains and basic-level categories, enabling a flexible, efficient, and scalable system for fine-grained categorization with humans in the loop. Our system outperforms existing state-of-the-art systems for relevance feedback-based image retrieval as well as interactive classification, resulting in a reduction of up to 43% in the average number of questions needed to correctly classify an image.",
    "year": 2014,
    "citationCount": 101,
    "openAccessPdf": {
      "url": "http://ttic.uchicago.edu/%7Esmaji/papers/similarity-cvpr14.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2014.115?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2014.115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2014-06-01",
    "authors": [
      {
        "authorId": "2367820",
        "name": "C. Wah"
      },
      {
        "authorId": "2996914",
        "name": "Grant Van Horn"
      },
      {
        "authorId": "3251767",
        "name": "Steve Branson"
      },
      {
        "authorId": "35208858",
        "name": "Subhransu Maji"
      },
      {
        "authorId": "1690922",
        "name": "P. Perona"
      },
      {
        "authorId": "50172592",
        "name": "Serge J. Belongie"
      }
    ]
  },
  {
    "paperId": "682b7459db9b03a823ff67909bd7cc550ee6acf7",
    "title": "Fang: a firewall analysis engine",
    "abstract": "Today, even a moderately sized corporate intranet contains multiple firewalls and routers, which are all used to enforce various aspects of the global corporate security policy. Configuring these devices to work in unison is difficult, especially if they are made by different vendors. Even testing or reverse engineering an existing configuration (say when a new security administrator takes over) is hard. Firewall configuration files are written in low level formalisms, whose readability is comparable to assembly code, and the global policy is spread over all the firewalls that are involved. To alleviate some of these difficulties, we designed and implemented a novel firewall analysis tool. Our software allows the administrator to easily discover and test the global firewall policy (either a deployed policy or a planned one). Our tool uses a minimal description of the network topology and directly parses the various vendor-specific low level configuration files. It interacts with the user through a query-and-answer session, which is conducted at a much higher level of abstruction. A typical question our tool can answer is \"from which machines can our DMZ be reached and with which services?\" Thus, the tool complements existing vulnerability analysis tools, as it can be used before a policy is actually deployed it operates on a more understandable level of abstraction, and it deals with all the firewalls at once.",
    "year": 2000,
    "citationCount": 322,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/SECPRI.2000.848455?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SECPRI.2000.848455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2000-05-14",
    "authors": [
      {
        "authorId": "2758626",
        "name": "Alain J. Mayer"
      },
      {
        "authorId": "1796565",
        "name": "A. Wool"
      },
      {
        "authorId": "49806926",
        "name": "Elisha Ziskind"
      }
    ]
  },
  {
    "paperId": "3f00db337bc2a8d8234e13a7dfc597731e2d47d2",
    "title": "Prediction of local structure in proteins using a library of sequence-structure motifs.",
    "abstract": "We describe a new method for local protein structure prediction based on a library of short sequence pattern that correlate strongly with protein three-dimensional structural elements. The library was generated using an automated method for finding correlations between protein sequence and local structure, and contains most previously described local sequence-structure correlations as well as new relationships, including a diverging type-II beta-turn, a frayed helix, and a proline-terminated helix. The query sequence is scanned for segments 7 to 19 residues in length that strongly match one of the 82 patterns in the library. Matching segments are assigned the three-dimensional structure characteristic of the corresponding sequence pattern, and backbone torsion angles for the entire query sequence are then predicted by piecing together mutually compatible segment predictions. In predictions of local structure in a test set of 55 proteins, about 50% of all residues, and 76% of residues covered by high-confidence predictions, were found in eight-residue segments within 1.4 A of their true structures. The predictions are complementary to traditional secondary structure predictions because they are considerably more specific in turn regions, and may contribute to ab initio tertiary structure prediction and fold recognition.",
    "year": 1998,
    "citationCount": 349,
    "openAccessPdf": {
      "url": "http://www.bioinfo.rpi.edu/~bystrc/pdf/Bystroff_98_jmb.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1006/JMBI.1998.1943?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1006/JMBI.1998.1943, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "publicationDate": "1998-08-21",
    "authors": [
      {
        "authorId": "1693824",
        "name": "C. Bystroff"
      },
      {
        "authorId": "2191555833",
        "name": "David Baker"
      }
    ]
  },
  {
    "paperId": "b0ca1ab22943e184dc2c2e91284efae3552f2e56",
    "title": "Explaining Wrong Queries Using Small Examples",
    "abstract": "For testing the correctness of SQL queries, a standard practice is to execute the query in question on some test database instance and compare its result with that of the correct query. Given two queries $Q_1$ and $Q_2$, we say that a database instance D is a counterexample (for $Q_1$ and $Q_2$) if $Q_1(D)$ differs from $Q_2(D)$; such a counterexample can serve as an explanation of why $Q_1$ and $Q_2$ are not equivalent. While the test database instance may serve as a counterexample, it may be too large or complex to understand where the inequivalence arises. Therefore, in this paper, given a known counterexample D for $Q_1$ and $Q_2$, we aim to find the smallest counterexample $D' \\subseteq D$ where $Q_1(D') \\neq Q_2(D')$. The problem in general is NP-hard. Drawing techniques from provenance and constraint solving, we develop a suite of algorithms for finding small counterexamples for different classes of queries, including those involving negation and aggregation. We evaluate the effectiveness and scalability of our algorithms on student queries from an undergraduate database course, and on queries from the TPC-H benchmark. We also report a user study from the course where we deployed our tool to help students with an assignment on relational algebra.",
    "year": 2019,
    "citationCount": 35,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3299869.3319866",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1904.04467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationDate": "2019-04-09",
    "authors": [
      {
        "authorId": "2594274",
        "name": "Zhengjie Miao"
      },
      {
        "authorId": "31938009",
        "name": "Sudeepa Roy"
      },
      {
        "authorId": "9125547",
        "name": "Jun Yang"
      }
    ]
  },
  {
    "paperId": "bc3831bd9f065b93eb82607f11f4d7d8ad2b6148",
    "title": "Learning concept importance using a weighted dependence model",
    "abstract": "Modeling query concepts through term dependencies has been shown to have a significant positive effect on retrieval performance, especially for tasks such as web search, where relevance at high ranks is particularly critical. Most previous work, however, treats all concepts as equally important, an assumption that often does not hold, especially for longer, more complex queries. In this paper, we show that one of the most effective existing term dependence models can be naturally extended by assigning weights to concepts. We demonstrate that the weighted dependence model can be trained using existing learning-to-rank techniques, even with a relatively small number of training queries. Our study compares the effectiveness of both endogenous (collection-based) and exogenous (based on external sources) features for determining concept importance. To test the weighted dependence model, we perform experiments on both publicly available TREC corpora and a proprietary web corpus. Our experimental results indicate that our model consistently and significantly outperforms both the standard bag-of-words model and the unweighted term dependence model, and that combining endogenous and exogenous features generally results in the best retrieval effectiveness.",
    "year": 2010,
    "citationCount": 197,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1718487.1718492?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1718487.1718492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2010-02-04",
    "authors": [
      {
        "authorId": "1815447",
        "name": "Michael Bendersky"
      },
      {
        "authorId": "1680617",
        "name": "Donald Metzler"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      }
    ]
  },
  {
    "paperId": "2d593e925d5b08dfb6fa0a5479eefe246d98f28a",
    "title": "Metamorphic Model-Based Testing Applied on NASA DAT -- An Experience Report",
    "abstract": "Testing is necessary for all types of systems, but becomes difficult when the tester cannot easily determine whether the system delivers the correct result or not. NASA's Data Access Toolkit allows NASA analysts to query a large database of telemetry data. Since the user is unfamiliar with the data and several data transformations can occur, it is impossible to determine whether the system behaves correctly or not in full scale production situations. Small scale testing was already conducted manually by other teams and unit testing was conducted on individual functions. However, there was still a need for full scale acceptance testing on a broad scale. We describe how we addressed this testing problem by applying the idea of metamorphic testing [1]. Specifically, we base it on equivalence of queries and by using the system itself for testing. The approach is implemented using a model-based testing approach in combination with a test data generation and test case outcome analysis strategy. We also discuss some of the issues that were detected using this approach.",
    "year": 2015,
    "citationCount": 69,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.5555/2819009.2819030?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5555/2819009.2819030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2015-05-16",
    "authors": [
      {
        "authorId": "1736314",
        "name": "M. Lindvall"
      },
      {
        "authorId": "1809711",
        "name": "D. Ganesan"
      },
      {
        "authorId": "2008249",
        "name": "Ragnar Ardal"
      },
      {
        "authorId": "38095687",
        "name": "Robert E. Wiegand"
      }
    ]
  },
  {
    "paperId": "d9c0ff32895a19c04c6d767ff680c07a649d7748",
    "title": "Non-adaptive probabilistic group testing with noisy measurements: Near-optimal bounds with efficient algorithms",
    "abstract": "We consider the problem of detecting a small subset of defective items from a large set via non-adaptive \u201crandom pooling\u201d group tests. We consider both the case when the measurements are noiseless, and the case2 when the measurements are noisy (the outcome of each group test may be independently faulty with probability q). Order-optimal results for these scenarios are known in the literature. We give information-theoretic lower bounds on the query complexity of these problems, and provide corresponding computationally efficient algorithms that match the lower bounds up to a constant factor. To the best of our knowledge this work is the first to explicitly estimate such a constant that characterizes the gap between the upper and lower bounds for these problems.",
    "year": 2011,
    "citationCount": 147,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1107.4540",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1107.4540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2011-07-22",
    "authors": [
      {
        "authorId": "2515535",
        "name": "C. L. Chan"
      },
      {
        "authorId": "2260909",
        "name": "Pak Hou Che"
      },
      {
        "authorId": "47846882",
        "name": "S. Jaggi"
      },
      {
        "authorId": "1699322",
        "name": "Venkatesh Saligrama"
      }
    ]
  },
  {
    "paperId": "b005d880638d69a7a7d1e4ae941884d9cbccbb16",
    "title": "Anytime Ranking for Impact-Ordered Indexes",
    "abstract": "The ability for a ranking function to control its own execution time is useful for managing load, reigning in outliers, and adapting to different types of queries. We propose a simple yet effective anytime algorithm for impact-ordered indexes that builds on a score-at-a-time query evaluation strategy. In our approach, postings segments are processed in decreasing order of their impact scores, and the algorithm early terminates when a specified number of postings have been processed. With a simple linear model and a few training topics, we can determine this threshold given a time budget in milliseconds. Experiments on two web test collections show that our approach can accurately control query evaluation latency and that aggressive limits on execution time lead to minimal decreases in effectiveness.",
    "year": 2015,
    "citationCount": 55,
    "openAccessPdf": {
      "url": "https://cs.uwaterloo.ca/%7Ejimmylin/publications/Lin_Trotman_ICTIR2015.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2808194.2809477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2808194.2809477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2015-09-27",
    "authors": [
      {
        "authorId": "145580839",
        "name": "Jimmy J. Lin"
      },
      {
        "authorId": "145980720",
        "name": "A. Trotman"
      }
    ]
  },
  {
    "paperId": "293245f34b25d92b44a964591d9c8402aa5150c8",
    "title": "Paratome: an online tool for systematic identification of antigen-binding regions in antibodies based on sequence or structure",
    "abstract": "Antibodies are capable of specifically recognizing and binding antigens. Identification of the antigen-binding site, commonly dubbed paratope, is of high importance both for medical and biological applications. To date, the identification of antigen-binding regions (ABRs) relies on tools for the identification of complementarity-determining regions (CDRs). However, we have shown that up to 22% of the residues that actually bind the antigen fall outside the traditionally defined CDRs. The Paratome web server predicts the ABRs of an antibody, given its amino acid sequence or 3D structure. It is based on a set of consensus regions derived from a structural alignment of a non-redundant set of all known antibody\u2013antigen complexes. Given a query sequence or structure, the server identifies the regions in the query antibody that correspond to the consensus ABRs. An independent set of antibody\u2013antigen complexes was used to test the server and it was shown to correctly identify at least 94% of the antigen-binding residues. The Paratome web server is freely available at http://www.ofranlab.org/paratome/.",
    "year": 2012,
    "citationCount": 131,
    "openAccessPdf": {
      "url": "https://academic.oup.com/nar/article-pdf/40/W1/W521/18783945/gks480.pdf",
      "status": "GOLD",
      "license": "CCBYNC",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC3394289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine",
      "Biology"
    ],
    "publicationDate": "2012-06-06",
    "authors": [
      {
        "authorId": "1712781",
        "name": "Vered Kunik"
      },
      {
        "authorId": "2471271",
        "name": "Shaul Ashkenazi"
      },
      {
        "authorId": "1730408",
        "name": "Yanay Ofran"
      }
    ]
  },
  {
    "paperId": "f855a3813c8c6d0f7a69f4234f4e56d95a4b0611",
    "title": "Evaluating diversified search results using per-intent graded relevance",
    "abstract": "Search queries are often ambiguous and/or underspecified. To accomodate different user needs, search result diversification has received attention in the past few years. Accordingly, several new metrics for evaluating diversification have been proposed, but their properties are little understood. We compare the properties of existing metrics given the premises that (1) queries may have multiple intents; (2) the likelihood of each intent given a query is available; and (3) graded relevance assessments are available for each intent. We compare a wide range of traditional and diversified IR metrics after adding graded relevance assessments to the TREC 2009 Web track diversity task test collection which originally had binary relevance assessments. Our primary criterion is discriminative power, which represents the reliability of a metric in an experiment. Our results show that diversified IR experiments with a given number of topics can be as reliable as traditional IR experiments with the same number of topics, provided that the right metrics are used. Moreover, we compare the intuitiveness of diversified IR metrics by closely examining the actual ranked lists from TREC. We show that a family of metrics called D#-measures have several advantages over other metrics such as \u03b1-nDCG and Intent-Aware metrics.",
    "year": 2011,
    "citationCount": 163,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2009916.2010055?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2009916.2010055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2011-07-24",
    "authors": [
      {
        "authorId": "1725544",
        "name": "T. Sakai"
      },
      {
        "authorId": "35119829",
        "name": "Ruihua Song"
      }
    ]
  },
  {
    "paperId": "211d7f4534bb3d98893fad74e0690e17b1861942",
    "title": "A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval",
    "abstract": "Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collection.",
    "year": 2001,
    "citationCount": 283,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3130348.3130377?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3130348.3130377, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2001-09-01",
    "authors": [
      {
        "authorId": "1736467",
        "name": "ChengXiang Zhai"
      },
      {
        "authorId": "1739581",
        "name": "J. Lafferty"
      }
    ]
  },
  {
    "paperId": "43cc6d9dc5f02ef55850dfeed7440b744501ddde",
    "title": "A \u201cstring of feature graphs\u201d model for recognition of complex activities in natural videos",
    "abstract": "Videos usually consist of activities involving interactions between multiple actors, sometimes referred to as complex activities. Recognition of such activities requires modeling the spatio-temporal relationships between the actors and their individual variabilities. In this paper, we consider the problem of recognition of complex activities in a video given a query example. We propose a new feature model based on a string representation of the video which respects the spatio-temporal ordering. This ordered arrangement of local collections of features (e.g., cuboids, STIP), which are the characters in the string, are initially matched using graph-based spectral techniques. Final recognition is obtained by matching the string representations of the query and the test videos in a dynamic programming framework which allows for variability in sampling rates and speed of activity execution. The method does not require tracking or recognition of body parts, is able to identify the region of interest in a cluttered scene, and gives reasonable performance with even a single query example. We test our approach in an example-based video retrieval framework with two publicly available complex activity datasets and provide comparisons against other methods that have studied this problem.",
    "year": 2011,
    "citationCount": 158,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2011.6126548?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2011.6126548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2011-11-06",
    "authors": [
      {
        "authorId": "38437731",
        "name": "Utkarsh Gaur"
      },
      {
        "authorId": "144079770",
        "name": "Yingying Zhu"
      },
      {
        "authorId": "145178895",
        "name": "Bi Song"
      },
      {
        "authorId": "1404727582",
        "name": "A. Roy-Chowdhury"
      }
    ]
  },
  {
    "paperId": "24e13eec93021e886a876134e5cee1270ea70cc5",
    "title": "Context-aware ranking in web search",
    "abstract": "The context of a search query often provides a search engine meaningful hints for answering the current query better. Previous studies on context-aware search were either focused on the development of context models or limited to a relatively small scale investigation under a controlled laboratory setting. Particularly, about context-aware ranking for Web search, the following two critical problems are largely remained unsolved. First, how can we take advantage of different types of contexts in ranking? Second, how can we integrate context information into a ranking model? In this paper, we tackle the above two essential problems analytically and empirically. We develop different ranking principles for different types of contexts. Moreover, we adopt a learning-to-rank approach and integrate the ranking principles into a state-of-the-art ranking model by encoding the context information as features of the model. We empirically test our approach using a large search log data set obtained from a major commercial search engine. Our evaluation uses both human judgments and implicit user click data. The experimental results clearly show that our context-aware ranking approach improves the ranking of a commercial search engine which ignores context information. Furthermore, our method outperforms a baseline method which considers context information in ranking.",
    "year": 2010,
    "citationCount": 180,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1835449.1835525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1835449.1835525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2010-07-19",
    "authors": [
      {
        "authorId": "47013419",
        "name": "B. Xiang"
      },
      {
        "authorId": "71790825",
        "name": "Daxin Jiang"
      },
      {
        "authorId": "145525190",
        "name": "J. Pei"
      },
      {
        "authorId": "2112329648",
        "name": "Xiaohui Sun"
      },
      {
        "authorId": "2227868312",
        "name": "Enhong Chen"
      },
      {
        "authorId": "49404233",
        "name": "Hang Li"
      }
    ]
  },
  {
    "paperId": "ab294caba43b5c8c5a7aebe97f62c0f8c842c99c",
    "title": "Using graded relevance assessments in IR evaluation",
    "abstract": "This article proposes evaluation methods based on the use of nondichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) generalized recall and precision based directly on multiple grade relevance assessments (i.e., not dichotomizing the assessments). We demonstrate the use of the traditional and the novel evaluation measures in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (InQuery1) in a text database consisting of newspaper articles. To gain insight into the retrieval process, one should use both graded relevance assessments and effectiveness measures that enable one to observe the differences, if any, between retrieval methods in retrieving documents of different levels of relevance. In modern times of information overload, one should pay attention, in particular, to the capability of retrieval methods retrieving highly relevant documents.",
    "year": 2002,
    "citationCount": 263,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/asi.10137",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1002/asi.10137?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/asi.10137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2002-11-01",
    "authors": [
      {
        "authorId": "2732839",
        "name": "Jaana Kek\u00e4l\u00e4inen"
      },
      {
        "authorId": "2768186",
        "name": "K. J\u00e4rvelin"
      }
    ]
  },
  {
    "paperId": "cfa60348c131d45b43f2b78fe4b251381e725203",
    "title": "On Delaying Collision Checking in PRM Planning: Application to Multi-Robot Coordination",
    "abstract": "This paper describes the foundations and algorithms of a new probabilistic roadmap (PRM) planner that is: single-query\u2014instead of pre-computing a roadmap covering the entire free space, it uses the two input query configurations to explore as little space as possible; bi-directional\u2014it explores the robot's free space by building a roadmap made of two trees rooted at the query configurations; and lazy in checking collisions\u2014it delays collision tests along the edges of the roadmap until they are absolutely needed. Several observations motivated this strategy: (1) PRM planners spend a large fraction of their time testing connections for collision; (2) most connections in a roadmap are not on the final path; (3) the collision test for a connection is most expensive when there is no collision; and (4) any short connection between two collision-free configurations has high prior probability of being collision-free. The strengths of single-query and bi-directional sampling techniques and those of delayed collision checking reinforce each other. Experimental results show that this combination reduces planning time by a large factor, making it possible to efficiently handle difficult planning problems, such as problems involving multiple robots in geometrically complex environments. This paper specifically describes the application of the planner to multi-robot planning and compares results obtained when the planner uses a centralized planning approach (PRM planning is then performed in the joint configuration space of the robots) and when it uses a decoupled approach (the PRM planner is invoked several times, first to compute a path of each robot independent of the others, and then to coordinate those paths). On a simulated six-robot welding station combining 36 degrees of freedom, centralized planning has proven to be a much more effective approach than decoupled planning.",
    "year": 2002,
    "citationCount": 264,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1177/027836402320556458?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/027836402320556458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationDate": "2002-01-01",
    "authors": [
      {
        "authorId": "1398136403",
        "name": "Gildardo S\u00e1nchez-Ante"
      },
      {
        "authorId": "1694248",
        "name": "J. Latombe"
      }
    ]
  },
  {
    "paperId": "4dee4429e06cf8bb86de907e0f140789c264cc88",
    "title": "The automatic identification of stop words",
    "abstract": "A stop word may be identified as a word that has the same likehhood of occurring in those documents not relevant to a query as in those documents relevant to the query. In this paper we show how the concept of relevance may be replaced by the condition of being highly rated by a similarity measure. Thus it becomes possible to identify the stop words in a cullectmn by automated statistical testing. We describe the nature of the statistical test as it is realized with a vector retrieval methodology based on the cosine coefficient of document-document similarity. As an example, this tech nique is then applied to a large MEDLINE \" subset in the area of biotechnology. The initial processing of this datahase involves a 310 word stop list of common non-content terms. Our technique is then applied and 75% of the remaining terms are identified as stop words. We compare retrieval with and without the removal of these stop words and find that of the top twenty documents retrieved in response to a random query document. seventeen of these are the same on the average for the two methods We also examine the differences and conclude that where the user prefers one method over the other, the new method with the reduced term set is favored about three times out of four.",
    "year": 1992,
    "citationCount": 328,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1177/016555159201800106?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/016555159201800106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1992-02-01",
    "authors": [
      {
        "authorId": "33528084",
        "name": "John Wilbur"
      },
      {
        "authorId": "2182166",
        "name": "K. Sirotkin"
      }
    ]
  },
  {
    "paperId": "2e6efca73c6dd21776dc798a6d3ef0f3f42312a2",
    "title": "Undergraduate Perceptions of Information Literacy: Defining, Attaining, and Self-Assessing Skills",
    "abstract": "This investigation uses interview data on student conceptions of and experiences with interacting with information. In addition, student scores on the Information Literacy Test (ILT) provide data that allow for comparison of student perceptions to their level of information literacy as measured by a standardized test. A relational approach, informed by competency theory and the imposed query model, provide the framework for the study design and interpretation of the data. Findings reveal a general view of information literacy focused on product rather than process, a perception of achieving information skills on their own, a preference for people over other information sources, and an emphasis on personal interest as key to successful information seeking.",
    "year": 2009,
    "citationCount": 172,
    "openAccessPdf": {
      "url": "https://doi.org/10.5860/crl.70.4.336",
      "status": "GOLD",
      "license": "CCBYNC",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.5860/CRL.70.4.336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5860/CRL.70.4.336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Psychology"
    ],
    "publicationDate": "2009-07-01",
    "authors": [
      {
        "authorId": "40030853",
        "name": "M. Gross"
      },
      {
        "authorId": "3227767",
        "name": "D. Latham"
      }
    ]
  },
  {
    "paperId": "431fedc6093bc2fdf512632e582410d008743e99",
    "title": "Repeatable evaluation of search services in dynamic environments",
    "abstract": "In dynamic environments, such as the World Wide Web, a changing document collection, query population, and set of search services demands frequent repetition of search effectiveness (relevance) evaluations. Reconstructing static test collections, such as in TREC, requires considerable human effort, as large collection sizes demand judgments deep into retrieved pools. In practice it is common to perform shallow evaluations over small numbers of live engines (often pairwise, engine A vs. engine B) without system pooling. Although these evaluations are not intended to construct reusable test collections, their utility depends on conclusions generalizing to the query population as a whole. We leverage the bootstrap estimate of the reproducibility probability of hypothesis tests in determining the query sample sizes required to ensure this, finding they are much larger than those required for static collections. We propose a semiautomatic evaluation framework to reduce this effort. We validate this framework against a manual evaluation of the top ten results of ten Web search engines across 896 queries in navigational and informational tasks. Augmenting manual judgments with pseudo-relevance judgments mined from Web taxonomies reduces both the chances of missing a correct pairwise conclusion, and those of finding an errant conclusion, by approximately 50%.",
    "year": 2007,
    "citationCount": 255,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1292591.1292592?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1292591.1292592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2007-11-01",
    "authors": [
      {
        "authorId": "1912899",
        "name": "Eric C. Jensen"
      },
      {
        "authorId": "3037749",
        "name": "S. Beitzel"
      },
      {
        "authorId": "144087841",
        "name": "Abdur Chowdhury"
      },
      {
        "authorId": "1741208",
        "name": "O. Frieder"
      }
    ]
  },
  {
    "paperId": "4a9bae935672f017800d8a0c954212411ec250d5",
    "title": "Human-In-The-Loop Automatic Program Repair",
    "abstract": "We introduce LEARN2FIX, the first human-in-the-loop, semi-automatic repair technique when no bug oracle\u2013except for the user who is reporting the bug\u2013is available. Our approach negotiates with the user the condition under which the bug is observed. Only when a budget of queries to the user is exhausted, it attempts to repair the bug. A query can be thought of as the following question: \u201cWhen executing this alternative test input, the program produces the following output; is the bug observed\u201d? Through systematic queries, LEARN2FIX trains an automatic bug oracle that becomes increasingly more accurate in predicting the user\u2019s response. Our key challenge is to maximize the oracle\u2019s accuracy in predicting which tests are bug-revealing given a small budget of queries. From the alternative tests that were labeled by the user, test-driven automatic repair produces the patch. Our experiments demonstrate that LEARN2FIX learns a sufficiently accurate automatic oracle with a reasonably low labeling effort (lt. 20 queries). Given LEARN2FIX\u2019s test suite, the GenProg test-driven repair tool produces a higher-quality patch (i.e., passing a larger proportion of validation tests) than using manual test suites provided with the repair benchmark.",
    "year": 2019,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1912.07758",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1912.07758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2019-12-16",
    "authors": [
      {
        "authorId": "34775038",
        "name": "Marcel B\u00f6hme"
      },
      {
        "authorId": "1469099639",
        "name": "Charaka Geethal"
      },
      {
        "authorId": "2123149",
        "name": "Van-Thuan Pham"
      }
    ]
  },
  {
    "paperId": "773572004ede02f61897699192142c5f7732256e",
    "title": "On the Efficiency of FHE-Based Private Queries",
    "abstract": "Private query processing is a very attractive problem in the fields of both cryptography and databases. In this work, we restrict our attention to the efficiency aspect of the problem, particularly for basic queries with conditions on various combinations of <italic>equality</italic>. Without loss of generality, these conditions can be regarded as a Boolean function, and this Boolean function can then be evaluated at ciphertexts produced by a fully homomorphic encryption (FHE) scheme <italic>without decryption</italic>. From the efficiency perspective, the remaining concern is to efficiently test the equality function without severely downgrading the performance of FHE-based querying solutions. To this end, we first analyze the multiplicative depth required for an equality test algorithm with respect to the plaintext space inhabited by general FHE schemes. The primary reason for this approach is that given an equality test algorithm, its efficiency is measured in terms of the multiplicative depth required to construct its arithmetic circuit expression. Indeed, the implemented equality test algorithm dominates the entire performance of FHE-based query solutions, apart from the performance of the underlying FHE scheme. Then, we measure the multiplicative depth considering an FHE scheme that takes an extension field as its plaintext space and that supports the depth-free evaluation of Frobenius maps. According to our analysis, when the plaintext space of an FHE scheme is a field of characteristic <inline-formula><tex-math notation=\"LaTeX\">$2$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq1-2568182.gif\"/></alternatives></inline-formula>, the equality test algorithm for <inline-formula><tex-math notation=\"LaTeX\">$\\ell$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq2-2568182.gif\"/></alternatives></inline-formula>-bit messages requires the lowest multiplicative depth <inline-formula><tex-math notation=\"LaTeX\">$\\lceil \\log {\\ell}\\rceil$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq3-2568182.gif\"/></alternatives></inline-formula>. Furthermore, we design a set of private query protocols for conjunctive, disjunctive, and threshold queries based on the equality test algorithm. Similarly, applying the equality test algorithm over <inline-formula><tex-math notation=\"LaTeX\">$\\mathbb {F}_{2^{\\ell}}$</tex-math><alternatives><inline-graphic xlink:href=\"lee-ieq4-2568182.gif\"/></alternatives> </inline-formula>, our querying protocols require the minimum depths. More specifically, a multiplicative depth of <inline-formula><tex-math notation=\"LaTeX\">$\\lceil \\log {\\ell}\\rceil +\\lceil \\log {(1+\\rho)}\\rceil$</tex-math> <alternatives><inline-graphic xlink:href=\"lee-ieq5-2568182.gif\"/></alternatives></inline-formula> is required for conjunctive and disjunctive queries, and a depth of <inline-formula><tex-math notation=\"LaTeX\">$\\lceil \\log {\\ell}\\rceil +2\\lceil \\log {(1+\\rho)}\\rceil$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq6-2568182.gif\"/></alternatives></inline-formula> is required for threshold conjunctive queries, when their query conditions have <inline-formula><tex-math notation=\"LaTeX\">$\\rho$</tex-math> <alternatives><inline-graphic xlink:href=\"lee-ieq7-2568182.gif\"/></alternatives></inline-formula> attributes to be compared. Finally, we provide a communication-efficient version of our solutions, though with additional computational costs, when an upper bound <inline-formula><tex-math notation=\"LaTeX\">$\\delta$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq8-2568182.gif\"/></alternatives></inline-formula>\u00a0(<inline-formula> <tex-math notation=\"LaTeX\">$0\\leq \\delta \\leq 1$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq9-2568182.gif\"/></alternatives></inline-formula>) on the selectivity of a database is given. Consequently, we reduce the communication cost from <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math> <alternatives><inline-graphic xlink:href=\"lee-ieq10-2568182.gif\"/></alternatives></inline-formula> to approximately <inline-formula><tex-math notation=\"LaTeX\">$\\lfloor \\delta n\\rfloor$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq11-2568182.gif\"/></alternatives></inline-formula> ciphertexts with <inline-formula> <tex-math notation=\"LaTeX\">$\\lceil \\log {n}\\rceil$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq12-2568182.gif\"/></alternatives></inline-formula> additional depth when the database consists of <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math><alternatives> <inline-graphic xlink:href=\"lee-ieq13-2568182.gif\"/></alternatives></inline-formula> tuples.",
    "year": 2016,
    "citationCount": 41,
    "openAccessPdf": {
      "url": "http://eprint.iacr.org/2015/1176.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/TDSC.2016.2568182?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TDSC.2016.2568182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2016-05-12",
    "authors": [
      {
        "authorId": "3022263",
        "name": "Myungsun Kim"
      },
      {
        "authorId": "2110209822",
        "name": "H. Lee"
      },
      {
        "authorId": "143630342",
        "name": "S. Ling"
      },
      {
        "authorId": "1576102218",
        "name": "Huaxiong Wang"
      }
    ]
  },
  {
    "paperId": "4e508dbca0243da1f1def590060bec0e45c6478d",
    "title": "The Impact of Equivalent Mutants",
    "abstract": "If a mutation is not killed by a test suite, this usuallymeans that the test suite is not adequate. However, itmay also be that the mutant keeps the program\u2019s seman-tics unchanged\u2014and thus cannot be detected by any test.We found such equivalent mutants to be surprisingly com-mon: In an experiment on the JAXEN XPATH query engine,8/20 = 40% of all mutations turned out to be equivalent.Worse, checking the equivalency took us 15 minutes for asingle mutation. Equivalent mutants thus make it impossi-ble to automatically assess test suites by means of mutationtesting. To identify equivalent mutants, we are currently investi-gating the impact of a mutation on the execution: the morea mutation alters the execution, the higher the chance of itbeing non-equivalent. First experiments assessing the im-pact on code coverage are promising.",
    "year": 2009,
    "citationCount": 156,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSTW.2009.37?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSTW.2009.37, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-04-01",
    "authors": [
      {
        "authorId": "2338818977",
        "name": "Bernhard J. M. Gr\u00fcn"
      },
      {
        "authorId": "39627250",
        "name": "David Schuler"
      },
      {
        "authorId": "2248160771",
        "name": "Andreas Zeller"
      }
    ]
  },
  {
    "paperId": "fc9344147e3ddd8dd8ec86452961745df50e5c1e",
    "title": "Cross-lingual relevance models",
    "abstract": "We propose a formal model of Cross-Language Information Retrieval that does not rely on either query translation or document translation. Our approach leverages recent advances in language modeling to directly estimate an accurate topic model in the target language, starting with a query in the source language. The model integrates popular techniques of disambiguation and query expansion in a unified formal framework. We describe how the topic model can be estimated with either a parallel corpus or a dictionary. We test the framework by constructing Chinese topic models from English queries and using them in the CLIR task of TREC9. The model achieves performance around 95% of the strong mono-lingual baseline in terms of average precision. In initial precision, our model outperforms the mono-lingual baseline by 20%. The main contribution of this work is the unified formal model which integrates techniques that are essential for effective Cross-Language Retrieval.",
    "year": 2002,
    "citationCount": 245,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/564376.564408?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/564376.564408, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2002-08-11",
    "authors": [
      {
        "authorId": "1757708",
        "name": "V. Lavrenko"
      },
      {
        "authorId": "1751618",
        "name": "M. Choquette"
      },
      {
        "authorId": "144456145",
        "name": "W. Bruce Croft"
      }
    ]
  },
  {
    "paperId": "4f361cc1751f6260dfe676d3c1c547c80505b5d0",
    "title": "The power of geometric duality",
    "abstract": "This paper uses a new formulation of the notion of duality that allows the unified treatment of a number of geometric problems. In particular, we are able to apply our approach to solve two long-standing problems of computational geometry: one is to obtain a quadratic algorithm for computing the minimum-area triangle with vertices chosen amongn points in the plane; the other is to produce an optimal algorithm for the half-plane range query problem. This problem is to preprocessn points in the plane, so that given a test half-plane, one can efficiently determine all points lying in the half-plane. We describe an optimalO(k + logn) time algorithm for answering such queries, wherek is the number of points to be reported. The algorithm requiresO(n) space andO(n logn) preprocessing time. Both of these results represent significant improvements over the best methods previously known. In addition, we give a number of new combinatorial results related to the computation of line arrangements.",
    "year": 1983,
    "citationCount": 348,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/SFCS.1983.75?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SFCS.1983.75, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "1983-11-07",
    "authors": [
      {
        "authorId": "1730838",
        "name": "B. Chazelle"
      },
      {
        "authorId": "2231847421",
        "name": "L. Guibas"
      },
      {
        "authorId": "32433071",
        "name": "Der-Tsai Lee"
      }
    ]
  },
  {
    "paperId": "42b2b4e0a296641bc54d8a7260fc839de046df85",
    "title": "Distribution-free junta testing",
    "abstract": "We study the problem of testing whether an unknown n-variable Boolean function is a k-junta in the distribution-free property testing model, where the distance between functions is measured with respect to an arbitrary and unknown probability distribution over {0,1}n. Our first main result is that distribution-free k-junta testing can be performed, with one-sided error, by an adaptive algorithm that uses \u00d5(k2)/\u0454 queries (independent of n). Complementing this, our second main result is a lower bound showing that any non-adaptive distribution-free k-junta testing algorithm must make \u03a9(2k/3) queries even to test to accuracy \u0454=1/3. These bounds establish that while the optimal query complexity of non-adaptive k-junta testing is 2\u0398(k), for adaptive testing it is poly(k), and thus show that adaptivity provides an exponential improvement in the distribution-free query complexity of testing juntas.",
    "year": 2018,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3188745.3188842",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/1802.04859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationDate": "2018-02-13",
    "authors": [
      {
        "authorId": "2145307359",
        "name": "Xi Chen"
      },
      {
        "authorId": "2145313070",
        "name": "Zhengyang Liu"
      },
      {
        "authorId": "1729835",
        "name": "R. Servedio"
      },
      {
        "authorId": "2053803566",
        "name": "Ying Sheng"
      },
      {
        "authorId": "2216565964",
        "name": "Jinyu Xie"
      }
    ]
  },
  {
    "paperId": "cb6d15ddc0ff247e3e4e59ef54a5e81fa6146fb8",
    "title": "Towards context-aware search by learning a very large variable length hidden markov model from search logs",
    "abstract": "Capturing the context of a user's query from the previous queries and clicks in the same session may help understand the user's information need. A context-aware approach to document re-ranking, query suggestion, and URL recommendation may improve users' search experience substantially. In this paper, we propose a general approach to context-aware search. To capture contexts of queries, we learn a variable length Hidden Markov Model (vlHMM) from search sessions extracted from log data. Although the mathematical model is intuitive, how to learn a large vlHMM with millions of states from hundreds of millions of search sessions poses a grand challenge. We develop a strategy for parameter initialization in vlHMM learning which can greatly reduce the number of parameters to be estimated in practice. We also devise a method for distributed vlHMM learning under the map-reduce model. We test our approach on a real data set consisting of 1.8 billion queries, 2.6 billion clicks, and 840 million search sessions, and evaluate the effectiveness of the vlHMM learned from the real data on three search applications: document re-ranking, query suggestion, and URL recommendation. The experimental results show that our approach is both effective and efficient.",
    "year": 2009,
    "citationCount": 130,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1526709.1526736?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1526709.1526736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-04-20",
    "authors": [
      {
        "authorId": "4205363",
        "name": "Huanhuan Cao"
      },
      {
        "authorId": "1753226",
        "name": "Daxin Jiang"
      },
      {
        "authorId": "2143960747",
        "name": "Jian Pei"
      },
      {
        "authorId": "2288152743",
        "name": "Enhong Chen"
      },
      {
        "authorId": "2339488325",
        "name": "Hang Li"
      }
    ]
  },
  {
    "paperId": "3487526289074cdd3f2224c34e04d1838c06cc4c",
    "title": "Advanced Topics In Database Research",
    "abstract": "Database applications features such as Structured Query Language programming, exception handling, integrity constraints, and table triggers pose difficulties for maintenance activities, especially for regression testing that follows modifying database applications. In this chapter, we address these difficulties and propose a two-phase regression testing methodology. In phase 1, we explore control flow and data flow analysis issues of database applications. Then, we propose an impact analysis technique that is based on dependencies that exist among the components of database applications. This analysis leads to selecting test cases from the initial test suite for regression testing the modified application. In phase 2, we propose two algorithms for reducing the number of regression test cases. The Graph Walk algorithm walks through the control flow graph of database modules and selects a safe set of test cases to retest. The Call Graph Firewall algorithm uses a firewall for the inter-procedural level. Our experience with this regression testing methodology shows that the impact analysis technique is adequate for selecting regression tests and that phase 2 techniques can be used for further reduction in the number of these tests.",
    "year": 2005,
    "citationCount": 243,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.4018/978-1-59140-255-8.CH008?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4018/978-1-59140-255-8.CH008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2005-03-01",
    "authors": [
      {
        "authorId": "1746885",
        "name": "K. Siau"
      }
    ]
  },
  {
    "paperId": "8cd697c76b5b9ea1d389fd6aad3fb013da303a36",
    "title": "Direct Sum Testing",
    "abstract": "The k-fold direct sum encoding of a string \u03b1 \u2208 --0,1}n is a function f\u03b1 that takes as input sets S \u2286 [n] of size k and outputs f\u03b1 (S) = \u2211i \u2208 S \u03b1i (mod 2. In this paper we prove a Direct Sum Testing theorem. We describe a three query test that accepts with probability one any function of the form f\u03b1 for some \u03b1, and rejects with probability \u03a9(\u03b5) functions f that are \u03b5 being a direct sum encoding. This theorem has a couple of additional guises: Linearity testing: By identifying the subsets of [n] with vectors in --0,1}ne natural way, our result can be thought of as a linearity testing theorem for functions whose domain is restricted to the k'th layer of the hypercube (i.e. the set of n-bit strings with Hamming weight k). Tensor power testing: By moving to --1,1 notation, the direct sum encoding is equivalent (up to a difference that is negligible when k \u00ab \u221an) to a tensor power. Thus our theorem implies a three query test for deciding if a given tensor \u03b1 \u2208 --- 1,1}nk is a tensor power of a single dimensional vector \u03b1 \u2208 ----1,1}n, i.e. whether there is some \u03b1 such that f = \u03b1k. We also provide a four query test for checking if a given \u00b11 matrix has rank 1. Our test naturally extends the linearity test of Blum, Luby, and Rubinfeld (STOC '90). Our analysis proceeds by first handling the k=n/2 case, and then reducing this case to the general k < n/2 case, using a recent direct product testing theorem of Dinur and Steurer (CCC '2014). The k < n/2 case is proven via a new proof for linearity testing on the hypercube, which we extend to the restricted domain of the n/2-th layer of the hypercube.",
    "year": 2015,
    "citationCount": 34,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2688073.2688078?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2688073.2688078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2015-01-11",
    "authors": [
      {
        "authorId": "2588655",
        "name": "R. David"
      },
      {
        "authorId": "1699356",
        "name": "Irit Dinur"
      },
      {
        "authorId": "1752311",
        "name": "Elazar Goldenberg"
      },
      {
        "authorId": "2523903",
        "name": "Guy Kindler"
      },
      {
        "authorId": "2065636",
        "name": "Igor Shinkar"
      }
    ]
  },
  {
    "paperId": "18257110908b685268f14facd74c5f4d8cae5bdc",
    "title": "Automatic Text Categorization and Its Application to Text Retrieval",
    "abstract": "We develop an automatic text categorization approach and investigate its application to text retrieval. The categorization approach is derived from a combination of a learning paradigm known as instance-based learning and an advanced document retrieval technique known as retrieval feedback. We demonstrate the effectiveness of our categorization approach using two real-world document collections from the MEDLINE database. Next, we investigate the application of automatic categorization to text retrieval. Our experiments clearly indicate that automatic categorization improves the retrieval performance compared with no categorization. We also demonstrate that the retrieval performance using automatic categorization achieves the same retrieval quality as the performance using manual categorization. Furthermore, detailed analysis of the retrieval performance on each individual test query is provided.",
    "year": 1999,
    "citationCount": 205,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/69.824599?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/69.824599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1999-11-01",
    "authors": [
      {
        "authorId": "144594306",
        "name": "Wai Lam"
      },
      {
        "authorId": "144881417",
        "name": "M. Ruiz"
      },
      {
        "authorId": "144684950",
        "name": "P. Srinivasan"
      }
    ]
  },
  {
    "paperId": "20c0d7c37e5ba906485b1df568a8ded03e0e5c3a",
    "title": "Extending Faceted Search to the General Web",
    "abstract": "Faceted search helps users by offering drill-down options as a complement to the keyword input box, and it has been used successfully for many vertical applications, including e-commerce and digital libraries. However, this idea is not well explored for general web search, even though it holds great potential for assisting multi-faceted queries and exploratory search. In this paper, we explore this potential by extending faceted search into the open-domain web setting, which we call Faceted Web Search. To tackle the heterogeneous nature of the web, we propose to use query-dependent automatic facet generation, which generates facets for a query instead of the entire corpus. To incorporate user feedback on these query facets into document ranking, we investigate both Boolean filtering and soft ranking models. We evaluate Faceted Web Search systems by their utility in assisting users to clarify search intent and find subtopic information. We describe how to build reusable test collections for such tasks, and propose an evaluation method that considers both gain and cost for users. Our experiments testify to the potential of Faceted Web Search, and show Boolean filtering feedback models, which are widely used in conventional faceted search, are less effective than soft ranking models.",
    "year": 2014,
    "citationCount": 60,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2661829.2661964?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2661829.2661964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2014-11-03",
    "authors": [
      {
        "authorId": "40414862",
        "name": "Weize Kong"
      },
      {
        "authorId": "144890574",
        "name": "James Allan"
      }
    ]
  },
  {
    "paperId": "eadb5588755fb37fffa7b1a0bf69a19c71b82f07",
    "title": "WevQuery: Testing Hypotheses about Web Interaction Patterns",
    "abstract": "Remotely stored user interaction logs, which give access to a wealth of data generated by large numbers of users, have been long used to understand if interactive systems meet the expectations of designers. Unfortunately, detailed insight into users' interaction behaviour still requires a high degree of expertise and domain specific knowledge. We present WevQuery, a scalable system to query user interaction logs in order to allow designers to test their hypotheses about users' behaviour. WevQuery supports this purpose using a graphical notation to define the interaction patterns designers are seeking. WevQuery is scalable as the queries can then be executed against large user interaction datasets by employing the MapReduce paradigm. This way WevQuery provides designers effortless access to harvest users' interaction patterns, removing the burden of low-level interaction data analysis. We present two scenarios to showcase the potential of WevQuery, from the design of the queries to their execution on real interaction data accounting for 5.7m events generated by 2,445 unique users.",
    "year": 2017,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "https://pure.manchester.ac.uk/ws/files/56135996/0_mainSingleCol.pdf",
      "status": "GREEN",
      "license": "mit",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/3095806?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3095806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2017-06-30",
    "authors": [
      {
        "authorId": "2884921",
        "name": "Aitor Apaolaza"
      },
      {
        "authorId": "1727424",
        "name": "Markel Vigo"
      }
    ]
  },
  {
    "paperId": "35ef1dc77d4055cc902a9890bc520ea1db92a687",
    "title": "Predicting the effectiveness of queries and retrieval systems",
    "abstract": "We consider users' attempts to express their information needs through queries, or search requests and try to predict whether those requests will be of high or low quality. The second type of methods under investigation are those which attempt to estimate the quality of search systems themselves. Given a number of search systems to consider, these methods estimate how well or how poorly the systems will perform in comparison to each other.\n First, pre-retrieval predictors are investigated, which predict a query's effectiveness before the retrieval step and are thus independent of the ranked list of results. Such predictors base their predictions solely on query terms, collection statistics and possibly external sources. Twenty-two prediction algorithms are categorized and their quality is assessed on three different TREC test collections. A number of newly applied methods for combining various predictors are examined to obtain a better prediction of a query's effectiveness.\n Building on the analysis of pre-retrieval predictors, post-retrieval approaches are then investigated, which estimate a query's effectiveness on the basis of the retrieved results. The thesis focuses in particular on the Clarity Score approach and provides an analysis of its sensitivity towards different variables such as the collection, the query set and the retrieval approach. Adaptations to Clarity Score are introduced which improve the estimation accuracy of the original algorithm.\n The utility of query effectiveness prediction methods is commonly evaluated by reporting correlation coefficients, such as Kendall's Tau. Largely unexplored though is the question of the relationship between the current evaluation methodology for query effectiveness prediction and the change in effectiveness of retrieval systems that employ a predictor. We investigate this question by examining how the observed quality of predictors (with respect to Kendall's Tau) affects the retrieval effectiveness in two adaptive system settings: selective query expansion and meta-search.\n The last part of the thesis is concerned with the task of estimating the ranking of retrieval systems according to their retrieval effectiveness without relying on costly relevance judgments. Five different system ranking estimation approaches are evaluated on a wide range of data sets which cover a variety of retrieval tasks and test collections. It is shown that under certain conditions, automatic methods yield a highly accurate ranking of systems.\n Available online at http://www.cs.utwente.nl/~hauffc/phd/thesis.pdf.",
    "year": 2010,
    "citationCount": 114,
    "openAccessPdf": {
      "url": "https://research.utwente.nl/files/6036833/thesis_C_Hauff.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1842890.1842906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1842890.1842906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2010-08-18",
    "authors": [
      {
        "authorId": "2731925",
        "name": "C. Hauff"
      }
    ]
  },
  {
    "paperId": "41993f597e08e9af3fcbe0a10f192bde7742c4fd",
    "title": "NaLIX: an interactive natural language interface for querying XML",
    "abstract": "Database query languages can be intimidating to the non-expert, leading to the immense recent popularity for keyword based search in spite of its significant limitations. The holy grail has been the development of a natural language query interface. We present NaLIX, a generic interactive natural language query interface to an XML database. Our system can accept an arbitrary English language sentence as query input, which can include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression that can be evaluated against an XML database. The translation is done through mapping grammatical proximity of natural language parsed tokens to proximity of corresponding elements in the result XML. In this demonstration, we show that NaLIX, while far from being able to pass the Turing test, is perfectly usable in practice, and able to handle even quite complex queries in a variety of application domains. In addition, we also demonstrate how carefully designed features in NaLIX facilitate the interactive query process and improve the usability of the interface.",
    "year": 2005,
    "citationCount": 185,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1066157.1066281?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1066157.1066281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2005-06-14",
    "authors": [
      {
        "authorId": "1718694",
        "name": "Yunyao Li"
      },
      {
        "authorId": "2370345",
        "name": "Huahai Yang"
      },
      {
        "authorId": "145531067",
        "name": "H. V. Jagadish"
      }
    ]
  },
  {
    "paperId": "1e0dd12f2bff234a4df71641bc95068733506858",
    "title": "Handwritten Word Spotting with Corrected Attributes",
    "abstract": "We propose an approach to multi-writer word spotting, where the goal is to find a query word in a dataset comprised of document images. We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform query-by-example, where the query is an image, and query-by-string, where the query is a string. We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. We test our approach on two public datasets showing state-of-the-art results.",
    "year": 2013,
    "citationCount": 61,
    "openAccessPdf": {
      "url": "https://hal.inria.fr/hal-00906787/file/handwritten_iccv13.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2013.130?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2013.130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2013-12-01",
    "authors": [
      {
        "authorId": "145467588",
        "name": "Jon Almaz\u00e1n"
      },
      {
        "authorId": "1821267",
        "name": "Albert Gordo"
      },
      {
        "authorId": "1686569",
        "name": "A. Forn\u00e9s"
      },
      {
        "authorId": "2864362",
        "name": "Ernest Valveny"
      }
    ]
  },
  {
    "paperId": "42f1a911434ecd7d213489273212d45506764987",
    "title": "Compressing bitmap indexes for faster search operations",
    "abstract": "We study the effects of compression on bitmap indexes. The main operations on the bitmaps during query processing are bitwise logical operations. Using the general purpose compression schemes the logical operations on the compressed bitmaps are much slower than on the uncompressed bitmaps. Specialized compression schemes, like the byte-aligned bitmap code (BBC), are usually faster in performing logical operations than the general purpose schemes, but in many cases they are still orders of magnitude slower than the uncompressed scheme. To make the compressed bitmap indexes operate more efficiently, we designed a CPU-friendly scheme which we refer to as the word-aligned hybrid code (WAH). Tests on both synthetic and real application data show that the new scheme significantly outperforms well-known compression schemes at a modest increase in storage space. Compared to BBC, WAH performs logical operations about 12 times faster and uses only 60% more space. Compared to the uncompressed scheme, in most test cases WAH is faster while still using less space. We further verified with additional tests that the improvement in logical operation speed translates to similar improvement in query processing speed.",
    "year": 2002,
    "citationCount": 168,
    "openAccessPdf": {
      "url": "https://digital.library.unt.edu/ark:/67531/metadc736363/m2/1/high_res_d/795969.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/SSDM.2002.1029710?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SSDM.2002.1029710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2002-07-24",
    "authors": [
      {
        "authorId": "145389708",
        "name": "Kesheng Wu"
      },
      {
        "authorId": "1743163",
        "name": "E. Otoo"
      },
      {
        "authorId": "1747737",
        "name": "A. Shoshani"
      }
    ]
  },
  {
    "paperId": "70549b74567379bfa2d7b911d8ecf4ce65c4f3df",
    "title": "Optimal Testing of Reed-Muller Codes",
    "abstract": "We consider the problem of testing if a given function $f : \\F_2^n \\right arrow \\F_2$ is close to any degree $d$ polynomial in $n$ variables, also known as the Reed-Muller testing problem. %The Gowers norm is based on a natural $2^{d+1}$-query test for this property. Alon et al.~\\cite{AKKLR} proposed and analyzed a natural $2^{d+1}$-query test for this problem. This test turned out to be intimately related to the Gowers norm. Alon et. al. showed that this test accepts every degree $d$ polynomial with probability $1$, while it rejects functions that are $\\Omega(1)$-far with probability $\\Omega(1/(d 2^{d}))$. We give an asymptotically optimal analysis of this test, and show that it rejects functions that are (even only) $\\Omega(2^{-d})$-far with $\\Omega(1)$-probability (so the rejection probability is a universal constant independent of $d$ and $n$). This implies a tight relationship between the $(d+1)^{\\rm{st}}$-Gowers norm of a function and its maximal correlation with degree $d$ polynomials, when the correlation is close to 1. Our proof works by induction on $n$ and yields a new analysis of even the classical Blum-Luby-Rubinfeld~\\cite{BLR} linearity test, for the setting of functions mapping $\\F_2^n$ to $\\F_2$. The optimality follows from a tighter analysis of counterexamples to the ``inverse conjecture for the Gowers norm'' constructed by \\cite{GT07, LMS}. Our result has several implications. First, it shows that the Gowers norm test is tolerant, in that it also accepts close code words. Second, it improves the parameters of an XOR lemma for polynomials given by Viola and Wigderson~\\cite{VW}. Third, it implies a ``query hierarchy'' result for property testing of affine-invariant properties. That is, for every function $q(n)$, it gives an affine-invariant property that is testable with $O(q(n))$-queries, but not with $o(q(n))$-queries, complementing an analogous result of \\cite{GKNR08} for graph properties.",
    "year": 2009,
    "citationCount": 93,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/0910.0641",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://arxiv.org/abs/0910.0641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2009-10-04",
    "authors": [
      {
        "authorId": "1751082",
        "name": "Arnab Bhattacharyya"
      },
      {
        "authorId": "1767374",
        "name": "Swastik Kopparty"
      },
      {
        "authorId": "1710013",
        "name": "G. Schoenebeck"
      },
      {
        "authorId": "1748838",
        "name": "M. Sudan"
      },
      {
        "authorId": "144879318",
        "name": "David Zuckerman"
      }
    ]
  },
  {
    "paperId": "caecf11d25967f186075bba8f57a6b76d11d58d4",
    "title": "BoltzRank: learning to maximize expected ranking gain",
    "abstract": "Ranking a set of retrieved documents according to their relevance to a query is a popular problem in information retrieval. Methods that learn ranking functions are difficult to optimize, as ranking performance is typically judged by metrics that are not smooth. In this paper we propose a new listwise approach to learning to rank. Our method creates a conditional probability distribution over rankings assigned to documents for a given query, which permits gradient ascent optimization of the expected value of some performance measure. The rank probabilities take the form of a Boltzmann distribution, based on an energy function that depends on a scoring function composed of individual and pairwise potentials. Including pairwise potentials is a novel contribution, allowing the model to encode regularities in the relative scores of documents; existing models assign scores at test time based only on individual documents, with no pairwise constraints between documents. Experimental results on the LETOR3.0 data set show that our method out-performs existing learning approaches to ranking.",
    "year": 2009,
    "citationCount": 113,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1553374.1553513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1553374.1553513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationDate": "2009-06-14",
    "authors": [
      {
        "authorId": "1765951",
        "name": "M. Volkovs"
      },
      {
        "authorId": "1804104",
        "name": "R. Zemel"
      }
    ]
  },
  {
    "paperId": "9fe4dbadccf6e7131cb9694e81b9b1a1c5ee28ab",
    "title": "Test data for relational queries",
    "abstract": "An automatic technique for generating a comprehensive test database for a given query is studied. The test database is large enough to cover all essentially different situations under the given set of dependencies, and also large enough to illustrate the effect of each operation appearing in the query. On the other hand, the database attempts to do this in a minimal way. The method can be applied in the testing of queries, e.g. as an aid in learning a new query language. The basis of the construction is the definition of an adequate test case. We characterize this concept using Armstrong relations and show that adequate examples have the desired properties. We also give a method for producing reasonably small example databases for select-project-join queries where each relation scheme appears at most once in the query.",
    "year": 1985,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/6012.15415",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/6012.15415?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/6012.15415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1985-06-01",
    "authors": [
      {
        "authorId": "1712654",
        "name": "H. Mannila"
      },
      {
        "authorId": "1724448",
        "name": "Kari-Jouko R\u00e4ih\u00e4"
      }
    ]
  },
  {
    "paperId": "b2a9fc7b3c41d4c4b29c41d3e369d50f36f17c0f",
    "title": "Modeling behavioral factors ininteractive information retrieval",
    "abstract": "In real-life, information retrieval consists of sessions of one or more query iterations. Each iteration has several subtasks like query formulation, result scanning, document link clicking, document reading and judgment, and stopping. Each of the subtasks has behavioral factors associated with them. These factors include search goals and cost constraints, query formulation strategies, scanning and stopping strategies, and relevance assessment behav-ior. Traditional IR evaluation focuses on retrieval and result presentation methods, and interaction within a single-query session. In the present study we aim at assessing the effects of the behavioral factors on retrieval effectiveness. Our research questions include how effective is human behavior employing search strategies compared to various baselines under various search goals and time constraints. We examine both ideal as well as fallible human behavior and wish to identify robust behaviors, if any. Methodologically, we use extensive simulation of human behavior in a test collection. Our findings include that (a) human behavior using multi-query sessions may exceed in effectiveness comparable single-query sessions, (b) the same empirically observed behavioral patterns are reasonably effective under various search goals and constraints, but (c) remain on average clearly below the best possible ones. Moreover, there is no behavioral pattern for sessions that would be even close to winning in most cases; the information need (or topic) in relation to the test collection is a determining factor.",
    "year": 2013,
    "citationCount": 50,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/2505515.2505660?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2505515.2505660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2013-10-27",
    "authors": [
      {
        "authorId": "38287335",
        "name": "F. Baskaya"
      },
      {
        "authorId": "1942885",
        "name": "Heikki Keskustalo"
      },
      {
        "authorId": "2768186",
        "name": "K. J\u00e4rvelin"
      }
    ]
  },
  {
    "paperId": "69c8f6757ea5db08c46c0a278337f14c030c292d",
    "title": "On proximity oblivious testing",
    "abstract": "We initiate a systematic study of a special type of property testers. These testers consist of repeating a basic test for a number of times that depends on the proximity parameter, whereas the basic test is oblivious of the proximity parameter. We refer to such basic tests by the term Proximity-Oblivious Testers. While proximity-oblivious testers were studied before - most notably in the algebraic setting - the current study seems to be the first one to focus on graph properties. We provide a mix of positive and negative results, and in particular characterizations of the graph properties that have constant-query proximity-oblivious testers in the two standard models (i.e., the adjacency matrix and the bounded-degree models). Furthermore, we show that constant-query proximity-oblivious testers do not exist for many easily testable properties, and that even when proximity-oblivious testers exist, repeating them does not necessarily yield the best standard testers for the corresponding property.",
    "year": 2009,
    "citationCount": 80,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1536414.1536436?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1536414.1536436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2009-05-31",
    "authors": [
      {
        "authorId": "1707322",
        "name": "Oded Goldreich"
      },
      {
        "authorId": "1735952",
        "name": "D. Ron"
      }
    ]
  },
  {
    "paperId": "ea6fa7a9ed05d72a8debee4449fe141468ba6275",
    "title": "Automated storage and retrieval of thin-section CT images to assist diagnosis: system description and preliminary assessment.",
    "abstract": "A software system and database for computer-aided diagnosis with thin-section computed tomographic (CT) images of the chest was designed and implemented. When presented with an unknown query image, the system uses pattern recognition to retrieve visually similar images with known diagnoses from the database. A preliminary validation trial was conducted with 11 volunteers who were asked to select the best diagnosis for a series of test images, with and without software assistance. The percentage of correct answers increased from 29% to 62% with computer assistance. This finding suggests that this system may be useful for computer-assisted diagnosis.",
    "year": 2003,
    "citationCount": 187,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1148/RADIOL.2281020126?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/RADIOL.2281020126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationDate": "2003-07-01",
    "authors": [
      {
        "authorId": "2910630",
        "name": "A. Aisen"
      },
      {
        "authorId": "34560543",
        "name": "L. S. Broderick"
      },
      {
        "authorId": "1402247492",
        "name": "H. Winer-Muram"
      },
      {
        "authorId": "1729374",
        "name": "C. Brodley"
      },
      {
        "authorId": "1703247",
        "name": "A. Kak"
      },
      {
        "authorId": "2189698",
        "name": "C. Pavlopoulou"
      },
      {
        "authorId": "152313541",
        "name": "J. Dy"
      },
      {
        "authorId": "144154559",
        "name": "C. Shyu"
      },
      {
        "authorId": "2079083887",
        "name": "Alan Marchiori"
      }
    ]
  },
  {
    "paperId": "eb0b8e3cb4ee649c934836a98af1b6b6b3d04027",
    "title": "Information retrieval on Turkish texts",
    "abstract": "We study information retrieval (IR) on Turkish texts using a large-scale test collection that contains 408,305 documents and 72 ad hoc queries. We examine the effects of several stemming options and query-document matching functions on retrieval performance. We show that a simple word truncation approach, a word truncation approach that uses language dependent corpus statistics, and an elaborate lemmatizer-based stemmer provide similar retrieval effectiveness in Turkish IR. We investigate the effects of a range of search conditions on the retrieval performance; these include the scalability issues, query and document length effects, and the use of stopword list in indexing.",
    "year": 2008,
    "citationCount": 106,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1002/asi.20750?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/asi.20750, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2083563",
        "name": "F. Can"
      },
      {
        "authorId": "1906792",
        "name": "Seyit Kocberber"
      },
      {
        "authorId": "1896185",
        "name": "E. Balcik"
      },
      {
        "authorId": "34786610",
        "name": "Cihan Kaynak"
      },
      {
        "authorId": "1666223923",
        "name": "Huseyin Cagdas \u00d6calan"
      },
      {
        "authorId": "2683223",
        "name": "Onur M. Vursavas"
      }
    ]
  },
  {
    "paperId": "08253a5200ed1581592f2c99d65e844604364a62",
    "title": "Testing Fourier Dimensionality and Sparsity",
    "abstract": "We present a range of new results for testing properties of Boolean functions that are defined in terms of the Fourier spectrum. Broadly speaking, our results show that the property of a Boolean function having a concise Fourier representation is locally testable. \n \nWe first give an efficient algorithm for testing whether the Fourier spectrum of a Boolean function is supported in a low-dimensional subspace of ${\\mathbb F}_2^n$ (equivalently, for testing whether f is a junta over a small number of parities). We next give an efficient algorithm for testing whether a Boolean function has a sparse Fourier spectrum (small number of nonzero coefficients). In both cases we also prove lower bounds showing that any testing algorithm -- even an adaptive one -- must have query complexity within a polynomial factor of our algorithms, which are nonadaptive. Finally, we give an \"implicit learning\" algorithm that lets us test any sub-property of Fourier concision. \n \nOur technical contributions include new structural results about sparse Boolean functions and new analysis of the pairwise independent hashing of Fourier coefficients from [12].",
    "year": 2009,
    "citationCount": 89,
    "openAccessPdf": {
      "url": "http://www.cs.cmu.edu/~odonnell/papers/testing-sparsity.pdf",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1137/100785429?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/100785429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2009-07-06",
    "authors": [
      {
        "authorId": "1718214",
        "name": "Parikshit Gopalan"
      },
      {
        "authorId": "1410077373",
        "name": "R. O'Donnell"
      },
      {
        "authorId": "1729835",
        "name": "R. Servedio"
      },
      {
        "authorId": "1787363",
        "name": "Amir Shpilka"
      },
      {
        "authorId": "152842533",
        "name": "K. Wimmer"
      }
    ]
  },
  {
    "paperId": "98ebc0c61e702daf58043aab2abd4a0d10df7240",
    "title": "Nearly-linear size holographic proofs",
    "abstract": "We show how to construct holographic (or transparent) proofs of size nl\u201d that can be checked by a verifier that is allowed to read only O(1) bits of the proof and has access to O(log n) random bits, for all c > 0. In general, we construct proofs of size ~1+2 \u20180(9(\u201d)) (log n)q\u2019(n)l checkable by the query of 2q~(n)) bits, for any q(n) = O(log log n). An essential element of our construction is a proof that the low-degree test used by Arora and Safra [AS92] is effective on domains of size linear in the degree of the encoded polynomial.",
    "year": 1994,
    "citationCount": 195,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/195058.195132?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/195058.195132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1994-05-23",
    "authors": [
      {
        "authorId": "144640239",
        "name": "A. Polishchuk"
      },
      {
        "authorId": "2417095",
        "name": "D. Spielman"
      }
    ]
  },
  {
    "paperId": "27e8515623f9da903e7b5607f10c34b06bead175",
    "title": "Testing Juntas",
    "abstract": "We show that a Boolean function over n Boolean variables can be tested for the property of depending on only k of them, using a number of queries that depends only on k and the approximation parameter \\varepsilon. We present two tests, both non-adaptive, that require a number of queries that is polynomial k and linear in \\varepsilon ^{- 1}. The first test is stronger in that it has a 1-sided error, while the second test has a more compact analysis. We also present an adaptive version and a 2-sided error version of the first test, that have a somewhat better query complexity than the other algorithms.We then provide a lower bound of \\bar \\Omega (\\sqrt k) on the number of queries required for the non-adaptive testing of the above property; a lower bound of \\Omega (\\log (k + 1)) for adaptive algorithms naturally follows from this. In providing this we also prove a result about random walks on the group {\\rm Z}_2^9 that may be interesting in its own right. We show that for some t(q) = \\bar 0(q^2) the distributions of the random walk at times t and t + 2 are close to each other, independently of the step distribution of the walk.We also discuss related questions. In particular, when given in advance a known k-junta function h, we show how to test a function f for the property of being identical to h up to a permutation of the variables, in a number of queries that is polynomial in k and \\varepsilon.",
    "year": 2002,
    "citationCount": 155,
    "openAccessPdf": {
      "url": "https://doi.org/10.1016/j.jcss.2003.11.004",
      "status": "BRONZE",
      "license": "publisher-specific-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/SFCS.2002.1181887?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SFCS.2002.1181887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2002-11-16",
    "authors": [
      {
        "authorId": "2249141372",
        "name": "Eldar Fischer"
      },
      {
        "authorId": "2275582673",
        "name": "Guy Kindler"
      },
      {
        "authorId": "2066496624",
        "name": "D. Ron"
      },
      {
        "authorId": "1697937",
        "name": "S. Safra"
      },
      {
        "authorId": "2079458624",
        "name": "Alex Samorodnitsky"
      }
    ]
  },
  {
    "paperId": "9c539eece138b325112ffb4aaac35d4592bd3f8f",
    "title": "Large-scale content-based audio retrieval from text queries",
    "abstract": "In content-based audio retrieval, the goal is to find sound recordings (audio documents) based on their acoustic features. This content-based approach differs from retrieval approaches that index media files using metadata such as file names and user tags. In this paper, we propose a machine learning approach for retrieving sounds that is novel in that it (1) uses free-form text queries rather sound sample based queries, (2) searches by audio content rather than via textual meta data, and (3) can scale to very large number of audio documents and very rich query vocabulary. We handle generic sounds, including a wide variety of sound effects, animal vocalizations and natural scenes. We test a scalable approach based on a passive-aggressive model for image retrieval (PAMIR), and compare it to two state-of-the-art approaches; Gaussian mixture models (GMM) and support vector machines (SVM).\n We test our approach on two large real-world datasets: a collection of short sound effects, and a noisier and larger collection of user-contributed user-labeled recordings (25K files, 2000 terms vocabulary). We find that all three methods achieved very good retrieval performance. For instance, a positive document is retrieved in the first position of the ranking more than half the time, and on average there are more than 4 positive documents in the first 10 retrieved, for both datasets. PAMIR completed both training and retrieval of all data in less than 6 hours for both datasets, on a single machine. It was one to three orders of magnitude faster than the competing approaches. This approach should therefore scale to much larger datasets in the future.",
    "year": 2008,
    "citationCount": 114,
    "openAccessPdf": {
      "url": "http://ai.stanford.edu/~gal/Papers/chechik_MIR2008.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1460096.1460115?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1460096.1460115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2008-10-30",
    "authors": [
      {
        "authorId": "1732280",
        "name": "Gal Chechik"
      },
      {
        "authorId": "2042413",
        "name": "Eugene Ie"
      },
      {
        "authorId": "8430935",
        "name": "Martin Rehn"
      },
      {
        "authorId": "1751569",
        "name": "Samy Bengio"
      },
      {
        "authorId": "2057663",
        "name": "R. Lyon"
      }
    ]
  },
  {
    "paperId": "598d1b0ad3781183d313fb0f4d09bbf3dbf4ce24",
    "title": "Randomness-efficient low degree tests and short PCPs via epsilon-biased sets",
    "abstract": "We present the first explicit construction of Probabilistically Checkable Proofs (PCPs) and Locally Testable Codes (LTCs) of fixed constant query complexity which have almost-linear (= n * 2\u00d5(\u221alog n)) size. Such objects were recently shown to exist (nonconstructively) by Goldreich and Sudan[17]. Previous explicit constructions required size n1 + \u03a9(\u03b5) with 1/\u03b5 queries. The key to these constructions is a nearly optimal randomness-efficient version of the low degree test[32]. In a similar way we give a randomness-efficient version of the BLR linearity test[13] (which is used, for instance, in locally testing the Hadamard code). The derandomizations are obtained through \u03b5-biased sets for vector spaces over finite fields. The analysis of the derandomized tests rely on alternative views of \u03b5-biased sets --- as generating sets of Cayley expander graphs for the low degree test, and as defining linear error-correcting codes for the linearity test.",
    "year": 2003,
    "citationCount": 142,
    "openAccessPdf": {
      "url": "https://dash.harvard.edu/bitstream/1/2961580/2/Vadhan_EffShortPCPepsilon.pdf",
      "status": "GREEN",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/780542.780631?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/780542.780631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2003-06-09",
    "authors": [
      {
        "authorId": "1393608183",
        "name": "Eli Ben-Sasson"
      },
      {
        "authorId": "1748838",
        "name": "M. Sudan"
      },
      {
        "authorId": "1723744",
        "name": "S. Vadhan"
      },
      {
        "authorId": "1718867",
        "name": "A. Wigderson"
      }
    ]
  },
  {
    "paperId": "227f1a1d39e3b816179f7fd02a5d75454c179a8d",
    "title": "Improving trigram language modeling with the World Wide Web",
    "abstract": "We propose a method for using the World Wide Web to acquire trigram estimates for statistical language modeling. We submit an N-gram as a phrase query to Web search engines. The search engines return the number of Web pages containing the phrase, from which the N-gram count is estimated. The N-gram counts are then used to form Web-based trigram probability estimates. We discuss the properties of such estimates, and methods to interpolate them with traditional corpus based trigram estimates. We show that the interpolated models improve speech recognition word error rate significantly over a small test set.",
    "year": 2001,
    "citationCount": 151,
    "openAccessPdf": {
      "url": "https://figshare.com/articles/report/Improving_Trigram_Language_Modeling_with_the_World_Wide_Web/21710042/1/files/38515460.pdf",
      "status": "GREEN",
      "license": "CCBY",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP.2001.940885?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP.2001.940885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2001-05-07",
    "authors": [
      {
        "authorId": "1832364",
        "name": "Xiaojin Zhu"
      },
      {
        "authorId": "145903504",
        "name": "R. Rosenfeld"
      }
    ]
  },
  {
    "paperId": "9e8a66067ec01e486ea39c0d6bec42fee80a3dc0",
    "title": "A search engine for historical manuscript images",
    "abstract": "Many museum and library archives are digitizing their large collections of handwritten historical manuscripts to enable public access to them. These collections are only available in image formats and require expensive manual annotation work for access to them. Current handwriting recognizers have word error rates in excess of 50% and therefore cannot be used for such material. We describe two statistical models for retrieval in large collections of handwritten manuscripts given a text query. Both use a set of transcribed page images to learn a joint probability distribution between features computed from word images and their transcriptions. The models can then be used to retrieve unlabeled images of handwritten documents given a text query. We show experiments with a training set of 100 transcribed pages and a test set of 987 handwritten page images from the George Washington collection. Experiments show that the precision at 20 documents is about 0.4 to 0.5 depending on the model. To the best of our knowledge, this is the first automatic retrieval system for historical manuscripts using text queries, without manual transcription of the original corpus.",
    "year": 2004,
    "citationCount": 156,
    "openAccessPdf": {
      "url": "https://works.bepress.com/cgi/viewcontent.cgi?article=1057&context=r_manmatha",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1008992.1009056?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1008992.1009056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2004-07-25",
    "authors": [
      {
        "authorId": "2352980",
        "name": "T. Rath"
      },
      {
        "authorId": "1758550",
        "name": "R. Manmatha"
      },
      {
        "authorId": "1757708",
        "name": "V. Lavrenko"
      }
    ]
  },
  {
    "paperId": "0f0da80b240e9e3e68a72caaf6449d04646b5cac",
    "title": "Testing for Concise Representations",
    "abstract": "We describe a general method for testing whether a function on n input variables has a concise representation. The approach combines ideas from the junta test of Fischer et al. 16 with ideas from learning theory, and yields property testers that make po!y(s/epsiv) queries (independent of n) for Boolean function classes such as s-term DNF formulas (answering a question posed by Parnas et al. [12]), sizes. decision trees, sizes Boolean formulas, and sizes Boolean circuits. The method can be applied to non-Boolean valued function classes as well. This is achieved via a generalization of the notion of van at ion/row Fischer et al. to non-Boolean functions. Using this generalization we extend the original junta test of Fischer et al. to work for non-Boolean functions, and give poly(s/e)-query testing algorithms for non-Boolean valued function classes such as sizes algebraic circuits and s-sparse polynomials over finite fields. We also prove an Omega(radic(s)) query lower bound for nonadaptively testing s-sparse polynomials over finite fields of constant size. This shows that in some instances, our general method yields a property tester with query complexity that is optimal (for nonadaptive algorithms) up to a polynomial factor.",
    "year": 2007,
    "citationCount": 124,
    "openAccessPdf": {
      "url": "http://www.cs.columbia.edu/~rocco/Public/focs07long.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/FOCS.2007.32?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/FOCS.2007.32, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2007-10-21",
    "authors": [
      {
        "authorId": "1808354",
        "name": "Ilias Diakonikolas"
      },
      {
        "authorId": "2109158855",
        "name": "Homin K. Lee"
      },
      {
        "authorId": "3347254",
        "name": "Kevin Matulef"
      },
      {
        "authorId": "1730859",
        "name": "Krzysztof Onak"
      },
      {
        "authorId": "145109224",
        "name": "R. Rubinfeld"
      },
      {
        "authorId": "1729835",
        "name": "R. Servedio"
      },
      {
        "authorId": "1790382",
        "name": "Andrew Wan"
      }
    ]
  },
  {
    "paperId": "6be4cf09d79c4ff8fc09ce25088cf3f27840c2c7",
    "title": "On filtering false positive transmembrane protein predictions.",
    "abstract": "While helical transmembrane (TM) region prediction tools achieve high (>90%) success rates for real integral membrane proteins, they produce a considerable number of false positive hits in sequences of known nontransmembrane queries. We propose a modification of the dense alignment surface (DAS) method that achieves a substantial decrease in the false positive error rate. Essentially, a sequence that includes possible transmembrane regions is compared in a second step with TM segments in a sequence library of documented transmembrane proteins. If the performance of the query sequence against the library of documented TM segment-containing sequences in this test is lower than an empirical threshold, it is classified as a non-transmembrane protein. The probability of false positive prediction for trusted TM region hits is expressed in terms of E-values. The modified DAS method, the DAS-TMfilter algorithm, has an unchanged high sensitivity for TM segments ( approximately 95% detected in a learning set of 128 documented transmembrane proteins). At the same time, the selectivity measured over a non-redundant set of 526 soluble proteins with known 3D structure is approximately 99%, mainly because a large number of falsely predicted single membrane-pass proteins are eliminated by the DAS-TMfilter algorithm.",
    "year": 2002,
    "citationCount": 151,
    "openAccessPdf": {
      "url": "https://academic.oup.com/peds/article-pdf/15/9/745/18547016/150745.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1093/PROTEIN/15.9.745?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/PROTEIN/15.9.745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Medicine",
      "Mathematics"
    ],
    "publicationDate": "2002-09-01",
    "authors": [
      {
        "authorId": "5155369",
        "name": "M. Cserz\u00f6"
      },
      {
        "authorId": "1763951",
        "name": "F. Eisenhaber"
      },
      {
        "authorId": "2970868",
        "name": "B. Eisenhaber"
      },
      {
        "authorId": "145547346",
        "name": "I. Simon"
      }
    ]
  },
  {
    "paperId": "cfadbe8eefa435aceec0c208befee00879d16c55",
    "title": "Direct Product Testing",
    "abstract": "A direct product function is a function of the form g(x<sub>1</sub>, \u22ef, x<sub>k</sub>)=(g<sub>1</sub>(x<sub>1</sub>), \u22ef, g(x<sub>k</sub>)). We show that the direct product property is locally testable with two queries, that is, a canonical two-query test distinguishes between direct product functions and functions that are far from direct products with constant probability. This local testing question comes up naturally in the context of PCPs, where direct products play a prominent role for gap amplification. We consider the following natural two query test for a given function f:[N]<sup>k</sup>\u2192[M]<sup>k</sup> Two query direct product test: Choose x, y that agree on a random set A of t coordinates and accept if f(x)<sub>A</sub>=f(y)<sub>A</sub>. We provide a comprehensive analysis of this test for all parameters N, M, k, t\u2264O(k) and success probability \u03b4>0. Our main result is that if a given function f:[N]<sup>k</sup>\u2192[M]<sup>k</sup> passes the test with probability \u03b4\u22651-\u03b5 then there is a direct product function g such that P[f(x)=g(x)]\u22651-O(\u03b5). This is the first result relating success in the above (or any) test to the fraction of the domain on which f is equal to a direct product function. This test has been analyzed in previous works for the case t\u226ak\u226aN, and results show closeness of f to a direct product under a less natural measure of \"approximate agreement\". In the small soundness regime, we prove that if the test above passes with probability \u03b4 \u2265 exp(-k), then the function agrees with a direct product function on local parts of the domain. This extends the previous range of parameters of \u03b4\u2265exp(-3\u221ak) to the entire meaningful range of \u03b4>exp(-k).",
    "year": 2014,
    "citationCount": 36,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/CCC.2014.27?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CCC.2014.27, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationDate": "2014-06-11",
    "authors": [
      {
        "authorId": "1699356",
        "name": "Irit Dinur"
      },
      {
        "authorId": "3238779",
        "name": "David Steurer"
      }
    ]
  },
  {
    "paperId": "7031e30a1d041d8e32f55b526f2039334c662b78",
    "title": "Does Situation Awareness Add to the Validity of Cognitive Tests?",
    "abstract": "Objective: Does adding situation awareness (SA) to a battery of cognitive tests improve prediction? Background: Identifying variables that predict skilled performance in a complex task aids in understanding the nature of skill and also aids in the selection of operators to perform that task. SA is thought to be an important predictor of performance. SA is often thought to be based on underlying cognitive mechanisms. Method: Three performance measures taken from the Federal Aviation Administration's (FAA's) Air Traffic Scenarios Test, the low-fidelity simulation component of the FAA's controller selection battery, were used as criterion variables in a hierarchical regression. After predicting performance based on a battery of cognitive (e.g., intelligence, working memory, spatial memory) and noncognitive tests (e.g., cognitive style, personality, demographics), we added measures of SA. Results: SA did provide increases in prediction, but only when measured with the Situation Present Assessment Method, an on-line query method. When the same questions were asked off line, SA did not enter the model in two cases and improved prediction by only 2% in the third. Conclusion: Thus, some measures of SA do show incremental validity, even against a backdrop of a large number of cognitive variables. Application: On-line measures of SA can be a worthwhile addition to standard batteries of tests used to predict performance in cognitively oriented industrial tasks.",
    "year": 2006,
    "citationCount": 108,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1518/001872006779166316?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1518/001872006779166316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Psychology",
      "Computer Science",
      "Medicine"
    ],
    "publicationDate": "2006-12-01",
    "authors": [
      {
        "authorId": "2065050959",
        "name": "Francis T. Durso"
      },
      {
        "authorId": "2367104",
        "name": "M. Bleckley"
      },
      {
        "authorId": "1915412",
        "name": "Andrew R. Dattel"
      }
    ]
  },
  {
    "paperId": "4deb229a8067ea405857b176f0ddf4b908270f1f",
    "title": "QCS: A system for querying, clustering and summarizing documents",
    "abstract": "Information retrieval systems consist of many complicated components. Research and development of such systems is often hampered by the difficulty in evaluating how each particular component would behave across multiple systems. We present a novel integrated information retrieval system-the Query, Cluster, Summarize (QCS) system-which is portable, modular, and permits experimentation with different instantiations of each of the constituent text analysis components. Most importantly, the combination of the three types of methods in the QCS design improves retrievals by providing users more focused information organized by topic. \n \nWe demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences (DUC) as measured by the best known automatic metric for summarization system evaluation, ROUGE. Although the DUC data and evaluations were originally designed to test multidocument summarization, we developed a framework to extend it to the task of evaluation for each of the three components: query, clustering, and summarization. Under this framework, we then demonstrate that the QCS system (end-to-end) achieves performance as good as or better than the best summarization engines. \n \nGiven a query, QCS retrieves relevant documents, separates the retrieved documents into topic clusters, and creates a single summary for each cluster. In the current implementation, Latent Semantic Indexing is used for retrieval, generalized spherical k-means is used for the document clustering, and a method coupling sentence \"trimming\" and a hidden Markov model, followed by a pivoted QR decomposition, is used to create a single extract summary for each cluster. The user interface is designed to provide access to detailed information in a compact and useful format. \n \nOur system demonstrates the feasibility of assembling an effective IR system from existing software libraries, the usefulness of the modularity of the design, and the value of this particular combination of modules.",
    "year": 2007,
    "citationCount": 102,
    "openAccessPdf": {
      "url": "https://digital.library.unt.edu/ark:/67531/metadc875439/m2/1/high_res_d/893129.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.2172/894745?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2172/894745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2007-11-01",
    "authors": [
      {
        "authorId": "1749902",
        "name": "Daniel M. Dunlavy"
      },
      {
        "authorId": "117489779",
        "name": "D. O\u2019Leary"
      },
      {
        "authorId": "11579914",
        "name": "John M. Conroy"
      },
      {
        "authorId": "2468970",
        "name": "J. Schlesinger"
      }
    ]
  },
  {
    "paperId": "79a29950b07f7fc2664d058734f94b3a638e62a5",
    "title": "An experimental investigation of set intersection algorithms for text searching",
    "abstract": "The intersection of large ordered sets is a common problem in the context of the evaluation of boolean queries to a search engine. In this article, we propose several improved algorithms for computing the intersection of sorted arrays, and in particular for searching sorted arrays in the intersection context. We perform an experimental comparison with the algorithms from the previous studies from Demaine, L\u00f3pez-Ortiz, and Munro [ALENEX 2001] and from Baeza-Yates and Salinger [SPIRE 2005]; in addition, we implement and test the intersection algorithm from Barbay and Kenyon [SODA 2002] and its randomized variant [SAGA 2003]. We consider both the random data set from Baeza-Yates and Salinger, the Google queries used by Demaine et al., a corpus provided by Google, and a larger corpus from the TREC Terabyte 2006 efficiency query stream, along with its own query log. We measure the performance both in terms of the number of comparisons and searches performed, and in terms of the CPU time on two different architectures. Our results confirm or improve the results from both previous studies in their respective context (comparison model on real data, and CPU measures on random data) and extend them to new contexts. In particular, we show that value-based search algorithms perform well in posting lists in terms of the number of comparisons performed.",
    "year": 2009,
    "citationCount": 71,
    "openAccessPdf": {
      "url": "http://www.cs.uwaterloo.ca/%7Eajsaling/papers/fiats.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1498698.1564507?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1498698.1564507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-12-01",
    "authors": [
      {
        "authorId": "3121337",
        "name": "J\u00e9r\u00e9my F\u00e9lix Barbay"
      },
      {
        "authorId": "1398262740",
        "name": "A. L\u00f3pez-Ortiz"
      },
      {
        "authorId": "2409297",
        "name": "Tyler Lu"
      },
      {
        "authorId": "2765127",
        "name": "Alejandro Salinger"
      }
    ]
  },
  {
    "paperId": "055370f1e989a8a726058073dad8dd64866b8882",
    "title": "BBM: bayesian browsing model from petabyte-scale data",
    "abstract": "Given a quarter of petabyte click log data, how can we estimate the relevance of each URL for a given query? In this paper, we propose the Bayesian Browsing Model (BBM), a new modeling technique with following advantages: (a) it does exact inference; (b) it is single-pass and parallelizable; (c) it is effective.\n We present two sets of experiments to test model effectiveness and efficiency. On the first set of over 50 million search instances of 1.1 million distinct queries, BBM out-performs the state-of-the-art competitor by 29.2% in log-likelihood while being 57 times faster. On the second click-log set, spanning a quarter of petabyte data, we showcase the scalability of BBM: we implemented it on a commercial MapReduce cluster, and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs.",
    "year": 2009,
    "citationCount": 74,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1557019.1557081?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1557019.1557081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-06-28",
    "authors": [
      {
        "authorId": "2152504682",
        "name": "Chao Liu"
      },
      {
        "authorId": "2065454171",
        "name": "Fan Guo"
      },
      {
        "authorId": "1702392",
        "name": "C. Faloutsos"
      }
    ]
  },
  {
    "paperId": "c07551bcef50b1d8d3ac7a08b5689e920721824e",
    "title": "Full predicate coverage for testing SQL database queries",
    "abstract": "In the field of database applications a considerable part of the business logic is implemented using a semi\u2010declarative language: the Structured Query Language (SQL). Because of the different semantics of SQL compared with other procedural languages, the conventional coverage criteria for testing are not directly applicable. This paper presents a criterion specifically tailored for SQL queries (SQLFpc). It is based on Masking Modified Condition Decision Coverage (MCDC) or Full Predicate Coverage and takes into account a wide range of the syntax and semantics of SQL, including selection, joining, grouping, aggregations, subqueries, case expressions and null values. The criterion assesses the coverage of the test data in relation to the query that is executed and it is expressed as a set of rules that are automatically generated and efficiently evaluated against a test database. The use of the criterion is illustrated in a case study, which includes complex queries. Copyright \u00a9 2010 John Wiley & Sons, Ltd.",
    "year": 2010,
    "citationCount": 68,
    "openAccessPdf": {
      "url": "http://giis.uniovi.es/testing/papers/stvr-2010-sqlfpc.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1002/stvr.424?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/stvr.424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2010-09-01",
    "authors": [
      {
        "authorId": "1713716",
        "name": "J. Tuya"
      },
      {
        "authorId": "1713277",
        "name": "Mar\u00eda Jos\u00e9 Su\u00e1rez Cabal"
      },
      {
        "authorId": "1738961",
        "name": "C. Riva"
      }
    ]
  },
  {
    "paperId": "78346392b3d157173d7a3b15869e3c107ca3b715",
    "title": "Fundamental techniques for order optimization",
    "abstract": "Decision support applications are growing in popularity as more business data is kept on-line. Such applications typically include complex SQL queries that can test a query optimizer's ability to produce an efficient access plan. Many access plan strategies exploit the physical ordering of data provided by indexes or sorting. Sorting is an expensive operation, however. Therefore, it is imperative that sorting is optimized in some way or avoided all together. Toward that goal, this paper describes novel optimization techniques for pushing down sorts in joins, minimizing the number of sorting columns, and detecting when sorting can be avoided because of predicates, keys, or indexes. A set of fundamental operations is described that provide the foundation for implementing such techniques. The operations exploit data properties that arise from predicate application, uniqueness, and functional dependencies. These operations and techniques have been implemented in IBM's DB2/CS.",
    "year": 1996,
    "citationCount": 126,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/235968.233320",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/233269.233320?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/233269.233320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1996-03-25",
    "authors": [
      {
        "authorId": "32540896",
        "name": "David E. Simmen"
      },
      {
        "authorId": "2915064",
        "name": "E. Shekita"
      },
      {
        "authorId": "2020120",
        "name": "Timothy Malkemus"
      }
    ]
  },
  {
    "paperId": "13281106c65139826841aa501f0c6cebb5e05068",
    "title": "Semantic context transfer across heterogeneous sources for domain adaptive video search",
    "abstract": "Automatic video search based on semantic concept detectors has recently received significant attention. Since the number of available detectors is much smaller than the size of human vocabulary, one major challenge is to select appropriate detectors to response user queries. In this paper, we propose a novel approach that leverages heterogeneous knowledge sources for domain adaptive video search. First, instead of utilizing WordNet as most existing works, we exploit the context information associated with Flickr images to estimate query-detector similarity. The resulting measurement, named Flickr context similarity (FCS), reflects the co-occurrence statistics of words in image context rather than textual corpus. Starting from an initial detector set determined by FCS, our approach novelly transfers semantic context learned from test data domain to adaptively refine the query-detector similarity. The semantic context transfer process provides an effective means to cope with the domain shift between external knowledge source (e.g., Flickr context) and test data, which is a critical issue in video search. To the best of our knowledge, this work represents the first research aiming to tackle the challenging issue of domain change in video search. Extensive experiments on 120 textual queries over TRECVID 2005-2008 data sets demonstrate the effectiveness of semantic context transfer for domain adaptive video search. Results also show that the FCS is suitable for measuring query-detector similarity, producing better performance to various other popular measures.",
    "year": 2009,
    "citationCount": 66,
    "openAccessPdf": {
      "url": "https://ink.library.smu.edu.sg/sis_research/6527",
      "status": "GREEN",
      "license": "CCBYNCND",
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/1631272.1631296?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1631272.1631296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "2009-10-19",
    "authors": [
      {
        "authorId": "1717861",
        "name": "Yu-Gang Jiang"
      },
      {
        "authorId": "143977389",
        "name": "C. Ngo"
      },
      {
        "authorId": "9546964",
        "name": "Shih-Fu Chang"
      }
    ]
  },
  {
    "paperId": "6ac424215edfe3cc6942d00f3d692b7f010b684e",
    "title": "R* optimizer validation and performance evaluation for local queries",
    "abstract": "Few database query optimizer models have been validated against actual performance. This paper presents the methodology and results of a thorough validation of the optimizer and evaluation of the performance of the experimental distributed relational database management system R*, which inherited and extended to a distributed environment the optimization algorithms of System R. Optimizer estimated costs and actual R* resources consumed were written to database tables using new SQL commands, permitting automated control from SQL application programs of test data collection and reduction. A number of tests were run over a wide variety of dynamically-created test databases, SQL queries, and system parameters. The results for single-table access, sorting, and local 2-table joins are reported here. The tests confirmed the accuracy of the majority of the I/O cost model, the significant contribution of CPU cost to total cost, and the need to model CPU cost in more detail than was done in System R. The R* optimizer now retains cost components separately and estimates the number of CPU instructions, including those for applying different kinds of predicates. The sensitivity of I/O cost to buffer space motivated the development of more detailed models of buffer utilization unclustered index scans and nested-loop joins often benefit from pages remaining in the buffers, whereas concurrent scans of the data pages and the index pages for multiple tables during joins compete for buffer share. Without an index on the join column of the inner table, the optimizer correctly avoids the nested-loop join, confirming the need for merge-scan joins. When the join column of the inner is indexed, the optimizer overestimates the cost of the nested-loop join, whose actual performance is very sensitive to three parameters that are extremely difficult to estimate (1) the join (result) cardinality, (2) the outer table's cardinality, and (3) the number of buffer pages available to store the inner table. Suggestions are given for improved database statistics, prefetch and page replacement strategies for the buffer manager, and the use of temporary indexes and Bloom filters (hashed semijoins) to reduce access of unneeded data.",
    "year": 1986,
    "citationCount": 170,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/16856.16863",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1145/16894.16863?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/16894.16863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1986-06-15",
    "authors": [
      {
        "authorId": "2143087",
        "name": "L. Mackert"
      },
      {
        "authorId": "1793770",
        "name": "G. Lohman"
      }
    ]
  },
  {
    "paperId": "4fa5856ce0aa7331c7a9ee70a225b0a7d5aa1d37",
    "title": "Retrieving images by 2D shape: a comparison of computation methods with human perceptual judgments",
    "abstract": "In content based image retrieval, systems allow users to ask for objects similar in shape to a query object. However, there is no clear understanding of how computational shape similarity corresponds to human shape similarity. In this paper several shape similarity measures were evaluated on planar, connected, non-occluded binary shapes. Shape similarity using algebraic moments, spline curve distances, cumulative turning angle, signal of curvature and Hausdorff- distance were compared to human similarity judgments on twenty test shapes against a large image database.",
    "year": 1994,
    "citationCount": 136,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.171777?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.171777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science",
      "Engineering"
    ],
    "publicationDate": "1994-04-01",
    "authors": [
      {
        "authorId": "1792053",
        "name": "B. Scassellati"
      },
      {
        "authorId": "2914624",
        "name": "S. Alexopoulos"
      },
      {
        "authorId": "1712991",
        "name": "M. Flickner"
      }
    ]
  },
  {
    "paperId": "7a80c42e575e0c22f831d30d042952c66dc5b080",
    "title": "Distance measures for color image retrieval",
    "abstract": "We address the issue of image database retrieval based on color using various vector distance metrics. Our system is based on color segmentation where only a few representative color vectors are extracted from each image and used as image indices. These vectors are then used with vector distance measures to determine similarity between a query color and a database image. We test numerous popular vector distance measures in our system and find that directional measures provide the most accurate and perceptually relevant retrievals.",
    "year": 1998,
    "citationCount": 118,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP.1998.723652?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP.1998.723652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationDate": "1998-10-04",
    "authors": [
      {
        "authorId": "1717060",
        "name": "D. Androutsos"
      },
      {
        "authorId": "1705037",
        "name": "K. Plataniotis"
      },
      {
        "authorId": "1707519",
        "name": "A. Venetsanopoulos"
      }
    ]
  }
]